{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30f2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /fs04/scratch2/lb64/projects/llm-bn/testing\n",
      "Added project root to Python path: /fs04/scratch2/lb64/projects/llm-bn\n",
      "Python path now includes: ['/fs04/scratch2/lb64/projects/llm-bn', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python313.zip', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/lib-dynload', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages', '/fs04/scratch2/lb64/projects/llm-bn/testing', '/fs04/scratch2/lb64/projects/llm-bn/bni_netica']\n",
      "Loaded 4 nets from /fs04/scratch2/lb64/projects/llm-bn/benchmarking/data/nets_dataset.parquet\n",
      "[DEBUG] get_answer_from_tool_agent called with model=ibm/granite4:micro, prompt length=69\n",
      "[DEBUG] prompt: QUERY: '\n",
      "Is changing the evidence of E going to change the probability of D?\n",
      "'\n",
      "[DEBUG] prompt: QUERY: '\n",
      "Is changing the evidence of E going to change the probability of D?\n",
      "'\n",
      "\n",
      "From the query above, extract the correct parameters for the tools using these nodes and states below. If the query related to multiple nodes, check for the abbreviations first (e.g., 'Tuberculosis or Cancer' can be 'TbOrCa') then check for the full names.\n",
      "NODES AND STATES:\n",
      "C ['False', 'True']\n",
      "E ['False', 'True']\n",
      "D ['False', 'True']\n",
      "A ['False', 'True']\n",
      "B ['False', 'True']\n",
      "\n",
      "\n",
      "[DEBUG] Initialized assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Final assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Initialized assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Final assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Initialized assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Final assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Initialized assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Final assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Initialized assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] Final assistant_msg: {'role': 'assistant', 'content': '', 'tool_calls': []}\n",
      "[DEBUG] chat_with_tools final_answer: type=<class 'NoneType'>, value=None\n",
      "[DEBUG] chat_with_tools returning single value: final_answer\n",
      "[DEBUG] chat_with_tools returned: type=<class 'NoneType'>, value=None\n",
      "[DEBUG] raw_answer from chat_with_tools (no log): type=<class 'NoneType'>, value=None\n",
      "[ERROR] extract_text received None answer\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.is_running():\n\u001b[32m     75\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ans = loop.run_until_complete(\u001b[43mget_answer_from_tool_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_debug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     77\u001b[39m     raw_response_time = time.time() - start_time\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/bn_helpers/tool_agent.py:664\u001b[39m, in \u001b[36mget_answer_from_tool_agent\u001b[39m\u001b[34m(net, prompt, model, model_temperature, max_tokens, max_rounds, require_tool, ollama_url, is_output_log, is_debug, model_top_p)\u001b[39m\n\u001b[32m    662\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] raw_answer from chat_with_tools (no log): type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(raw_answer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    663\u001b[39m     answer = extract_text(raw_answer)\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_debug:\n\u001b[32m    665\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] extract_text result (no log): type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(answer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    666\u001b[39m text = answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/bn_helpers/tool_agent.py:605\u001b[39m, in \u001b[36mextract_text\u001b[39m\u001b[34m(answer)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m         obj = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    607\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] extract_text JSON decode failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/json/__init__.py:339\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    340\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[31mTypeError\u001b[39m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to Python path so we can import modules from the project root\n",
    "# In Jupyter notebooks, __file__ is not defined, so we use getcwd() and navigate up\n",
    "current_dir = os.getcwd()\n",
    "# If we're in the testing directory, go up one level to get to project root\n",
    "if current_dir.endswith('/testing'):\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # If we're already in project root, use current directory\n",
    "    project_root = current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "print(f\"Added project root to Python path: {project_root}\")\n",
    "print(f\"Python path now includes: {[p for p in sys.path if 'llm-bn' in p]}\")\n",
    "\n",
    "import requests, json\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from ollama_helper.ollama_helper import answer_this_prompt\n",
    "from bn_helpers.get_structures_print_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "from bn_helpers.bn_helpers import BnToolBox\n",
    "from bn_helpers.utils import get_path, set_findings, temporarily_set_findings\n",
    "from benchmarking.data_utils import load_nets_from_parquet\n",
    "from ollama_helper.ollama_helper import answer_this_prompt\n",
    "from ollama_helper.prompts import TAKE_QUIZ_PROMPT\n",
    "from bn_helpers.bn_helpers import BnToolBox\n",
    "from bn_helpers.get_structures_print_tools import get_BN_structure\n",
    "from bn_helpers.tool_agent import get_answer_from_tool_agent, chat_with_tools\n",
    "from benchmarking.quiz_generator import (\n",
    "    create_dependency_quiz, create_common_cause_quiz, create_common_effect_quiz, create_blocked_evidence_quiz, \n",
    "    create_evidence_change_relationship_quiz, create_probability_quiz, create_highest_impact_evidence_quiz)\n",
    "from benchmarking.benchmarking_utils import pick_two_random_nodes, fake_random_nodes\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "from bn_helpers.constants import MODEL, MODEL_QUIZ, OLLAMA_URL\n",
    "from benchmarking.question_types import (DEPENDENCY_QUESTIONS, COMMON_CAUSE_QUESTIONS, COMMON_EFFECT_QUESTIONS, BLOCKED_EVIDENCES_QUESTIONS, \n",
    "EVIDENCE_CHANGE_RELATIONSHIP_QUESTIONS, PROBABILITY_QUESTIONS, HIGHEST_IMPACT_EVIDENCE_QUESTIONS)\n",
    "from pydantic import BaseModel\n",
    "from benchmarking.model_evaluator import (dependency_test, validate_quiz_answer, model_do_quiz, \n",
    "two_nodes_question, probability_question, \n",
    "debug_print_elementary_quiz, debug_print_numerical_quiz, export_quiz_samples_to_csv)\n",
    "import asyncio\n",
    "import time\n",
    "from ollama_helper.ollama_helper import get_answer_from_ollama, get_quiz_answer_from_thinking_model\n",
    "from ollama_helper.structure_output import AnswerStructure\n",
    "\n",
    "data_output = os.path.join(project_root, \"benchmarking\", \"data\")\n",
    "net_5, net_10, net_30, net_60 = load_nets_from_parquet(os.path.join(data_output, \"nets_dataset.parquet\"))\n",
    "\n",
    "MODEL_QUIZ = \"qwen2.5:7b\"\n",
    "# MODEL_TOOLS = \"gpt-oss:latest\"\n",
    "# MODEL_TOOLS = \"qwen3:8b\"\n",
    "# MODEL_TOOLS = \"llama3.1:70b\"\n",
    "MODEL_TOOLS = \"ibm/granite4:micro\"\n",
    "# MODEL_TOOLS = MODEL_QUIZ\n",
    "# print(generate_chat(\"Print [A, C] with no additional text\", model=\"qwen2.5:3b\", num_predict=5))\n",
    "# print(answer_this_prompt(\"Print [A, C] with no additional text\", model=\"qwen2.5:7b\", format=AnswerStructure.model_json_schema()))\n",
    "# Use the project root we established earlier to create the correct path\n",
    "\n",
    "model = MODEL_TOOLS\n",
    "prompt = \"\"\"\n",
    "Is changing the evidence of E going to change the probability of D?\n",
    "\"\"\"\n",
    "max_tokens = 1000\n",
    "STREAM = True\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        start_time = time.time()\n",
    "        ans = loop.run_until_complete(get_answer_from_tool_agent(net_5, prompt, model=model, max_tokens=max_tokens, is_debug=True))\n",
    "        raw_response_time = time.time() - start_time\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        ans = loop.run_until_complete(get_answer_from_tool_agent(prompt, model=model, max_tokens=max_tokens, is_debug=True))\n",
    "        raw_response_time = time.time() - start_time\n",
    "except RuntimeError:\n",
    "    start_time = time.time()\n",
    "    ans = asyncio.run(get_answer_from_tool_agent(prompt, model=model, max_tokens=max_tokens, is_debug=True))\n",
    "    raw_response_time = time.time() - start_time\n",
    "\n",
    "print(ans)\n",
    "print(raw_response_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352e713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nets from: /fs04/scratch2/lb64/projects/llm-bn/benchmarking/data\n",
      "Loaded 4 nets from /fs04/scratch2/lb64/projects/llm-bn/benchmarking/data/nets_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "data_output = os.path.join(project_root, \"benchmarking\", \"data\")\n",
    "print(f\"Loading nets from: {data_output}\")\n",
    "net_5, net_10, net_30, net_60 = load_nets_from_parquet(os.path.join(data_output, \"nets_dataset.parquet\"))\n",
    "\n",
    "from benchmarking.model_evaluator import dependency_test, common_cause_test, common_effect_test, blocked_evidence_test, evidence_change_relationship_test, probability_test, highest_impact_evidence_test\n",
    "\n",
    "\n",
    "# print(f\"Net 5:\")\n",
    "# printNet(net_5)\n",
    "# print()\n",
    "\n",
    "# print('Net 10:')\n",
    "# printNet(net_10)\n",
    "# print()\n",
    "\n",
    "# print('Net 30:')\n",
    "# printNet(net_30)\n",
    "# print()\n",
    "\n",
    "# print('Net 60:')\n",
    "# printNet(net_60)\n",
    "# print()\n",
    "\n",
    "# export_quiz_samples_to_csv(\n",
    "#     net_5,\n",
    "#     num_questions=5,\n",
    "#     output_file_path=\"quiz_samples_net5.csv\",\n",
    "#     include_sets=[\"dependency\", \"common_cause\", \"common_effect\", \"blocked_evidence\", \"evidence_change_relationship\", \"probability\"]\n",
    "# )\n",
    "\n",
    "# # For elementary tests\n",
    "# debug_print_elementary_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=DEPENDENCY_QUESTIONS,\n",
    "#     create_quiz_function=create_dependency_quiz,\n",
    "#     has_evidence=False,\n",
    "#     num_questions=2\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# debug_print_elementary_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=COMMON_CAUSE_QUESTIONS,\n",
    "#     create_quiz_function=create_common_cause_quiz,\n",
    "#     has_evidence=False,\n",
    "#     num_questions=2\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# debug_print_elementary_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=COMMON_EFFECT_QUESTIONS,\n",
    "#     create_quiz_function=create_common_effect_quiz,\n",
    "#     has_evidence=False,\n",
    "#     num_questions=2\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# debug_print_elementary_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=BLOCKED_EVIDENCES_QUESTIONS,\n",
    "#     create_quiz_function=create_blocked_evidence_quiz,\n",
    "#     has_evidence=False,\n",
    "#     num_questions=2\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# debug_print_elementary_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=EVIDENCE_CHANGE_RELATIONSHIP_QUESTIONS,\n",
    "#     create_quiz_function=create_evidence_change_relationship_quiz,\n",
    "#     has_evidence=False,\n",
    "#     num_questions=2\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# debug_print_numerical_quiz(\n",
    "#     net=net_5,\n",
    "#     question_set=PROBABILITY_QUESTIONS,\n",
    "#     create_quiz_function=create_probability_quiz,\n",
    "#     has_evidence=True,\n",
    "#     num_questions=2\n",
    "# )\n",
    "\n",
    "\n",
    "# print('Dependency Test:--------------------------------')\n",
    "# dependency_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Common Cause Test:--------------------------------')\n",
    "# common_cause_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Common Effect Test:--------------------------------')\n",
    "# common_effect_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Blocked Evidence Test:--------------------------------')\n",
    "# blocked_evidence_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Evidence Change Relationship Test:--------------------------------')\n",
    "# evidence_change_relationship_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Probability Test:--------------------------------')\n",
    "# probability_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Highest Impact Evidence Test:--------------------------------')\n",
    "# highest_impact_evidence_test(net_5, num_questions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "785af237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Network ===\n",
      "\n",
      "C -> ['E', 'D', 'A', 'B']\n",
      "E -> ['D', 'A', 'B']\n",
      "D -> ['A']\n",
      "A -> ['B']\n",
      "B -> []\n",
      "\n",
      "C ['False', 'True']\n",
      "E ['False', 'True']\n",
      "D ['False', 'True']\n",
      "A ['False', 'True']\n",
      "B ['False', 'True']\n",
      "\n",
      "\n",
      "=== USER QUERY ===\n",
      " Is the relationship between C and E affected by the evidence of B? \n",
      "\n",
      "\n",
      "=== BayMin Answer ===\n",
      "\n",
      "[BayMin] tool_call #1: check_evidences_change_relationship_between_two_nodes({'evidence': ['B'], 'node1': 'C', 'node2': 'E'})\n",
      "[BayMin] tool_result #1: {'result': 'No - conditioning on B does not change the dependency between C and E. Before observing B, they were d-connected. After observing all evidence, they remain d-connected.'}\n",
      "[BayMin] tool_call #1: get_evidences_block_two_nodes({'node1': 'C', 'node2': 'E'})\n",
      "[BayMin] tool_result #1: {'result': 'There are no evidences that would block the dependency between C and E.'}\n",
      "No – conditioning on B does not change the dependency between C and E. Before observing B they were d-connected, and after observing all evidence they remain d-connected.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Network ===\\n\")\n",
    "# nets = get_nets()\n",
    "# myNet = nets[1]\n",
    "\n",
    "printNet(net_5)\n",
    "print()\n",
    "print(get_BN_node_states(net_5))\n",
    "\n",
    "\n",
    "question = (\n",
    "    # \"Is Visitinz Azia change the probability of Smokng?\"\n",
    "    # \"Which symtome has a higher impact on Lung Cancer knowing that person is visiting Asia?\"\n",
    "    # \"Is changing the evidence of A going to change the probability of B?\"\n",
    "    # \"What is the common effect of C and B?\"\n",
    "    # \"What is the probability of XRay given Lung Cancer, Smoking and Visit Asiaaa?\"\n",
    "    \"Is the relationship between C and E affected by the evidence of B?\"\n",
    "    # \"What is the probability of A given B is increased and C is present?\"\n",
    "    # \"Is the relationship between Vizit Azia and Lung Cancr get affected when we observe Tuberculosis or Cancer?\"\n",
    "    # \"What set of evidences would block the path between B and C?\"\n",
    "\n",
    ")\n",
    "MODEL = MODEL_TOOLS\n",
    "print(\"\\n=== USER QUERY ===\\n\", question, \"\\n\")\n",
    "print(\"\\n=== BayMin Answer ===\\n\")\n",
    "# chat_with_tools(net_5, question, max_tokens=1000, isDebug=True, model=MODEL, isTesting=False)\n",
    "print(get_answer_from_tool_agent(net_5, question, max_tokens=1000, is_debug=True))\n",
    "# answer = get_answer_from_tool_agent(myNet, question, max_tokens=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e340-3732-4cbd-92c3-4a1c98cf1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"add\"}\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to function successfully\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL = \"gpt-oss-bn-json\"\n",
    "def answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None):\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": 50, # only when stream = False work\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Send the POST request with streaming enabled\n",
    "    with requests.post(endpoint, headers=headers, json=payload, stream=True) as response:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Process the response incrementally\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        response_json = json.loads(line)\n",
    "                        chunk = response_json.get(\"response\", \"\")\n",
    "                        full_response += chunk\n",
    "                        \n",
    "                        # Render the response as Markdown\n",
    "                        if stream:\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(full_response))\n",
    "                        \n",
    "                return full_response\n",
    "            except json.JSONDecodeError as e:\n",
    "                return \"Failed to parse JSON: \" + str(e)\n",
    "        else:\n",
    "            return \"Failed to retrieve response: \" + str(response.status_code)\n",
    "\n",
    "class BnToolBox(BaseModel):\n",
    "    fnName: str\n",
    "\n",
    "def add(a=5, b=6):\n",
    "    print('Go to function successfully')\n",
    "    return a + b\n",
    "\n",
    "output = answer_this_prompt('output this function name: add', stream=True, format=BnToolBox.model_json_schema())\n",
    "\n",
    "bn_tool_box = BnToolBox.model_validate_json(output)\n",
    "if bn_tool_box.fnName == 'add':\n",
    "    print(add())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e5a36-7719-4e9d-a231-84c79e94b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"isConnected\"}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "def isConnected(net, fromNode, toNode):\n",
    "  relatedNodes = net.node(fromNode).getRelated(\"d_connected\")\n",
    "  for node in relatedNodes:\n",
    "    if node.name() == toNode:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "PROMPT = \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "fromNode = 'Alarm'\n",
    "toNode = 'Fire'\n",
    "\n",
    "PROMPT = PROMPT.format(BN=BN, fromNode=fromNode, toNode=toNode)\n",
    "inputPrompt = PROMPT + 'if user ask anything related to are these two nodes connected to each other, output this function name: isConnected'\n",
    "output2 = answer_this_prompt(inputPrompt, stream=True, format=BnToolBox.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}?\"\"\",\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}? What are the two nodes mentioned?\"\"\",\n",
    "    \"Within the Bayesian Network {BN}, does a path exist from {fromNode} to {toNode}?\",\n",
    "    \"In the graph {BN}, can information flow from {fromNode} to {toNode}?\", # top perform \n",
    "    \"Are {fromNode} and {toNode} dependent in the Bayesian Network {BN}?\",\n",
    "    \"In {BN}, is there any direct or indirect connection between {fromNode} and {toNode}?\",\n",
    "    \"Can {fromNode} influence {toNode} in the Bayesian Network {BN}?\",\n",
    "    \"Is {toNode} reachable from {fromNode} in the structure of {BN}?\",\n",
    "    \"Does {BN} contain a path that links {fromNode} to {toNode}?\",\n",
    "    \"Are there any edges—direct or through other nodes—connecting {fromNode} and {toNode} in {BN}?\",\n",
    "    \"Is {toNode} conditionally dependent on {fromNode} in the Bayesian Network {BN}?\",\n",
    "    \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]\n",
    "\n",
    "for question in questions:\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  print(f\"Question: {question.format(BN=net.name(), fromNode=fromNode, toNode=toNode)}\")\n",
    "  for net in listOfNets:\n",
    "      for _ in range(5):\n",
    "        total += 1\n",
    "        fromNode, toNode = pick_two_random_nodes(net)\n",
    "        if fromNode and toNode:\n",
    "            \n",
    "            correctIdentified, queryFromNode, queryToNode = correctIdentification(question, net, fromNode, toNode)\n",
    "            if correctIdentified:\n",
    "              correct += 1\n",
    "            else:\n",
    "              print(f\"Incorrect identification for {net.name()}\")\n",
    "              printNet(net)\n",
    "              print()\n",
    "              print(\"Expected:\", fromNode, \"->\", toNode)\n",
    "              print(\"Reality:\", queryFromNode, \"->\", queryToNode)\n",
    "              print(\"----------------------------------------------------\")\n",
    "\n",
    "  print(f\"Total: {total}, Correct: {correct}, Accuracy: {correct/total:.2%}\")\n",
    "  print(\"<------------------------------------------------------------------------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bni_netica.support_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "# from bni_netica.bn_helpers import BnHelper, QueryTwoNodes, ParamExtractor\n",
    "# from ollama_helper.ollama_helper import answer_this_prompt\n",
    "# from bni_netica.scripts import HELLO_SCRIPT, MENU_SCRIPT, GET_FN_SCRIPT\n",
    "\n",
    "# # PROMPT = \"\"\"Consider this question: '{question}'. \n",
    "# # What are the two nodes in this question? \n",
    "# # Make sure to correctly output the names of nodes exactly as mentioned in the network and in the order as the question mentioned. \n",
    "# # For example, if the question mentioned \"A and B\" then the two nodes are fromNode: A, toNode: B; or if the question mentioned \"Smoking and Cancer\" then the two nodes are fromNode: Smoking, toNode: Cancer. \n",
    "# # Answer in JSON format.\"\"\"\n",
    "\n",
    "# def query_menu(BN_string, net):\n",
    "#     \"\"\"Input: BN: string, net: object\"\"\"\n",
    "#     pre_query = f\"\"\"In this Bayesian Network: \n",
    "# {BN_string}\n",
    "# \"\"\"\n",
    "#     user_query = input(\"Enter your query here: \")\n",
    "#     get_fn_prompt = pre_query + \"\\n\" + user_query + GET_FN_SCRIPT\n",
    "\n",
    "#     get_fn = answer_this_prompt(get_fn_prompt, format=BnHelper.model_json_schema())\n",
    "#     print(\"\\nBayMin:\")\n",
    "#     print(get_fn)\n",
    "\n",
    "#     get_fn = BnHelper.model_validate_json(get_fn)\n",
    "#     fn = get_fn.function_name\n",
    "\n",
    "#     bn_helper = BnHelper(function_name=fn)\n",
    "#     param_extractor = ParamExtractor()\n",
    "    \n",
    "#     if fn == \"is_XY_dconnected\":\n",
    "        \n",
    "#         get_params = param_extractor.extract_two_nodes_from_query(pre_query, user_query)\n",
    "#         print(get_params)\n",
    "\n",
    "#         ans = bn_helper.is_XY_dconnected(net, get_params.from_node, get_params.to_node)\n",
    "\n",
    "#         if ans:\n",
    "#             template = f\"Yes, {get_params.from_node} is d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would change the probability of {get_params.to_node} and vice versa.\"\n",
    "#         else:\n",
    "#             template = f\"No, {get_params.from_node} is not d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would not change the probability of {get_params.to_node}.\"\n",
    "        \n",
    "#         explain_prompt = f\"\"\"User asked: In this '{BN_string}', '{user_query}'. We use {fn} function and the output is: '{ans}'. Follow this exact template to provide the answer: '{template}'.\"\"\"\n",
    "#         print(answer_this_prompt(explain_prompt))\n",
    "\n",
    "#     print()\n",
    "    \n",
    "#     print(MENU_SCRIPT)\n",
    "#     choice = int(input(\"Enter your choice: \"))\n",
    "#     print()\n",
    "\n",
    "#     if choice == 1:\n",
    "#         input(\"Enter your query here: \")\n",
    "#         print('This is a sample answer.\\n')\n",
    "#     elif choice == 2:\n",
    "#         input(\"Enter your query here: \")\n",
    "#         print('This is a sample answer.\\n')\n",
    "#     elif choice == 3:\n",
    "#         print(\"Not yet implemented\\n\")\n",
    "#         return \n",
    "#     elif choice == 4:\n",
    "#         print(\"Goodbye!\\n\")\n",
    "#         return    \n",
    "\n",
    "# def main():\n",
    "#     print(HELLO_SCRIPT)\n",
    "#     nets = get_nets()\n",
    "\n",
    "    \n",
    "#     for i, net in enumerate(nets):\n",
    "#         print(f\"{i}: {net.name()}\")\n",
    "\n",
    "#     print()\n",
    "#     choice = int(input(\"Enter the number of the network you want to use: \"))\n",
    "#     print()\n",
    "#     if choice < 0 or choice >= len(nets):\n",
    "#         print(\"Invalid choice. Exiting.\")\n",
    "#         return\n",
    "    \n",
    "#     net = nets[choice]\n",
    "#     print(f\"You chose: {net.name()}\")\n",
    "#     printNet(net)\n",
    "#     print('\\nBN states:\\n')\n",
    "#     print(get_BN_node_states(net))\n",
    "\n",
    "#     BN_string = get_BN_structure(net)\n",
    "#     query_menu(BN_string=BN_string, net=net)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
