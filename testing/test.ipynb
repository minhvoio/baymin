{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30f2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /fs04/scratch2/lb64/projects/llm-bn/testing\n",
      "Added project root to Python path: /fs04/scratch2/lb64/projects/llm-bn\n",
      "Python path now includes: ['/fs04/scratch2/lb64/projects/llm-bn', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python313.zip', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/lib-dynload', '/home/mvo1/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages']\n",
      "Loading Netica\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to Python path so we can import modules from the project root\n",
    "# In Jupyter notebooks, __file__ is not defined, so we use getcwd() and navigate up\n",
    "current_dir = os.getcwd()\n",
    "# If we're in the testing directory, go up one level to get to project root\n",
    "if current_dir.endswith('/testing'):\n",
    "    project_root = os.path.dirname(current_dir)\n",
    "else:\n",
    "    # If we're already in project root, use current directory\n",
    "    project_root = current_dir\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "print(f\"Added project root to Python path: {project_root}\")\n",
    "print(f\"Python path now includes: {[p for p in sys.path if 'llm-bn' in p]}\")\n",
    "\n",
    "import requests, json\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from ollama_helper.ollama_helper import answer_this_prompt, generate_chat\n",
    "from bn_helpers.get_structures_print_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "from bn_helpers.bn_helpers import AnswerStructure, BnToolBox\n",
    "from bn_helpers.utils import get_path, set_findings, temporarily_set_findings\n",
    "from benchmarking.data_utils import load_nets_from_parquet\n",
    "from ollama_helper.ollama_helper import answer_this_prompt\n",
    "from ollama_helper.prompts import TAKE_QUIZ_PROMPT\n",
    "from bn_helpers.bn_helpers import AnswerStructure, BnToolBox\n",
    "from bn_helpers.get_structures_print_tools import get_BN_structure\n",
    "from bn_helpers.tool_agent import get_answer_from_tool_agent, chat_with_tools\n",
    "from benchmarking.quiz_generator import (\n",
    "    create_dependency_quiz, create_common_cause_quiz, create_common_effect_quiz, create_blocked_evidence_quiz, \n",
    "    create_evidence_change_relationship_quiz, create_probability_quiz, create_highest_impact_evidence_quiz)\n",
    "from benchmarking.benchmarking_utils import pick_two_random_nodes, fake_random_nodes\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider\n",
    "from bn_helpers.constants import MODEL, MODEL_QUIZ, OLLAMA_URL\n",
    "from benchmarking.question_types import (DEPENDENCY_QUESTIONS, COMMON_CAUSE_QUESTIONS, COMMON_EFFECT_QUESTIONS, BLOCKED_EVIDENCES_QUESTIONS, \n",
    "EVIDENCE_CHANGE_RELATIONSHIP_QUESTIONS, PROBABILITY_QUESTIONS, HIGHEST_IMPACT_EVIDENCE_QUESTIONS)\n",
    "from pydantic import BaseModel\n",
    "from benchmarking.model_evaluator import (dependency_test, validate_quiz_answer, model_do_quiz, \n",
    "get_answer_from_ollama, two_nodes_question, probability_question)\n",
    "import asyncio\n",
    "\n",
    "MODEL_QUIZ = \"qwen2.5:7b\"\n",
    "MODEL_TOOLS = \"gpt-oss:latest\"\n",
    "# MODEL_TOOLS = \"qwen3:8b\"\n",
    "# MODEL_TOOLS = MODEL_QUIZ\n",
    "# print(generate_chat(\"Print [A, C] with no additional text\", model=\"qwen2.5:3b\", num_predict=5))\n",
    "# print(answer_this_prompt(\"Print [A, C] with no additional text\", model=\"qwen2.5:7b\", format=AnswerStructure.model_json_schema()))\n",
    "# Use the project root we established earlier to create the correct path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352e713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading nets from: /fs04/scratch2/lb64/projects/llm-bn/benchmarking/data\n",
      "Loaded 4 nets from /fs04/scratch2/lb64/projects/llm-bn/benchmarking/data/nets_dataset.parquet\n",
      "Net 5:\n",
      "Probability Test:--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/models/openai.py:484\u001b[39m, in \u001b[36mOpenAIChatModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    483\u001b[39m     extra_headers.setdefault(\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m, get_user_agent())\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    485\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m    486\u001b[39m         messages=openai_messages,\n\u001b[32m    487\u001b[39m         parallel_tool_calls=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    488\u001b[39m         tools=tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    489\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    490\u001b[39m         stream=stream,\n\u001b[32m    491\u001b[39m         stream_options={\u001b[33m'\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m NOT_GIVEN,\n\u001b[32m    492\u001b[39m         stop=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    493\u001b[39m         max_completion_tokens=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    494\u001b[39m         timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    495\u001b[39m         response_format=response_format \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    496\u001b[39m         seed=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    497\u001b[39m         reasoning_effort=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_reasoning_effort\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    498\u001b[39m         user=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_user\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    499\u001b[39m         web_search_options=web_search_options \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    500\u001b[39m         service_tier=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_service_tier\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    501\u001b[39m         prediction=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_prediction\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    502\u001b[39m         temperature=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    503\u001b[39m         top_p=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    504\u001b[39m         presence_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    505\u001b[39m         frequency_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    506\u001b[39m         logit_bias=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    507\u001b[39m         logprobs=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    508\u001b[39m         top_logprobs=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_top_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    509\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    510\u001b[39m         extra_body=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mextra_body\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    511\u001b[39m     )\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2584\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m         {\n\u001b[32m   2589\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m         },\n\u001b[32m   2624\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m     ),\n\u001b[32m   2628\u001b[39m     options=make_request_options(\n\u001b[32m   2629\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m     ),\n\u001b[32m   2631\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mInternalServerError\u001b[39m: Error code: 500 - {'error': {'message': 'unexpected error format in response', 'type': 'api_error', 'param': None, 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/benchmarking/model_evaluator.py:106\u001b[39m, in \u001b[36mraw_model_test\u001b[39m\u001b[34m(prompt, quiz, y, model, max_tokens, model_quiz)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ans = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_answer_from_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/benchmarking/model_evaluator.py:38\u001b[39m, in \u001b[36mget_answer_from_ollama\u001b[39m\u001b[34m(prompt, model, max_tokens)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# result = await agent.run(prompt)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# answer = result.output.answer\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# # print('get_answer_from_ollama:\\n', answer)\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# return answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\n\u001b[32m     39\u001b[39m prompt,\n\u001b[32m     40\u001b[39m model_settings={\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,   \u001b[38;5;66;03m# ✅ use this key inside model_settings\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# \"temperature\": 0.3,  # optional\u001b[39;00m\n\u001b[32m     43\u001b[39m }\n\u001b[32m     44\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.output.answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/agent/abstract.py:229\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, event_stream_handler)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    218\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    219\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     toolsets=toolsets,\n\u001b[32m    228\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    230\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    231\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    232\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/run.py:150\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(node=next_node):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_graph/graph.py:758\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_graph/graph.py:731\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    730\u001b[39m         ctx = GraphRunContext(state=\u001b[38;5;28mself\u001b[39m.state, deps=\u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:397\u001b[39m, in \u001b[36mModelRequestNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.AgentRunError(\u001b[33m'\u001b[39m\u001b[33mYou must finish streaming before calling run()\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_request(ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:439\u001b[39m, in \u001b[36mModelRequestNode._make_request\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    438\u001b[39m model_settings, model_request_parameters, message_history, _ = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m model_response = \u001b[38;5;28;01mawait\u001b[39;00m ctx.deps.model.request(message_history, model_settings, model_request_parameters)\n\u001b[32m    440\u001b[39m ctx.state.usage.requests += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/models/openai.py:402\u001b[39m, in \u001b[36mOpenAIChatModel.request\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    398\u001b[39m model_settings, model_request_parameters = \u001b[38;5;28mself\u001b[39m.prepare_request(\n\u001b[32m    399\u001b[39m     model_settings,\n\u001b[32m    400\u001b[39m     model_request_parameters,\n\u001b[32m    401\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._completions_create(\n\u001b[32m    403\u001b[39m     messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(OpenAIChatModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    404\u001b[39m )\n\u001b[32m    405\u001b[39m model_response = \u001b[38;5;28mself\u001b[39m._process_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/pydantic_ai/models/openai.py:514\u001b[39m, in \u001b[36mOpenAIChatModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (status_code := e.status_code) >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code=status_code, model_name=\u001b[38;5;28mself\u001b[39m.model_name, body=e.body) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mModelHTTPError\u001b[39m: status_code: 500, model_name: gpt-oss:latest, body: {'message': 'unexpected error format in response', 'type': 'api_error', 'param': None, 'code': None}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# print('Dependency Test:--------------------------------')\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# dependency_test(net_5, num_questions=1)\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print('Evidence Change Relationship Test:--------------------------------')\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# evidence_change_relationship_test(net_5, num_questions=1)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mProbability Test:--------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mprobability_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mHighest Impact Evidence Test:--------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m highest_impact_evidence_test(net_5, num_questions=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/benchmarking/model_evaluator.py:191\u001b[39m, in \u001b[36mprobability_test\u001b[39m\u001b[34m(net, model, model_quiz, max_tokens, num_questions, hasEvidence)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprobability_test\u001b[39m(net, model=MODEL, model_quiz=MODEL_QUIZ, max_tokens=\u001b[32m1000\u001b[39m, num_questions=\u001b[32m30\u001b[39m, hasEvidence=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumerical_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROBABILITY_QUESTIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_probability_quiz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_quiz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_quiz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasEvidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasEvidence\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/benchmarking/model_evaluator.py:183\u001b[39m, in \u001b[36mnumerical_test\u001b[39m\u001b[34m(net, question_set, create_quiz_function, model, model_quiz, hasEvidence, max_tokens, num_questions)\u001b[39m\n\u001b[32m    180\u001b[39m     prompt, node, question_output = probability_question(net, question_format=question, hasEvidence=hasEvidence)\n\u001b[32m    181\u001b[39m     quiz, y = create_quiz_function(question_output, net, node)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m raw_model_score = \u001b[43mraw_model_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_quiz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_quiz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m baymin_score = baymin_test(net, quiz, y, question_output, model=model, max_tokens=max_tokens, model_quiz=model_quiz)\n\u001b[32m    185\u001b[39m raw_model_total_score += raw_model_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/fs04/scratch2/lb64/projects/llm-bn/benchmarking/model_evaluator.py:110\u001b[39m, in \u001b[36mraw_model_test\u001b[39m\u001b[34m(prompt, quiz, y, model, max_tokens, model_quiz)\u001b[39m\n\u001b[32m    108\u001b[39m         ans = loop.run_until_complete(get_answer_from_ollama(prompt, model=model, max_tokens=max_tokens))\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     ans = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_answer_from_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m y_hat = model_do_quiz(quiz, ans, model=model_quiz)\n\u001b[32m    113\u001b[39m score = validate_quiz_answer(y, y_hat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lb64_scratch/miniconda3/envs/llm-bn/lib/python3.13/selectors.py:452\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    450\u001b[39m ready = []\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_output = os.path.join(project_root, \"benchmarking\", \"data\")\n",
    "print(f\"Loading nets from: {data_output}\")\n",
    "net_5, net_10, net_30, net_60 = load_nets_from_parquet(os.path.join(data_output, \"nets_dataset.parquet\"))\n",
    "\n",
    "from benchmarking.model_evaluator import dependency_test, common_cause_test, common_effect_test, blocked_evidence_test, evidence_change_relationship_test, probability_test, highest_impact_evidence_test\n",
    "\n",
    "\n",
    "print(f\"Net 5:\")\n",
    "# print('Dependency Test:--------------------------------')\n",
    "# dependency_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Common Cause Test:--------------------------------')\n",
    "# common_cause_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Common Effect Test:--------------------------------')\n",
    "# common_effect_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Blocked Evidence Test:--------------------------------')\n",
    "# blocked_evidence_test(net_5, num_questions=1)\n",
    "\n",
    "# print('Evidence Change Relationship Test:--------------------------------')\n",
    "# evidence_change_relationship_test(net_5, num_questions=1)\n",
    "\n",
    "print('Probability Test:--------------------------------')\n",
    "probability_test(net_5, num_questions=1)\n",
    "\n",
    "print('Highest Impact Evidence Test:--------------------------------')\n",
    "highest_impact_evidence_test(net_5, num_questions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785af237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Network ===\n",
      "\n",
      "VisitAsia -> ['Tuberculosis']\n",
      "Tuberculosis -> ['TbOrCa']\n",
      "Smoking -> ['Cancer', 'Bronchitis']\n",
      "Cancer -> ['TbOrCa']\n",
      "TbOrCa -> ['XRay', 'Dyspnea']\n",
      "XRay -> []\n",
      "Bronchitis -> ['Dyspnea']\n",
      "Dyspnea -> []\n",
      "\n",
      "VisitAsia ['visit', 'no_visit']\n",
      "Tuberculosis ['present', 'absent']\n",
      "Smoking ['smoker', 'non_smoker']\n",
      "Cancer ['present', 'absent']\n",
      "TbOrCa ['true', 'false']\n",
      "XRay ['abnormal', 'normal']\n",
      "Bronchitis ['present', 'absent']\n",
      "Dyspnea ['present', 'absent']\n",
      "\n",
      "\n",
      "=== USER QUERY ===\n",
      " Which symtome has a higher impact on Lung Cancer knowing that person is visiting Asia? \n",
      "\n",
      "\n",
      "=== BayMin Answer ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Answer**\\n\\nGiven that the person is visiting Asia, the symptom that most strongly affects the probability of lung cancer is **an abnormal X‑ray**.\\n\\n**Explanation**\\n\\nThe tool was used to evaluate how each symptom changes the posterior probability of Cancer when the background evidence “VisitAsia = visit” is present.  \\nThe results show that adding an abnormal X‑ray increases the probability of Cancer from 5.5\\u202f% to 44.71\\u202f%—a shift of 39.21\\u202f%.  \\nThe other symptoms (Bronchitis and Dyspnea) produce smaller changes.  \\nTherefore, among the symptoms considered, an abnormal X‑ray has the highest impact on lung‑cancer risk for someone visiting Asia.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== Network ===\\n\")\n",
    "nets = get_nets()\n",
    "myNet = nets[1]\n",
    "\n",
    "printNet(myNet)\n",
    "print()\n",
    "print(get_BN_node_states(myNet))\n",
    "\n",
    "\n",
    "question = (\n",
    "    # \"Is Visitinz Azia change the probability of Smokng?\"\n",
    "    \"Which symtome has a higher impact on Lung Cancer knowing that person is visiting Asia?\"\n",
    "    # \"Is changing the evidence of A going to change the probability of B?\"\n",
    "    # \"What is the common effect of C and B?\"\n",
    "    # \"What is the probability of XRay given Lung Cancer, Smoking and Visit Asiaaa?\"\n",
    "    # \"What is the probability of A given B is increased and C is present?\"\n",
    "    # \"Is the relationship between Vizit Azia and Lung Cancr get affected when we observe Tuberculosis or Cancer?\"\n",
    "    # \"What set of evidences would block the path between B and C?\"\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\n=== USER QUERY ===\\n\", question, \"\\n\")\n",
    "print(\"\\n=== BayMin Answer ===\\n\")\n",
    "chat_with_tools(myNet, question, max_tokens=5000)\n",
    "# answer = get_answer_from_tool_agent(myNet, question, max_tokens=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e340-3732-4cbd-92c3-4a1c98cf1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"add\"}\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to function successfully\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL = \"gpt-oss-bn-json\"\n",
    "def answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None):\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": 50, # only when stream = False work\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Send the POST request with streaming enabled\n",
    "    with requests.post(endpoint, headers=headers, json=payload, stream=True) as response:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Process the response incrementally\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        response_json = json.loads(line)\n",
    "                        chunk = response_json.get(\"response\", \"\")\n",
    "                        full_response += chunk\n",
    "                        \n",
    "                        # Render the response as Markdown\n",
    "                        if stream:\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(full_response))\n",
    "                        \n",
    "                return full_response\n",
    "            except json.JSONDecodeError as e:\n",
    "                return \"Failed to parse JSON: \" + str(e)\n",
    "        else:\n",
    "            return \"Failed to retrieve response: \" + str(response.status_code)\n",
    "\n",
    "class BnToolBox(BaseModel):\n",
    "    fnName: str\n",
    "\n",
    "def add(a=5, b=6):\n",
    "    print('Go to function successfully')\n",
    "    return a + b\n",
    "\n",
    "output = answer_this_prompt('output this function name: add', stream=True, format=BnToolBox.model_json_schema())\n",
    "\n",
    "bn_tool_box = BnToolBox.model_validate_json(output)\n",
    "if bn_tool_box.fnName == 'add':\n",
    "    print(add())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e5a36-7719-4e9d-a231-84c79e94b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"isConnected\"}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "def isConnected(net, fromNode, toNode):\n",
    "  relatedNodes = net.node(fromNode).getRelated(\"d_connected\")\n",
    "  for node in relatedNodes:\n",
    "    if node.name() == toNode:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "PROMPT = \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "fromNode = 'Alarm'\n",
    "toNode = 'Fire'\n",
    "\n",
    "PROMPT = PROMPT.format(BN=BN, fromNode=fromNode, toNode=toNode)\n",
    "inputPrompt = PROMPT + 'if user ask anything related to are these two nodes connected to each other, output this function name: isConnected'\n",
    "output2 = answer_this_prompt(inputPrompt, stream=True, format=BnToolBox.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}?\"\"\",\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}? What are the two nodes mentioned?\"\"\",\n",
    "    \"Within the Bayesian Network {BN}, does a path exist from {fromNode} to {toNode}?\",\n",
    "    \"In the graph {BN}, can information flow from {fromNode} to {toNode}?\", # top perform \n",
    "    \"Are {fromNode} and {toNode} dependent in the Bayesian Network {BN}?\",\n",
    "    \"In {BN}, is there any direct or indirect connection between {fromNode} and {toNode}?\",\n",
    "    \"Can {fromNode} influence {toNode} in the Bayesian Network {BN}?\",\n",
    "    \"Is {toNode} reachable from {fromNode} in the structure of {BN}?\",\n",
    "    \"Does {BN} contain a path that links {fromNode} to {toNode}?\",\n",
    "    \"Are there any edges—direct or through other nodes—connecting {fromNode} and {toNode} in {BN}?\",\n",
    "    \"Is {toNode} conditionally dependent on {fromNode} in the Bayesian Network {BN}?\",\n",
    "    \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]\n",
    "\n",
    "for question in questions:\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  print(f\"Question: {question.format(BN=net.name(), fromNode=fromNode, toNode=toNode)}\")\n",
    "  for net in listOfNets:\n",
    "      for _ in range(5):\n",
    "        total += 1\n",
    "        fromNode, toNode = pick_two_random_nodes(net)\n",
    "        if fromNode and toNode:\n",
    "            \n",
    "            correctIdentified, queryFromNode, queryToNode = correctIdentification(question, net, fromNode, toNode)\n",
    "            if correctIdentified:\n",
    "              correct += 1\n",
    "            else:\n",
    "              print(f\"Incorrect identification for {net.name()}\")\n",
    "              printNet(net)\n",
    "              print()\n",
    "              print(\"Expected:\", fromNode, \"->\", toNode)\n",
    "              print(\"Reality:\", queryFromNode, \"->\", queryToNode)\n",
    "              print(\"----------------------------------------------------\")\n",
    "\n",
    "  print(f\"Total: {total}, Correct: {correct}, Accuracy: {correct/total:.2%}\")\n",
    "  print(\"<------------------------------------------------------------------------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bni_netica.support_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "# from bni_netica.bn_helpers import BnHelper, QueryTwoNodes, ParamExtractor\n",
    "# from ollama_helper.ollama_helper import answer_this_prompt\n",
    "# from bni_netica.scripts import HELLO_SCRIPT, MENU_SCRIPT, GET_FN_SCRIPT\n",
    "\n",
    "# # PROMPT = \"\"\"Consider this question: '{question}'. \n",
    "# # What are the two nodes in this question? \n",
    "# # Make sure to correctly output the names of nodes exactly as mentioned in the network and in the order as the question mentioned. \n",
    "# # For example, if the question mentioned \"A and B\" then the two nodes are fromNode: A, toNode: B; or if the question mentioned \"Smoking and Cancer\" then the two nodes are fromNode: Smoking, toNode: Cancer. \n",
    "# # Answer in JSON format.\"\"\"\n",
    "\n",
    "# def query_menu(BN_string, net):\n",
    "#     \"\"\"Input: BN: string, net: object\"\"\"\n",
    "#     pre_query = f\"\"\"In this Bayesian Network: \n",
    "# {BN_string}\n",
    "# \"\"\"\n",
    "#     user_query = input(\"Enter your query here: \")\n",
    "#     get_fn_prompt = pre_query + \"\\n\" + user_query + GET_FN_SCRIPT\n",
    "\n",
    "#     get_fn = answer_this_prompt(get_fn_prompt, format=BnHelper.model_json_schema())\n",
    "#     print(\"\\nBayMin:\")\n",
    "#     print(get_fn)\n",
    "\n",
    "#     get_fn = BnHelper.model_validate_json(get_fn)\n",
    "#     fn = get_fn.function_name\n",
    "\n",
    "#     bn_helper = BnHelper(function_name=fn)\n",
    "#     param_extractor = ParamExtractor()\n",
    "    \n",
    "#     if fn == \"is_XY_dconnected\":\n",
    "        \n",
    "#         get_params = param_extractor.extract_two_nodes_from_query(pre_query, user_query)\n",
    "#         print(get_params)\n",
    "\n",
    "#         ans = bn_helper.is_XY_dconnected(net, get_params.from_node, get_params.to_node)\n",
    "\n",
    "#         if ans:\n",
    "#             template = f\"Yes, {get_params.from_node} is d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would change the probability of {get_params.to_node} and vice versa.\"\n",
    "#         else:\n",
    "#             template = f\"No, {get_params.from_node} is not d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would not change the probability of {get_params.to_node}.\"\n",
    "        \n",
    "#         explain_prompt = f\"\"\"User asked: In this '{BN_string}', '{user_query}'. We use {fn} function and the output is: '{ans}'. Follow this exact template to provide the answer: '{template}'.\"\"\"\n",
    "#         print(answer_this_prompt(explain_prompt))\n",
    "\n",
    "#     print()\n",
    "    \n",
    "#     print(MENU_SCRIPT)\n",
    "#     choice = int(input(\"Enter your choice: \"))\n",
    "#     print()\n",
    "\n",
    "#     if choice == 1:\n",
    "#         input(\"Enter your query here: \")\n",
    "#         print('This is a sample answer.\\n')\n",
    "#     elif choice == 2:\n",
    "#         input(\"Enter your query here: \")\n",
    "#         print('This is a sample answer.\\n')\n",
    "#     elif choice == 3:\n",
    "#         print(\"Not yet implemented\\n\")\n",
    "#         return \n",
    "#     elif choice == 4:\n",
    "#         print(\"Goodbye!\\n\")\n",
    "#         return    \n",
    "\n",
    "# def main():\n",
    "#     print(HELLO_SCRIPT)\n",
    "#     nets = get_nets()\n",
    "\n",
    "    \n",
    "#     for i, net in enumerate(nets):\n",
    "#         print(f\"{i}: {net.name()}\")\n",
    "\n",
    "#     print()\n",
    "#     choice = int(input(\"Enter the number of the network you want to use: \"))\n",
    "#     print()\n",
    "#     if choice < 0 or choice >= len(nets):\n",
    "#         print(\"Invalid choice. Exiting.\")\n",
    "#         return\n",
    "    \n",
    "#     net = nets[choice]\n",
    "#     print(f\"You chose: {net.name()}\")\n",
    "#     printNet(net)\n",
    "#     print('\\nBN states:\\n')\n",
    "#     print(get_BN_node_states(net))\n",
    "\n",
    "#     BN_string = get_BN_structure(net)\n",
    "#     query_menu(BN_string=BN_string, net=net)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
