{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d27c18-ebe3-4905-aaf6-90afc029f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ea6135da5f410fbc3f8f84fbd23742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834d97ae1adc40c7b2d4757392332480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# --- Load GPT-OSS locally ---\n",
    "model_id = \"openai/gpt-oss-20b\"  # or \"openai/gpt-oss-120b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7319a4e3-beed-4bdb-bcb0-69f7828b76b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    return_full_text=False, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0f441b-56b2-4fa3-8980-169dfa0eeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "class Query(BaseModel):\n",
    "    fromNode: str = Field(..., min_length=1)\n",
    "    toNode: str = Field(..., min_length=1)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Query)\n",
    "\n",
    "# 2. Prompt with format instructions from the parser\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a Bayesian Network expert. Consider this question: “In the graph {BN}, can information flow from {fromNode} to {toNode}?”, what are the two nodes in this question?\n",
    "\n",
    "Your task is to answer the question and return a JSON format matching the schema below.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "DO NOT include any explanation. Only return the JSON.\n",
    "\"\"\",\n",
    "    input_variables=[\"BN\", \"fromNode\", \"toNode\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"\"\"\n",
    "# You are a Bayesian Network expert. Consider this question: “In the graph, can information flow from fromNode to toNode?”, what are the two nodes in this question?\n",
    "\n",
    "# Your task is to answer the question and return a JSON format matching the schema below.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# DO NOT include any explanation. Only return the JSON.\n",
    "# \"\"\",\n",
    "#     input_variables=[],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315483c3-5633-4e60-84b0-b2f3f424f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "def extract_json_from_text(text: str) -> Any:\n",
    "    pattern = r\"\\{.*?\\}|\\[.*?\\]\"  # Matches {object} or [array], non-greedy\n",
    "    matches = re.findall(pattern, text, flags=re.DOTALL)\n",
    "\n",
    "    for candidate in matches:\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(f\"No JSON found in text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ba872f-f4b4-4645-b721-49fe44b4faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def parse_text(text: str) -> Query:\n",
    "    return Query.model_validate(extract_json_from_text(text))\n",
    "\n",
    "# chain: Runnable = prompt | llm | RunnableLambda(parse_text)\n",
    "# Equal to \n",
    "# def chain(input):\n",
    "#     prompt_text = prompt.invoke(input)\n",
    "#     llm_output = llm.invoke(prompt_text)\n",
    "#     parsed_result = parse_text(llm_output)\n",
    "#     return parsed_result\n",
    "\n",
    "# result = chain.invoke({})\n",
    "# print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb3ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isConnected(net, fromNode, toNode):\n",
    "  relatedNodes = net.node(fromNode).getRelated(\"d_connected\")\n",
    "  for node in relatedNodes:\n",
    "    if node.name() == toNode:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def correctIdentification(prompt, net, fromNode, toNode):\n",
    "    BN = \"\"\n",
    "    \n",
    "    for node in net.nodes():\n",
    "        BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "    chain: Runnable = prompt | llm \n",
    "    # chain: Runnable = prompt | llm | RunnableLambda(parse_text)\n",
    "    # result = chain.invoke({})\n",
    "\n",
    "    result = chain.invoke({\n",
    "        \"BN\": BN,\n",
    "        \"fromNode\": fromNode,\n",
    "        \"toNode\": toNode\n",
    "    })\n",
    "\n",
    "    queryFromNode = result.fromNode\n",
    "    queryToNode = result.toNode\n",
    "\n",
    "    return queryFromNode == fromNode and queryToNode == toNode, queryFromNode, queryToNode\n",
    "\n",
    "import random \n",
    "def pickTwoRandomNodes(net):\n",
    "    nodes = net.nodes()\n",
    "    if len(nodes) < 2:\n",
    "        return None, None\n",
    "    node1, node2 = random.sample(nodes, 2)\n",
    "    return node1.name(), node2.name()\n",
    "\n",
    "def printNet(net):\n",
    "    for node in net.nodes():\n",
    "        print(f\"{node.name()} -> {[child.name() for child in node.children()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a19363",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "\n",
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06c5f958",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fromNode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m fromNode, toNode = pickTwoRandomNodes(net)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fromNode \u001b[38;5;129;01mand\u001b[39;00m toNode:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     correctIdentified, queryFromNode, queryToNode = \u001b[43mcorrectIdentification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromNode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoNode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m correctIdentified:\n\u001b[32m     12\u001b[39m       correct += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcorrectIdentification\u001b[39m\u001b[34m(prompt, net, fromNode, toNode)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# chain: Runnable = prompt | llm | RunnableLambda(parse_text)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# result = chain.invoke({})\u001b[39;00m\n\u001b[32m     18\u001b[39m result = chain.invoke({\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBN\u001b[39m\u001b[33m\"\u001b[39m: BN,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfromNode\u001b[39m\u001b[33m\"\u001b[39m: fromNode,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtoNode\u001b[39m\u001b[33m\"\u001b[39m: toNode\n\u001b[32m     22\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m queryFromNode = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromNode\u001b[49m\n\u001b[32m     25\u001b[39m queryToNode = result.toNode\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m queryFromNode == fromNode \u001b[38;5;129;01mand\u001b[39;00m queryToNode == toNode, queryFromNode, queryToNode\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'fromNode'"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for net in listOfNets:\n",
    "  for _ in range(10):\n",
    "    total += 1\n",
    "    fromNode, toNode = pickTwoRandomNodes(net)\n",
    "    if fromNode and toNode:\n",
    "        \n",
    "        correctIdentified, queryFromNode, queryToNode = correctIdentification(prompt, net, fromNode, toNode)\n",
    "        if correctIdentified:\n",
    "          correct += 1\n",
    "        else:\n",
    "          print(f\"Incorrect identification for {net.name()}\")\n",
    "          printNet(net)\n",
    "          print()\n",
    "          print(\"Expected:\", fromNode, \"->\", toNode)\n",
    "          print(\"Reality:\", queryFromNode, \"->\", queryToNode)\n",
    "          print(\"----------------------------------------------------\")\n",
    "\n",
    "print(f\"Total: {total}, Correct: {correct}, Accuracy: {correct/total:.2%}\")\n",
    "print(\"<------------------------------------------------------------------------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ee5c58-8b23-4870-be06-9bcd9fa20d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> We need to parse the question: \"In the graph VisitAsia -> ['Tuberculosis'] Tuberculosis -> ['TbOrCa'] Smoking -> ['Cancer', 'Bronchitis'] Cancer -> ['TbOrCa'] TbOrCa -> ['XRay', 'Dyspnea'] XRay -> [] Bronchitis -> ['Dyspnea'] Dyspnea -> [], can information flow from Smoking to Lung Cancer?, what are the two nodes in this question?\"\n",
      "\n",
      "They ask: \"can information flow from Smoking to Lung Cancer?\" The two nodes are Smoking and Cancer? Wait they ask \"what are the two nodes in this question?\" So answer should be fromNode: \"Smoking\" toNode: \"Cancer\". Provide JSON accordingly.\n",
      "\n",
      "Let's produce JSON: {\"fromNode\":\"Smoking\",\"toNode\":\"Cancer\"}.\n",
      "\n",
      "Check schema: properties fromNode string, toNode string, required both. Good. Return only JSON.assistantfinal{\"fromNode\":\"Smoking\",\"toNode\":\"Cancer\"}\n"
     ]
    }
   ],
   "source": [
    "BN = \"\"\n",
    "\n",
    "for node in ChestClinicNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "probe = prompt | llm\n",
    "out = probe.invoke({\"BN\": BN, \"fromNode\": \"Smoking\", \"toNode\": \"Lung Cancer\"})\n",
    "print(type(out), out)  # you'll likely see a list or a message object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6ddbf-3654-4eaf-9040-615838c327b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-bn)",
   "language": "python",
   "name": "llm-bn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
