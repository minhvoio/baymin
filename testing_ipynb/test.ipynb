{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30f2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Netica\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # go up one folder\n",
    "\n",
    "import requests, json\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from ollama.prompt import answer_this_prompt, generate_chat\n",
    "from bn_helpers.get_structures_print_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "from bn_helpers.bn_helpers import AnswerStructure, BnHelper\n",
    "from bn_helpers.utils import get_path\n",
    "\n",
    "MODEL_QUIZ = \"qwen2.5:7b\"\n",
    "MODEL_TOOLS = \"gpt-oss:latest\"\n",
    "# MODEL_TOOLS = MODEL_QUIZ\n",
    "# print(generate_chat(\"Print [A, C] with no additional text\", model=\"qwen2.5:3b\", num_predict=5))\n",
    "# print(answer_this_prompt(\"Print [A, C] with no additional text\", model=\"qwen2.5:7b\", format=AnswerStructure.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4227b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_enforced_ollama.py\n",
    "import requests, json, inspect, typing\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "# ---------- Python type -> JSON Schema ----------\n",
    "def _pytype_to_schema(t):\n",
    "    origin = typing.get_origin(t)\n",
    "    args = typing.get_args(t)\n",
    "    if t in (int,): return {\"type\": \"integer\"}\n",
    "    if t in (float,): return {\"type\": \"number\"}\n",
    "    if t in (bool,): return {\"type\": \"boolean\"}\n",
    "    if t in (str,): return {\"type\": \"string\"}\n",
    "    if t in (dict, typing.Dict, typing.Mapping): return {\"type\": \"object\"}\n",
    "    if t in (list, typing.List, typing.Sequence): return {\"type\": \"array\"}\n",
    "    if origin is typing.Union and len(args) == 2 and type(None) in args:\n",
    "        other = args[0] if args[1] is type(None) else args[1]\n",
    "        sch = _pytype_to_schema(other)\n",
    "        if \"type\" in sch and isinstance(sch[\"type\"], str):\n",
    "            sch[\"type\"] = [sch[\"type\"], \"null\"]\n",
    "        return sch\n",
    "    if origin in (list, typing.List, typing.Sequence) and args:\n",
    "        return {\"type\": \"array\", \"items\": _pytype_to_schema(args[0])}\n",
    "    return {\"type\": \"string\"}\n",
    "\n",
    "def function_to_tool_schema(fn, *, name=None, description=None):\n",
    "    sig = inspect.signature(fn)\n",
    "    props, required = {}, []\n",
    "    for pname, p in sig.parameters.items():\n",
    "        if p.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n",
    "            continue\n",
    "        props[pname] = _pytype_to_schema(p.annotation) if p.annotation is not inspect._empty else {\"type\":\"string\"}\n",
    "        if p.default is inspect._empty:\n",
    "            required.append(pname)\n",
    "    return {\n",
    "        \"type\":\"function\",\n",
    "        \"function\":{\n",
    "            \"name\": name or fn.__name__,\n",
    "            \"description\": description or (fn.__doc__.strip() if fn.__doc__ else f\"Function {fn.__name__}\"),\n",
    "            \"parameters\":{\"type\":\"object\",\"properties\":props,\"required\":required}\n",
    "        }\n",
    "    }\n",
    "\n",
    "def _coerce_arg(value, param: inspect.Parameter):\n",
    "    if param.annotation in (int, float, bool, str):\n",
    "        try: return param.annotation(value)\n",
    "        except Exception: return value\n",
    "    return value\n",
    "\n",
    "def _bind_and_call(fn, kwargs: dict):\n",
    "    sig = inspect.signature(fn)\n",
    "    bound = {}\n",
    "    for pname, p in sig.parameters.items():\n",
    "        if p.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n",
    "            continue\n",
    "        if pname in kwargs:\n",
    "            bound[pname] = _coerce_arg(kwargs[pname], p)\n",
    "        elif p.default is not inspect._empty:\n",
    "            bound[pname] = p.default\n",
    "        else:\n",
    "            raise TypeError(f\"Missing required argument: {pname}\")\n",
    "    return fn(**bound)\n",
    "\n",
    "import json, requests\n",
    "\n",
    "def _normalize_args_for_key(args):\n",
    "    \"\"\"\n",
    "    Turn args dict into a hashable, order-stable key.\n",
    "    Handles nested dicts/lists by JSON-dumping with sorted keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.dumps(args, sort_keys=True, separators=(\",\", \":\"))\n",
    "    except TypeError:\n",
    "        # As a fallback, repr (still stable enough for dedup in most cases)\n",
    "        return repr(args)\n",
    "\n",
    "def chat_with_tools(\n",
    "    prompt: str,\n",
    "    fns: dict,\n",
    "    model: str = MODEL_TOOLS,    \n",
    "    temperature: float = 0.0,\n",
    "    num_predict: int = 200,\n",
    "    max_rounds: int = 4,\n",
    "    require_tool: bool = True,\n",
    "    bn_str: str = \"\",\n",
    "    ollama_url: str = OLLAMA_URL,\n",
    "):\n",
    "    tools = [function_to_tool_schema(fn, name=name) for name, fn in fns.items()]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a tool-using Bayesian Network assistant.\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"1) Do NOT perform calculations or external actions yourself if a tool can do it.\\n\"\n",
    "        \"2) Always call a tool when any tool could plausibly answer the user.\\n\"\n",
    "        \"3) After receiving tool results, if the return value is grammatically correct, return exactly that value;\\n\"\n",
    "        \"   otherwise fix only the grammar and return the corrected value.\\n\"\n",
    "        \"4) Do NOT verify factual correctness of the tool outputs — only grammar.\\n\"\n",
    "        # \"5) If a prior tool call failed or was rejected, do NOT repeat the same tool/argument combination.\\n\"\n",
    "        # \"6) If a tool call fails, try an alternative: adjust parameters (consistent with the prompt and provided node/state names) \"\n",
    "        # \"   or choose a different tool.\\n\"\n",
    "    )\n",
    "\n",
    "    # Give the model the “catalog” of node/state names to extract from\n",
    "    prompt += (\n",
    "        \"\\n\\nFrom the query above, extract the correct parameters for the tools using these nodes and states below. If the query related to multiple nodes, \"\n",
    "        \"check for the abbreviations first (e.g., 'Tuberculosis or Cancer' can be 'TbOrCa') then use the full names:\\n\"\n",
    "        f\"{bn_str}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\":\"system\",\"content\":system_prompt}, {\"role\":\"user\",\"content\":prompt}]\n",
    "    retries_left = max_rounds\n",
    "\n",
    "    # state to avoid repeating failed/attempted calls\n",
    "    seen_calls = set()          # {(tool_name, normalized_args_json)}\n",
    "    recent_errors = []          # keep last few error messages for model guidance\n",
    "\n",
    "    while retries_left > 0:\n",
    "        r = requests.post(\n",
    "            ollama_url,\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"tools\": tools,\n",
    "                \"options\": {\"temperature\": float(temperature), \"num_predict\": int(num_predict)},\n",
    "            },\n",
    "            stream=True,\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            raise RuntimeError(f\"Ollama error {r.status_code}: {r.text}\")\n",
    "\n",
    "        assistant_msg = {\"role\":\"assistant\",\"content\":\"\", \"tool_calls\":[]}\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                chunk = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            if \"message\" in chunk:\n",
    "                m = chunk[\"message\"]\n",
    "                if m.get(\"content\"):\n",
    "                    assistant_msg[\"content\"] += m[\"content\"]\n",
    "                if \"tool_calls\" in m and m[\"tool_calls\"]:\n",
    "                    assistant_msg[\"tool_calls\"] = m[\"tool_calls\"]\n",
    "            if chunk.get(\"done\"):\n",
    "                break\n",
    "\n",
    "        messages.append(assistant_msg)\n",
    "\n",
    "        # === If the assistant requested tool calls, try to execute them ===\n",
    "        if assistant_msg[\"tool_calls\"]:\n",
    "            tool_msgs = []\n",
    "\n",
    "            for i, call in enumerate(assistant_msg[\"tool_calls\"], 1):\n",
    "                fn_name = call[\"function\"][\"name\"]\n",
    "                args = call[\"function\"].get(\"arguments\", {}) or {}\n",
    "\n",
    "                # Normalize for dedup\n",
    "                arg_key = _normalize_args_for_key(args)\n",
    "                call_key = (fn_name, arg_key)\n",
    "\n",
    "                # Skip duplicate attempts (already tried same tool+args)\n",
    "                if call_key in seen_calls:\n",
    "                    # Tell the model not to repeat this combination\n",
    "                    recent_errors.append(\n",
    "                        f\"Duplicate call blocked: {fn_name}({arg_key}) was already attempted.\"\n",
    "                    )\n",
    "                    # Synthesize a 'tool' message indicating it's blocked to keep turn structure\n",
    "                    payload = {\"error\": \"DuplicateAttempt\", \"detail\": \"This exact tool/args were already tried.\"}\n",
    "                    tool_msgs.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_name\": fn_name,\n",
    "                        \"content\": json.dumps(payload)\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                print(f\"[BayMin] tool_call #{i}: {fn_name}({args})\")\n",
    "                seen_calls.add(call_key)\n",
    "\n",
    "                # Run it\n",
    "                if fn_name not in fns:\n",
    "                    payload = {\"error\": f\"ToolNotRegistered\", \"detail\": f\"Tool '{fn_name}' not registered\"}\n",
    "                    recent_errors.append(f\"{fn_name} not registered.\")\n",
    "                else:\n",
    "                    try:\n",
    "                        out = _bind_and_call(fns[fn_name], args)\n",
    "                        try:\n",
    "                            json.dumps(out)  # ensure JSON-serializable\n",
    "                            payload = {\"result\": out}\n",
    "                        except TypeError:\n",
    "                            payload = {\"result\": repr(out)}\n",
    "                    except Exception as e:\n",
    "                        payload = {\"error\": type(e).__name__, \"detail\": str(e)}\n",
    "                        recent_errors.append(f\"{fn_name} failed with {type(e).__name__}: {e}\")\n",
    "\n",
    "                print(f\"[BayMin] tool_result #{i}: {payload}\")\n",
    "                tool_msgs.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_name\": fn_name,\n",
    "                    \"content\": json.dumps(payload)\n",
    "                })\n",
    "\n",
    "            # Attach tool results\n",
    "            messages.extend(tool_msgs)\n",
    "\n",
    "            # If any tool produced an error, ask the model to try a different tool/params (avoiding seen_calls).\n",
    "            if any(json.loads(m[\"content\"]).get(\"error\") for m in tool_msgs):\n",
    "                tried_list = [\n",
    "                    f\"{name}({args})\" for (name, args) in list(seen_calls)[-6:]   # show up to last 6\n",
    "                ]\n",
    "                err_tail = \"\\n\".join(recent_errors[-6:]) if recent_errors else \"N/A\"\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\":\n",
    "                        \"Some tool calls failed or were blocked. \"\n",
    "                        \"Do NOT repeat any of the tried combinations. \"\n",
    "                        \"Try adjusting parameters (consistent with the provided nodes/states) or choose a different tool. \"\n",
    "                        \"Only call tools that are strictly necessary.\\n\"\n",
    "                        f\"Already tried: {tried_list}\\n\"\n",
    "                        f\"Recent issues: {err_tail}\\n\"\n",
    "                        \"If you now have sufficient tool results, finalize the answer. \"\n",
    "                        \"Otherwise, pick a new approach.\"\n",
    "                })\n",
    "            else:\n",
    "                # No errors → ask the model to finalize without calling tools again unless needed\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Use the tool results above to answer the question. \"\n",
    "                               \"Do not call any tools again unless strictly necessary.\"\n",
    "                })\n",
    "\n",
    "            retries_left -= 1\n",
    "            continue\n",
    "\n",
    "        # No tool calls returned this turn\n",
    "        if assistant_msg[\"content\"].strip():\n",
    "            # Model produced direct text. If tools required, nudge once more; else return it.\n",
    "            if require_tool:\n",
    "                messages.append({\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\": (\n",
    "                        \"Reminder: you must use the available tools when they can plausibly answer. \"\n",
    "                        \"Do not answer directly. Extract parameters from the nodes/states list provided and try again. \"\n",
    "                        \"Avoid repeating any previously tried tool/argument combinations.\"\n",
    "                    ),\n",
    "                })\n",
    "                retries_left -= 1\n",
    "                continue\n",
    "            else:\n",
    "                return assistant_msg[\"content\"].strip()\n",
    "\n",
    "        # If we asked to finalize but got empty, fall back to the most recent tool JSON (raw)\n",
    "        if messages and any(m.get(\"role\") == \"tool\" for m in messages[-5:]):\n",
    "            for m in reversed(messages):\n",
    "                if m.get(\"role\") == \"tool\":\n",
    "                    return m[\"content\"]  # raw JSON string\n",
    "\n",
    "        # Last resort: ask to use tools again\n",
    "        messages.append({\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"You returned no content. Use tools with new parameters; do not repeat prior attempts.\"\n",
    "        })\n",
    "        retries_left -= 1\n",
    "\n",
    "    # After all rounds, return the latest assistant content if any\n",
    "    for m in reversed(messages):\n",
    "        if m.get(\"role\") == \"assistant\" and m.get(\"content\", \"\").strip():\n",
    "            return m[\"content\"].strip()\n",
    "    # Or the last tool output as ultimate fallback\n",
    "    for m in reversed(messages):\n",
    "        if m.get(\"role\") == \"tool\":\n",
    "            return m[\"content\"]\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ad0666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisitAsia -> ['Tuberculosis']\n",
      "Tuberculosis -> ['TbOrCa']\n",
      "Smoking -> ['Cancer', 'Bronchitis']\n",
      "Cancer -> ['TbOrCa']\n",
      "TbOrCa -> ['XRay', 'Dyspnea']\n",
      "XRay -> []\n",
      "Bronchitis -> ['Dyspnea']\n",
      "Dyspnea -> []\n",
      "VisitAsia ['visit', 'no_visit']\n",
      "Tuberculosis ['present', 'absent']\n",
      "Smoking ['smoker', 'non_smoker']\n",
      "Cancer ['present', 'absent']\n",
      "TbOrCa ['true', 'false']\n",
      "XRay ['abnormal', 'normal']\n",
      "Bronchitis ['present', 'absent']\n",
      "Dyspnea ['present', 'absent']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nets = get_nets()\n",
    "myNet = nets[1]\n",
    "\n",
    "printNet(myNet)\n",
    "print(get_BN_node_states(myNet))\n",
    "\n",
    "# for i, net in enumerate(nets):\n",
    "#   print(f\"Net {i+1}:\")\n",
    "#   printNet(net)\n",
    "#   print(get_BN_node_states(net))\n",
    "#   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785af237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== USER QUERY ===\n",
      " What is the probability of XRay given Lung Cancer, Smoking and Visit Asiaaa? \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BayMin] tool_call #1: get_prob_node_given_any_evidence({'evidence': {'Cancer': 'present', 'Smoking': 'smoker', 'VisitAsia': 'visit'}, 'node': 'XRay'})\n",
      "[BayMin] tool_result #1: {'result': {'result': 'P(XRay | Cancer=present, Smoking=smoker, VisitAsia=visit):\\n  P(XRay=abnormal) = 0.9800\\n  P(XRay=normal) = 0.0200\\n\\nOriginal distribution:\\n  P(XRay=abnormal) = 0.9800\\n  P(XRay=normal) = 0.0200\\n\\nConclusion:\\n  No change detected — the updated beliefs are identical to the original.\\n'}}\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "\n",
      "P(XRay | Cancer=present, Smoking=smoker, VisitAsia=visit):\n",
      "  P(XRay=abnormal) = 0.9800\n",
      "  P(XRay=normal) = 0.0200\n",
      "\n",
      "Original distribution:\n",
      "  P(XRay=abnormal) = 0.9800\n",
      "  P(XRay=normal) = 0.0200\n",
      "\n",
      "Conclusion:\n",
      "  No change detected — the updated beliefs are identical to the original.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_explain_d_connected_tool(net):\n",
    "    def check_d_connected(from_node: str, to_node: str) -> dict:\n",
    "        \"\"\"Explain whether two nodes are d-connected and why.\n",
    "        d-connected means that entering evidence for one node will change the probability of the other node.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            is_conn = bn_helper.is_XY_connected(net, from_node, to_node)\n",
    "            if is_conn:\n",
    "                explanation = bn_helper.get_explain_XY_dconnected(net, from_node, to_node)\n",
    "            else:\n",
    "                explanation = bn_helper.get_explain_XY_dseparated(net, from_node, to_node)\n",
    "            return explanation\n",
    "        except Exception as e:\n",
    "            return {\"d_connected\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return check_d_connected\n",
    "\n",
    "def make_explain_common_cause_tool(net):\n",
    "    def check_common_cause(node1, node2):\n",
    "        \"\"\"Check if there is a common cause between two nodes.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            ans = bn_helper.get_common_cause(net, node1, node2)\n",
    "            if ans:\n",
    "                template = f\"The common cause(s) of {node1} and {node2} is/are: {', '.join(ans)}.\"\n",
    "            else:\n",
    "                template = f\"There is no common cause between {node1} and {node2}.\"\n",
    "            return template\n",
    "        except Exception as e:\n",
    "            return {\"common_cause\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return check_common_cause\n",
    "\n",
    "def make_explain_common_effect_tool(net):\n",
    "    def check_common_effect(node1, node2):\n",
    "        \"\"\"Check if there is a common effect between two nodes.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            ans = bn_helper.get_common_effect(net, node1, node2)\n",
    "            if ans:\n",
    "                template = f\"The common effect(s) of {node1} and {node2} is/are: {', '.join(ans)}.\"\n",
    "            else:\n",
    "                template = f\"There is no common effect between {node1} and {node2}.\"\n",
    "            return template\n",
    "        except Exception as e:\n",
    "            return {\"common_effect\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return check_common_effect\n",
    "\n",
    "def get_prob_node_tool(net):\n",
    "    def get_prob_node(node):\n",
    "        \"\"\"Get the probability of a node.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            prob, _ = bn_helper.get_prob_X(net, node)\n",
    "            return prob\n",
    "        except Exception as e:\n",
    "            return {\"prob_node\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return get_prob_node\n",
    "\n",
    "def get_prob_node_given_one_evidence_tool(net):\n",
    "    def get_prob_node_given_one_evidence(node, evidence, evidence_state):\n",
    "        \"\"\"Get the probability of a node given one evidence with its state.\n",
    "        If the evidence_state is not provided, it means the evidence is True.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            prob, _ = bn_helper.get_prob_X_given_Y(net, node, evidence, evidence_state)\n",
    "            return prob\n",
    "        except Exception as e:\n",
    "            return {\"prob_node_given_one_evidence\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return get_prob_node_given_one_evidence\n",
    "\n",
    "def get_prob_node_given_two_evidences_tool(net):\n",
    "    def get_prob_node_given_two_evidences(node, evidence1, evidence1_state, evidence2, evidence2_state):\n",
    "        \"\"\"Get the probability of a node given two evidences with their states.\n",
    "        If the evidence_state is not provided, it means the evidence is True/or pick the state that means it is observed.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            prob, _ = bn_helper.get_prob_X_given_YZ(net, node, evidence1, evidence1_state, evidence2, evidence2_state)\n",
    "            return prob\n",
    "        except Exception as e:\n",
    "            return {\"prob_node_given_two_evidences\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return get_prob_node_given_two_evidences\n",
    "\n",
    "def check_one_evidence_change_relationship_between_two_nodes_tool(net):\n",
    "    def check_one_evidence_change_relationship_between_two_nodes(node1, node2, evidence):\n",
    "        \"\"\"Check if changing the Evidence of one node will change the dependency relationship between Node1 and Node2.\n",
    "        This function is used to check if the relationship between Node1 and Node2 get affected when we observe Evidence.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            ans, details = bn_helper.does_Z_change_dependency_XY(net, node1, node2, evidence)\n",
    "            template = \"\"\n",
    "\n",
    "            if ans:\n",
    "                template = (f\"Yes, observing {evidence} changes the dependency between {node1} and {node2}. \"\n",
    "                            f\"Before observing {evidence}, {node1} and {node2} were \"\n",
    "                            f\"{'d-connected' if details['before'] else 'd-separated'}. After observing {evidence}, they are \"\n",
    "                            f\"{'d-connected' if details['after'] else 'd-separated'}.\")\n",
    "            else:\n",
    "                template = (f\"No, observing {evidence} does not change the dependency between {node1} and {node2}. \"\n",
    "                            f\"Before observing {evidence}, {node1} and {node2} were \"\n",
    "                            f\"{'d-connected' if details['before'] else 'd-separated'}. After observing {evidence}, they remain \"\n",
    "                            f\"{'d-connected' if details['after'] else 'd-separated'}.\")\n",
    "            return template\n",
    "        except Exception as e:\n",
    "            return {\"check_one_evidence_change_relationship_between_two_nodes\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return check_one_evidence_change_relationship_between_two_nodes\n",
    "\n",
    "def get_evidences_block_two_nodes_tool(net):\n",
    "    def get_evidences_block_two_nodes(node1, node2):\n",
    "        \"\"\"Get the evidences list that block the dependency/path between Node1 and Node2.\"\"\"\n",
    "        try:\n",
    "            bn_helper = BnHelper()\n",
    "            ans = bn_helper.evidences_block_XY(net, node1, node2)\n",
    "            template = f\"The evidences that would block the dependency between {node1} and {node2} are: {', '.join(ans)}.\"\n",
    "            return template\n",
    "        except Exception as e:\n",
    "            return {\"get_evidences_block_two_nodes\": None, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    return get_evidences_block_two_nodes\n",
    "\n",
    "def get_prob_node_given_any_evidence_tool(net):\n",
    "    \"\"\"\n",
    "    Returns a closure that can compute probabilities of any node\n",
    "    given arbitrary evidence.\n",
    "    \n",
    "    Usage:\n",
    "        tool = get_prob_node_given_any_evidence_tool(net)\n",
    "        result = tool(\"Disease\", {\"Test\": \"Positive\", \"Exposure\": \"Yes\"})\n",
    "    \"\"\"\n",
    "    bn_helper = BnHelper()\n",
    "\n",
    "    def get_prob_node_given_any_evidence(node: str, evidence: dict = None):\n",
    "        \"\"\"Get probability distribution of a node given any evidence dict.\"\"\"\n",
    "        try:\n",
    "            prob_str, _ = bn_helper.get_prob_X_given(net, node, evidence)\n",
    "            return {\"result\": prob_str}\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"result\": None,\n",
    "                \"error\": f\"{type(e).__name__}: {e}\"\n",
    "            }\n",
    "\n",
    "    return get_prob_node_given_any_evidence\n",
    "\n",
    "tools_map = {\n",
    "    \"check_d_connected\": make_explain_d_connected_tool(myNet),\n",
    "    \"check_common_cause\": make_explain_common_cause_tool(myNet),\n",
    "    \"check_common_effect\": make_explain_common_effect_tool(myNet),\n",
    "    \"get_prob_node\": get_prob_node_tool(myNet),\n",
    "    \"get_prob_node_given_any_evidence\": get_prob_node_given_any_evidence_tool(myNet),\n",
    "    # \"get_prob_node_given_one_evidence\": get_prob_node_given_one_evidence_tool(myNet),\n",
    "    # \"get_prob_node_given_two_evidences\": get_prob_node_given_two_evidences_tool(myNet),\n",
    "    \"check_one_evidence_change_relationship_between_two_nodes\": check_one_evidence_change_relationship_between_two_nodes_tool(myNet),\n",
    "    \"get_evidences_block_two_nodes\": get_evidences_block_two_nodes_tool(myNet),\n",
    "}\n",
    "\n",
    "question = (\n",
    "    # \"Is Visiting Asia change the probability of Smoking?\"\n",
    "    # \"Is changing the evidence of A going to change the probability of B?\"\n",
    "    # \"What is the common effect of C and B?\"\n",
    "    \"What is the probability of XRay given Lung Cancer, Smoking and Visit Asiaaa?\"\n",
    "    # \"What is the probability of A given B is increased and C is present?\"\n",
    "    # \"Is the relationship between Vizit Azia and Lung Cancr get affected when we observe Tuberculosis or Cancer?\"\n",
    "    # \"What set of evidences would block the path between B and C?\"\n",
    ")\n",
    "\n",
    "bn_str = get_BN_node_states(myNet)\n",
    "\n",
    "\n",
    "print(\"\\n=== USER QUERY ===\\n\", question, \"\\n\")\n",
    "\n",
    "answer = chat_with_tools(\n",
    "    prompt=question,\n",
    "    fns=tools_map,\n",
    "    model=MODEL_TOOLS,\n",
    "    require_tool=True,    \n",
    "    temperature=0.0,\n",
    "    num_predict=300,\n",
    "    max_rounds=4,\n",
    "    bn_str=bn_str,\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel\n",
    "class StructureAnswer(BaseModel):\n",
    "    result: str\n",
    "\n",
    "print(\"\\n=== FINAL ANSWER ===\\n\")\n",
    "# validated_answer = StructureAnswer.model_validate_json(answer)\n",
    "# print(validated_answer.result)\n",
    "import json\n",
    "\n",
    "def extract_text(answer: str) -> str:\n",
    "    try:\n",
    "        obj = json.loads(answer)\n",
    "        if isinstance(obj, dict):\n",
    "            if \"result\" in obj[\"result\"]:\n",
    "                return obj[\"result\"][\"result\"]\n",
    "            if \"result\" in obj:\n",
    "                return obj[\"result\"]\n",
    "            if \"error\" in obj:\n",
    "                return f\"Error: {obj['error']}: {obj.get('detail','')}\".strip()\n",
    "        return answer\n",
    "    except json.JSONDecodeError:\n",
    "        return answer\n",
    "\n",
    "print(extract_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bad3e340-3732-4cbd-92c3-4a1c98cf1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"add\"}\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to function successfully\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL = \"gpt-oss-bn-json\"\n",
    "def answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None):\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": 50, # only when stream = False work\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Send the POST request with streaming enabled\n",
    "    with requests.post(endpoint, headers=headers, json=payload, stream=True) as response:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Process the response incrementally\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        response_json = json.loads(line)\n",
    "                        chunk = response_json.get(\"response\", \"\")\n",
    "                        full_response += chunk\n",
    "                        \n",
    "                        # Render the response as Markdown\n",
    "                        if stream:\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(full_response))\n",
    "                        \n",
    "                return full_response\n",
    "            except json.JSONDecodeError as e:\n",
    "                return \"Failed to parse JSON: \" + str(e)\n",
    "        else:\n",
    "            return \"Failed to retrieve response: \" + str(response.status_code)\n",
    "\n",
    "class BnHelpers(BaseModel):\n",
    "    fnName: str\n",
    "\n",
    "def add(a=5, b=6):\n",
    "    print('Go to function successfully')\n",
    "    return a + b\n",
    "\n",
    "output = answer_this_prompt('output this function name: add', stream=True, format=BnHelpers.model_json_schema())\n",
    "\n",
    "bnHelpers = BnHelpers.model_validate_json(output)\n",
    "if bnHelpers.fnName == 'add':\n",
    "    print(add())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2e5a36-7719-4e9d-a231-84c79e94b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"isConnected\"}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "def isConnected(net, fromNode, toNode):\n",
    "  relatedNodes = net.node(fromNode).getRelated(\"d_connected\")\n",
    "  for node in relatedNodes:\n",
    "    if node.name() == toNode:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "PROMPT = \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "fromNode = 'Alarm'\n",
    "toNode = 'Fire'\n",
    "\n",
    "PROMPT = PROMPT.format(BN=BN, fromNode=fromNode, toNode=toNode)\n",
    "inputPrompt = PROMPT + 'if user ask anything related to are these two nodes connected to each other, output this function name: isConnected'\n",
    "output2 = answer_this_prompt(inputPrompt, stream=True, format=BnHelpers.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}?\"\"\",\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}? What are the two nodes mentioned?\"\"\",\n",
    "    \"Within the Bayesian Network {BN}, does a path exist from {fromNode} to {toNode}?\",\n",
    "    \"In the graph {BN}, can information flow from {fromNode} to {toNode}?\", # top perform \n",
    "    \"Are {fromNode} and {toNode} dependent in the Bayesian Network {BN}?\",\n",
    "    \"In {BN}, is there any direct or indirect connection between {fromNode} and {toNode}?\",\n",
    "    \"Can {fromNode} influence {toNode} in the Bayesian Network {BN}?\",\n",
    "    \"Is {toNode} reachable from {fromNode} in the structure of {BN}?\",\n",
    "    \"Does {BN} contain a path that links {fromNode} to {toNode}?\",\n",
    "    \"Are there any edges—direct or through other nodes—connecting {fromNode} and {toNode} in {BN}?\",\n",
    "    \"Is {toNode} conditionally dependent on {fromNode} in the Bayesian Network {BN}?\",\n",
    "    \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]\n",
    "\n",
    "for question in questions:\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  print(f\"Question: {question.format(BN=net.name(), fromNode=fromNode, toNode=toNode)}\")\n",
    "  for net in listOfNets:\n",
    "      for _ in range(5):\n",
    "        total += 1\n",
    "        fromNode, toNode = pickTwoRandomNodes(net)\n",
    "        if fromNode and toNode:\n",
    "            \n",
    "            correctIdentified, queryFromNode, queryToNode = correctIdentification(question, net, fromNode, toNode)\n",
    "            if correctIdentified:\n",
    "              correct += 1\n",
    "            else:\n",
    "              print(f\"Incorrect identification for {net.name()}\")\n",
    "              printNet(net)\n",
    "              print()\n",
    "              print(\"Expected:\", fromNode, \"->\", toNode)\n",
    "              print(\"Reality:\", queryFromNode, \"->\", queryToNode)\n",
    "              print(\"----------------------------------------------------\")\n",
    "\n",
    "  print(f\"Total: {total}, Correct: {correct}, Accuracy: {correct/total:.2%}\")\n",
    "  print(\"<------------------------------------------------------------------------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bni_netica.support_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "from bni_netica.bn_helpers import BnHelper, QueryTwoNodes, ParamExtractor\n",
    "from ollama.prompt import answer_this_prompt\n",
    "from bni_netica.scripts import HELLO_SCRIPT, MENU_SCRIPT, GET_FN_SCRIPT\n",
    "\n",
    "# PROMPT = \"\"\"Consider this question: '{question}'. \n",
    "# What are the two nodes in this question? \n",
    "# Make sure to correctly output the names of nodes exactly as mentioned in the network and in the order as the question mentioned. \n",
    "# For example, if the question mentioned \"A and B\" then the two nodes are fromNode: A, toNode: B; or if the question mentioned \"Smoking and Cancer\" then the two nodes are fromNode: Smoking, toNode: Cancer. \n",
    "# Answer in JSON format.\"\"\"\n",
    "\n",
    "def query_menu(BN_string, net):\n",
    "    \"\"\"Input: BN: string, net: object\"\"\"\n",
    "    pre_query = f\"\"\"In this Bayesian Network: \n",
    "{BN_string}\n",
    "\"\"\"\n",
    "    user_query = input(\"Enter your query here: \")\n",
    "    get_fn_prompt = pre_query + \"\\n\" + user_query + GET_FN_SCRIPT\n",
    "\n",
    "    get_fn = answer_this_prompt(get_fn_prompt, format=BnHelper.model_json_schema())\n",
    "    print(\"\\nBayMin:\")\n",
    "    print(get_fn)\n",
    "\n",
    "    get_fn = BnHelper.model_validate_json(get_fn)\n",
    "    fn = get_fn.function_name\n",
    "\n",
    "    bn_helper = BnHelper(function_name=fn)\n",
    "    param_extractor = ParamExtractor()\n",
    "    \n",
    "    if fn == \"is_XY_connected\":\n",
    "        \n",
    "        get_params = param_extractor.extract_two_nodes_from_query(pre_query, user_query)\n",
    "        print(get_params)\n",
    "\n",
    "        ans = bn_helper.is_XY_connected(net, get_params.from_node, get_params.to_node)\n",
    "\n",
    "        if ans:\n",
    "            template = f\"Yes, {get_params.from_node} is d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would change the probability of {get_params.to_node} and vice versa.\"\n",
    "        else:\n",
    "            template = f\"No, {get_params.from_node} is not d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would not change the probability of {get_params.to_node}.\"\n",
    "        \n",
    "        explain_prompt = f\"\"\"User asked: In this '{BN_string}', '{user_query}'. We use {fn} function and the output is: '{ans}'. Follow this exact template to provide the answer: '{template}'.\"\"\"\n",
    "        print(answer_this_prompt(explain_prompt))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(MENU_SCRIPT)\n",
    "    choice = int(input(\"Enter your choice: \"))\n",
    "    print()\n",
    "\n",
    "    if choice == 1:\n",
    "        input(\"Enter your query here: \")\n",
    "        print('This is a sample answer.\\n')\n",
    "    elif choice == 2:\n",
    "        input(\"Enter your query here: \")\n",
    "        print('This is a sample answer.\\n')\n",
    "    elif choice == 3:\n",
    "        print(\"Not yet implemented\\n\")\n",
    "        return \n",
    "    elif choice == 4:\n",
    "        print(\"Goodbye!\\n\")\n",
    "        return    \n",
    "\n",
    "def main():\n",
    "    print(HELLO_SCRIPT)\n",
    "    nets = get_nets()\n",
    "\n",
    "    \n",
    "    for i, net in enumerate(nets):\n",
    "        print(f\"{i}: {net.name()}\")\n",
    "\n",
    "    print()\n",
    "    choice = int(input(\"Enter the number of the network you want to use: \"))\n",
    "    print()\n",
    "    if choice < 0 or choice >= len(nets):\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    net = nets[choice]\n",
    "    print(f\"You chose: {net.name()}\")\n",
    "    printNet(net)\n",
    "    print('\\nBN states:\\n')\n",
    "    print(get_BN_node_states(net))\n",
    "\n",
    "    BN_string = get_BN_structure(net)\n",
    "    query_menu(BN_string=BN_string, net=net)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
