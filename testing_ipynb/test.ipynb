{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30f2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"answer\": \"[A, C]\" }\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # go up one folder\n",
    "\n",
    "import requests, json\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from ollama.prompt import answer_this_prompt, generate_chat\n",
    "from bn_helpers.bn_helpers import AnswerStructure, BnHelper\n",
    "\n",
    "# print(generate_chat(\"Print [A, C] with no additional text\", model=\"qwen2.5:3b\", num_predict=5))\n",
    "print(answer_this_prompt(\"Print [A, C] with no additional text\", model=\"qwen2.5:7b\", format=AnswerStructure.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0623a67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 + 58 = **100**\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "endpoint = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"add_numbers\",\n",
    "        \"description\": \"Add two numbers together\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": { \"a\": {\"type\": \"number\"}, \"b\": {\"type\": \"number\"} },\n",
    "            \"required\": [\"a\",\"b\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# 1) Ask with tools\n",
    "r = requests.post(endpoint, json={\n",
    "    \"model\": \"gpt-oss:latest\",    # or llama3.1:tools / qwen3:tools, etc.\n",
    "    \"messages\": [ {\"role\":\"user\",\"content\":\"What is 42 + 58?\"} ],\n",
    "    \"tools\": tools\n",
    "}, stream=True)\n",
    "\n",
    "assistant_msg = {\"role\":\"assistant\",\"content\":\"\",\"tool_calls\":[]}\n",
    "for line in r.iter_lines(decode_unicode=True):\n",
    "    if not line: \n",
    "        continue\n",
    "    chunk = json.loads(line)\n",
    "    if \"message\" in chunk:\n",
    "        # accumulate any content (often empty with thinking models)\n",
    "        assistant_msg[\"content\"] += chunk[\"message\"].get(\"content\",\"\")\n",
    "        # capture tool calls if/when they arrive\n",
    "        if \"tool_calls\" in chunk[\"message\"]:\n",
    "            assistant_msg[\"tool_calls\"] = chunk[\"message\"][\"tool_calls\"]\n",
    "    if chunk.get(\"done\"):\n",
    "        break\n",
    "\n",
    "# 2) Execute the tool(s)\n",
    "if not assistant_msg[\"tool_calls\"]:\n",
    "    raise RuntimeError(\"No tool calls returned\")\n",
    "\n",
    "call = assistant_msg[\"tool_calls\"][0]\n",
    "fn_name = call[\"function\"][\"name\"]\n",
    "args = call[\"function\"][\"arguments\"]\n",
    "res = args[\"a\"] + args[\"b\"]  # run your real function here\n",
    "\n",
    "# 3) Send result back with role:\"tool\" and tool_name (NO tool_call_id)\n",
    "messages = [\n",
    "    {\"role\":\"user\", \"content\":\"What is 42 + 58?\"},\n",
    "    assistant_msg,  # include the assistant turn that asked for the tool\n",
    "    {\"role\":\"tool\", \"tool_name\": fn_name, \"content\": json.dumps({\"sum\": res})}\n",
    "]\n",
    "\n",
    "r2 = requests.post(endpoint, json={\n",
    "    \"model\": \"gpt-oss:latest\",\n",
    "    \"messages\": messages\n",
    "}, stream=True)\n",
    "\n",
    "final_text = \"\"\n",
    "for line in r2.iter_lines(decode_unicode=True):\n",
    "    if not line: \n",
    "        continue\n",
    "    chunk = json.loads(line)\n",
    "    if \"message\" in chunk:\n",
    "        final_text += chunk[\"message\"].get(\"content\",\"\")\n",
    "    if chunk.get(\"done\"):\n",
    "        break\n",
    "\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bad3e340-3732-4cbd-92c3-4a1c98cf1a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"add\"}\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to function successfully\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "MODEL = \"gpt-oss-bn-json\"\n",
    "def answer_this_prompt(prompt, stream=False, model=MODEL, temperature=0, format=None):\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": 50, # only when stream = False work\n",
    "        \"format\": format\n",
    "    }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    endpoint = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Send the POST request with streaming enabled\n",
    "    with requests.post(endpoint, headers=headers, json=payload, stream=True) as response:\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Process the response incrementally\n",
    "                full_response = \"\"\n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    if line.strip():  # Skip empty lines\n",
    "                        response_json = json.loads(line)\n",
    "                        chunk = response_json.get(\"response\", \"\")\n",
    "                        full_response += chunk\n",
    "                        \n",
    "                        # Render the response as Markdown\n",
    "                        if stream:\n",
    "                            clear_output(wait=True)\n",
    "                            display(Markdown(full_response))\n",
    "                        \n",
    "                return full_response\n",
    "            except json.JSONDecodeError as e:\n",
    "                return \"Failed to parse JSON: \" + str(e)\n",
    "        else:\n",
    "            return \"Failed to retrieve response: \" + str(response.status_code)\n",
    "\n",
    "class BnHelpers(BaseModel):\n",
    "    fnName: str\n",
    "\n",
    "def add(a=5, b=6):\n",
    "    print('Go to function successfully')\n",
    "    return a + b\n",
    "\n",
    "output = answer_this_prompt('output this function name: add', stream=True, format=BnHelpers.model_json_schema())\n",
    "\n",
    "bnHelpers = BnHelpers.model_validate_json(output)\n",
    "if bnHelpers.fnName == 'add':\n",
    "    print(add())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2e5a36-7719-4e9d-a231-84c79e94b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"fnName\":\"isConnected\"}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "def isConnected(net, fromNode, toNode):\n",
    "  relatedNodes = net.node(fromNode).getRelated(\"d_connected\")\n",
    "  for node in relatedNodes:\n",
    "    if node.name() == toNode:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "\n",
    "BN = \"\"\n",
    "for node in FireNet.nodes():\n",
    "    BN += f\"{node.name()} -> {[child.name() for child in node.children()]}\\n\"\n",
    "\n",
    "PROMPT = \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "fromNode = 'Alarm'\n",
    "toNode = 'Fire'\n",
    "\n",
    "PROMPT = PROMPT.format(BN=BN, fromNode=fromNode, toNode=toNode)\n",
    "inputPrompt = PROMPT + 'if user ask anything related to are these two nodes connected to each other, output this function name: isConnected'\n",
    "output2 = answer_this_prompt(inputPrompt, stream=True, format=BnHelpers.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}?\"\"\",\n",
    "    \"\"\"In this Bayesian Networks: {BN}, is {fromNode} connected to {toNode}? What are the two nodes mentioned?\"\"\",\n",
    "    \"Within the Bayesian Network {BN}, does a path exist from {fromNode} to {toNode}?\",\n",
    "    \"In the graph {BN}, can information flow from {fromNode} to {toNode}?\", # top perform \n",
    "    \"Are {fromNode} and {toNode} dependent in the Bayesian Network {BN}?\",\n",
    "    \"In {BN}, is there any direct or indirect connection between {fromNode} and {toNode}?\",\n",
    "    \"Can {fromNode} influence {toNode} in the Bayesian Network {BN}?\",\n",
    "    \"Is {toNode} reachable from {fromNode} in the structure of {BN}?\",\n",
    "    \"Does {BN} contain a path that links {fromNode} to {toNode}?\",\n",
    "    \"Are there any edges—direct or through other nodes—connecting {fromNode} and {toNode} in {BN}?\",\n",
    "    \"Is {toNode} conditionally dependent on {fromNode} in the Bayesian Network {BN}?\",\n",
    "    \"Within {BN}, is {fromNode} an ancestor of {toNode}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]\n",
    "\n",
    "for question in questions:\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  print(f\"Question: {question.format(BN=net.name(), fromNode=fromNode, toNode=toNode)}\")\n",
    "  for net in listOfNets:\n",
    "      for _ in range(5):\n",
    "        total += 1\n",
    "        fromNode, toNode = pickTwoRandomNodes(net)\n",
    "        if fromNode and toNode:\n",
    "            \n",
    "            correctIdentified, queryFromNode, queryToNode = correctIdentification(question, net, fromNode, toNode)\n",
    "            if correctIdentified:\n",
    "              correct += 1\n",
    "            else:\n",
    "              print(f\"Incorrect identification for {net.name()}\")\n",
    "              printNet(net)\n",
    "              print()\n",
    "              print(\"Expected:\", fromNode, \"->\", toNode)\n",
    "              print(\"Reality:\", queryFromNode, \"->\", queryToNode)\n",
    "              print(\"----------------------------------------------------\")\n",
    "\n",
    "  print(f\"Total: {total}, Correct: {correct}, Accuracy: {correct/total:.2%}\")\n",
    "  print(\"<------------------------------------------------------------------------->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bni_netica.support_tools import get_nets, printNet, get_BN_structure, get_BN_node_states\n",
    "from bni_netica.bn_helpers import BnHelper, QueryTwoNodes, ParamExtractor\n",
    "from ollama.prompt import answer_this_prompt\n",
    "from bni_netica.scripts import HELLO_SCRIPT, MENU_SCRIPT, GET_FN_SCRIPT\n",
    "\n",
    "# PROMPT = \"\"\"Consider this question: '{question}'. \n",
    "# What are the two nodes in this question? \n",
    "# Make sure to correctly output the names of nodes exactly as mentioned in the network and in the order as the question mentioned. \n",
    "# For example, if the question mentioned \"A and B\" then the two nodes are fromNode: A, toNode: B; or if the question mentioned \"Smoking and Cancer\" then the two nodes are fromNode: Smoking, toNode: Cancer. \n",
    "# Answer in JSON format.\"\"\"\n",
    "\n",
    "def query_menu(BN_string, net):\n",
    "    \"\"\"Input: BN: string, net: object\"\"\"\n",
    "    pre_query = f\"\"\"In this Bayesian Network: \n",
    "{BN_string}\n",
    "\"\"\"\n",
    "    user_query = input(\"Enter your query here: \")\n",
    "    get_fn_prompt = pre_query + \"\\n\" + user_query + GET_FN_SCRIPT\n",
    "\n",
    "    get_fn = answer_this_prompt(get_fn_prompt, format=BnHelper.model_json_schema())\n",
    "    print(\"\\nBayMin:\")\n",
    "    print(get_fn)\n",
    "\n",
    "    get_fn = BnHelper.model_validate_json(get_fn)\n",
    "    fn = get_fn.function_name\n",
    "\n",
    "    bn_helper = BnHelper(function_name=fn)\n",
    "    param_extractor = ParamExtractor()\n",
    "    \n",
    "    if fn == \"is_XY_connected\":\n",
    "        \n",
    "        get_params = param_extractor.extract_two_nodes_from_query(pre_query, user_query)\n",
    "        print(get_params)\n",
    "\n",
    "        ans = bn_helper.is_XY_connected(net, get_params.from_node, get_params.to_node)\n",
    "\n",
    "        if ans:\n",
    "            template = f\"Yes, {get_params.from_node} is d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would change the probability of {get_params.to_node} and vice versa.\"\n",
    "        else:\n",
    "            template = f\"No, {get_params.from_node} is not d-connected to {get_params.to_node}, which means that entering evidence for {get_params.from_node} would not change the probability of {get_params.to_node}.\"\n",
    "        \n",
    "        explain_prompt = f\"\"\"User asked: In this '{BN_string}', '{user_query}'. We use {fn} function and the output is: '{ans}'. Follow this exact template to provide the answer: '{template}'.\"\"\"\n",
    "        print(answer_this_prompt(explain_prompt))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(MENU_SCRIPT)\n",
    "    choice = int(input(\"Enter your choice: \"))\n",
    "    print()\n",
    "\n",
    "    if choice == 1:\n",
    "        input(\"Enter your query here: \")\n",
    "        print('This is a sample answer.\\n')\n",
    "    elif choice == 2:\n",
    "        input(\"Enter your query here: \")\n",
    "        print('This is a sample answer.\\n')\n",
    "    elif choice == 3:\n",
    "        print(\"Not yet implemented\\n\")\n",
    "        return \n",
    "    elif choice == 4:\n",
    "        print(\"Goodbye!\\n\")\n",
    "        return    \n",
    "\n",
    "def main():\n",
    "    print(HELLO_SCRIPT)\n",
    "    nets = get_nets()\n",
    "\n",
    "    \n",
    "    for i, net in enumerate(nets):\n",
    "        print(f\"{i}: {net.name()}\")\n",
    "\n",
    "    print()\n",
    "    choice = int(input(\"Enter the number of the network you want to use: \"))\n",
    "    print()\n",
    "    if choice < 0 or choice >= len(nets):\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    net = nets[choice]\n",
    "    print(f\"You chose: {net.name()}\")\n",
    "    printNet(net)\n",
    "    print('\\nBN states:\\n')\n",
    "    print(get_BN_node_states(net))\n",
    "\n",
    "    BN_string = get_BN_structure(net)\n",
    "    query_menu(BN_string=BN_string, net=net)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
