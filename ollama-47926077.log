time=2025-10-17T16:16:25.374+11:00 level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/mvo1/lb64_scratch/ollama-models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-10-17T16:16:25.516+11:00 level=INFO source=images.go:477 msg="total blobs: 31"
time=2025-10-17T16:16:25.523+11:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-10-17T16:16:25.529+11:00 level=INFO source=routes.go:1357 msg="Listening on 127.0.0.1:11434 (version 0.11.4)"
time=2025-10-17T16:16:25.529+11:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-10-17T16:16:25.949+11:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af library=cuda variant=v12 compute=8.6 driver=12.2 name="NVIDIA A40" total="44.4 GiB" available="44.1 GiB"
[GIN] 2025/10/17 - 16:16:28 | 200 |  484.414621ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-17T16:21:06.087+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340847104 required="21.3 GiB"
time=2025-10-17T16:21:06.255+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="923.3 GiB" free_swap="0 B"
time=2025-10-17T16:21:06.255+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T16:21:06.314+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 35623"
time=2025-10-17T16:21:06.314+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:21:06.314+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:21:06.328+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:21:06.328+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35623"
time=2025-10-17T16:21:06.331+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:21:06.395+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-17T16:21:06.582+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:21:15.683+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:21:16.242+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:21:16.242+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:21:16.242+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:21:16.242+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:21:16.242+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:21:16.251+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T16:21:16.251+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:21:35.154+11:00 level=INFO source=server.go:637 msg="llama runner started in 28.84 seconds"
[GIN] 2025/10/17 - 16:21:52 | 200 | 47.434838608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:23:30 | 200 |  13.04035577s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:23:38 | 200 |  8.742342139s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:23:49 | 200 | 10.208568472s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:24:02 | 200 | 13.460884691s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T16:24:13.906+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T16:24:14.069+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="922.2 GiB" free_swap="0 B"
time=2025-10-17T16:24:14.069+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T16:24:14.132+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43775"
time=2025-10-17T16:24:14.132+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:24:14.132+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:24:14.144+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:24:14.144+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43775"
time=2025-10-17T16:24:14.160+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:24:14.225+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:24:14.328+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:24:14.411+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:24:14.437+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:24:14.437+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:24:14.437+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:24:14.437+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:24:14.437+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:24:14.444+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T16:24:14.444+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:24:18.233+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.10 seconds"
[GIN] 2025/10/17 - 16:24:25 | 200 | 12.232000261s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:24:31 | 200 |  6.408100569s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:24:35 | 200 |  4.160583143s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T16:33:46.000+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T16:33:46.164+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="922.2 GiB" free_swap="0 B"
time=2025-10-17T16:33:46.164+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T16:33:46.234+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43723"
time=2025-10-17T16:33:46.234+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:33:46.234+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:33:46.249+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:33:46.249+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:33:46.249+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43723"
time=2025-10-17T16:33:46.316+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:33:46.408+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:33:46.500+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:33:46.518+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:33:46.518+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:33:46.518+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:33:46.518+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:33:46.518+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:33:46.527+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T16:33:46.527+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:33:50.283+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 16:34:03 | 200 | 17.602141283s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:38:55 | 200 | 17.723068129s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:39:04 | 200 |  9.683685648s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:39:08 | 200 |  3.325347111s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:40:39 | 200 |  7.709118291s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T16:40:40.671+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340847104 required="14.9 GiB"
time=2025-10-17T16:40:40.852+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.3 GiB" free_swap="0 B"
time=2025-10-17T16:40:40.852+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T16:40:40.920+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 36581"
time=2025-10-17T16:40:40.920+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:40:40.920+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:40:40.932+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:40:40.933+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:40:40.933+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36581"
time=2025-10-17T16:40:40.990+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:40:41.073+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:40:41.178+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:40:41.178+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:40:41.178+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:40:41.178+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:40:41.179+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:40:41.184+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:40:41.187+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T16:40:41.187+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:40:44.976+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 16:40:51 | 200 | 11.753795082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:40:56 | 200 |  4.910394653s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:40:57 | 200 |  999.622814ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:41:01 | 200 |  4.077157912s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T16:41:24.734+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T16:41:24.899+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.2 GiB" free_swap="0 B"
time=2025-10-17T16:41:24.899+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T16:41:24.960+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43289"
time=2025-10-17T16:41:24.961+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:41:24.961+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:41:24.980+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:41:24.973+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:41:24.973+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43289"
time=2025-10-17T16:41:25.036+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:41:25.127+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:41:25.231+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:41:25.235+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:41:25.235+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:41:25.235+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:41:25.235+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:41:25.235+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:41:25.244+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T16:41:25.244+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:41:28.771+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.81 seconds"
[GIN] 2025/10/17 - 16:41:34 | 200 | 10.631248658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:41:50 | 200 |   4.97212653s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:41:56 | 200 |   6.23617892s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:42:02 | 200 |  6.252273015s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:42:12 | 200 |  9.574122598s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T16:42:13.674+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T16:42:13.837+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.2 GiB" free_swap="0 B"
time=2025-10-17T16:42:13.837+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T16:42:13.905+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 33661"
time=2025-10-17T16:42:13.906+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:42:13.906+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:42:13.925+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:42:13.918+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:42:13.918+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33661"
time=2025-10-17T16:42:13.983+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:42:14.063+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:42:14.176+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:42:14.187+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:42:14.187+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:42:14.187+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:42:14.187+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:42:14.187+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:42:14.194+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T16:42:14.194+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:42:17.712+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.81 seconds"
[GIN] 2025/10/17 - 16:42:22 | 200 | 10.222508543s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:23 | 200 |  1.202239222s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:29 | 200 |  5.673011484s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:30 | 200 |  921.971441ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:36 | 200 |  5.862742291s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:37 | 200 |  1.480927304s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:42:46 | 200 |  8.505847332s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T16:44:56.517+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T16:44:56.681+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.3 GiB" free_swap="0 B"
time=2025-10-17T16:44:56.682+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T16:44:56.750+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43325"
time=2025-10-17T16:44:56.750+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:44:56.750+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:44:56.770+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:44:56.762+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:44:56.763+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43325"
time=2025-10-17T16:44:56.826+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:44:56.916+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:44:57.020+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:44:57.022+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:44:57.022+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:44:57.022+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:44:57.022+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:44:57.022+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:44:57.031+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T16:44:57.032+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:45:00.546+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.80 seconds"
[GIN] 2025/10/17 - 16:45:09 | 200 |  13.63550894s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:46:23 | 200 |  9.966948108s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:46:28 | 200 |  4.541923633s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:46:39 | 200 | 11.050263619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:46:55 | 200 |  4.541431717s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T16:46:56.266+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T16:46:56.430+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.3 GiB" free_swap="0 B"
time=2025-10-17T16:46:56.430+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T16:46:56.481+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 38873"
time=2025-10-17T16:46:56.481+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:46:56.481+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:46:56.493+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:46:56.494+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38873"
time=2025-10-17T16:46:56.504+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:46:56.573+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:46:56.653+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:46:56.756+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:46:56.764+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:46:56.764+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:46:56.764+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:46:56.765+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:46:56.765+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:46:56.772+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T16:46:56.772+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:47:00.332+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.85 seconds"
[GIN] 2025/10/17 - 16:47:06 | 200 | 11.376502533s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:47:12 | 200 |  6.298325811s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:47:13 | 200 |  975.096321ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:47:22 | 200 |  9.107624472s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:47:24 | 200 |  1.487791384s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:47:37 | 200 |  12.64636671s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T16:52:38.337+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T16:52:38.503+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.4 GiB" free_swap="0 B"
time=2025-10-17T16:52:38.503+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T16:52:38.570+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 33829"
time=2025-10-17T16:52:38.570+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:52:38.570+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:52:38.580+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:52:38.582+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:52:38.582+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33829"
time=2025-10-17T16:52:38.646+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:52:38.737+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:52:38.832+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:52:38.845+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:52:38.845+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:52:38.845+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:52:38.845+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:52:38.845+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:52:38.854+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T16:52:38.854+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:52:42.390+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.82 seconds"
[GIN] 2025/10/17 - 16:52:53 | 200 | 15.131851531s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:55:49 | 200 | 26.609268093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:55:55 | 200 |  5.497599023s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:56:01 | 200 |  5.961852312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 16:56:16 | 200 |  4.054843314s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T16:56:18.247+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T16:56:18.414+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.4 GiB" free_swap="0 B"
time=2025-10-17T16:56:18.414+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T16:56:18.482+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45475"
time=2025-10-17T16:56:18.482+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T16:56:18.482+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T16:56:18.501+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T16:56:18.494+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T16:56:18.495+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45475"
time=2025-10-17T16:56:18.553+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T16:56:18.631+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T16:56:18.737+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T16:56:18.737+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T16:56:18.737+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T16:56:18.737+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T16:56:18.737+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T16:56:18.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T16:56:18.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T16:56:18.763+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T16:56:22.313+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.83 seconds"
[GIN] 2025/10/17 - 16:56:44 | 200 | 27.429141858s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:56:50 | 200 |   6.54517775s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:56:51 | 200 |  992.249957ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:56:55 | 200 |  3.511762731s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:56:56 | 200 |    865.4019ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:57:04 | 200 |    8.2901697s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 16:57:08 | 200 |  4.008355802s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T17:02:02.128+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T17:02:02.297+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.4 GiB" free_swap="0 B"
time=2025-10-17T17:02:02.297+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T17:02:02.365+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42789"
time=2025-10-17T17:02:02.366+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:02:02.366+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:02:02.379+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:02:02.380+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:02:02.381+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42789"
time=2025-10-17T17:02:02.444+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:02:02.522+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:02:02.630+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:02:02.631+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:02:02.631+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:02:02.631+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:02:02.631+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:02:02.631+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:02:02.640+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T17:02:02.641+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:02:06.177+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.81 seconds"
[GIN] 2025/10/17 - 17:02:11 | 200 | 10.419392319s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:02:21 | 200 |   9.61475936s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:02:29 | 200 |  8.269323667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:05:04 | 200 |  8.932772234s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:05:11 | 200 |  6.912071635s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T17:05:12.526+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T17:05:12.697+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.3 GiB" free_swap="0 B"
time=2025-10-17T17:05:12.698+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T17:05:12.767+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42733"
time=2025-10-17T17:05:12.767+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:05:12.767+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:05:12.786+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:05:12.779+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:05:12.779+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42733"
time=2025-10-17T17:05:12.837+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:05:12.916+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:05:13.024+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:05:13.024+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:05:13.024+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:05:13.024+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:05:13.024+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:05:13.031+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T17:05:13.032+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:05:13.047+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:05:16.591+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.82 seconds"
[GIN] 2025/10/17 - 17:05:21 | 200 | 10.442497526s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:05:27 | 200 |  5.736097388s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:05:32 | 200 |  4.755878869s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T17:13:43.787+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T17:13:43.954+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="927.4 GiB" free_swap="0 B"
time=2025-10-17T17:13:43.954+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T17:13:44.019+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43643"
time=2025-10-17T17:13:44.020+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:13:44.020+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:13:44.031+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:13:44.033+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:13:44.033+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43643"
time=2025-10-17T17:13:44.098+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:13:44.190+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:13:44.282+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:13:44.298+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:13:44.298+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:13:44.298+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:13:44.298+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:13:44.298+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:13:44.307+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T17:13:44.307+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:13:47.818+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.80 seconds"
[GIN] 2025/10/17 - 17:13:57 | 200 | 13.737330419s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:18:33 | 200 | 31.732816136s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:18:44 | 200 | 10.108268957s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:18:50 | 200 |  6.487485487s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:21:24 | 200 | 21.555222672s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T17:21:25.472+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T17:21:25.637+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="963.1 GiB" free_swap="0 B"
time=2025-10-17T17:21:25.637+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T17:21:25.701+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42333"
time=2025-10-17T17:21:25.702+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:21:25.702+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:21:25.714+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:21:25.714+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:21:25.715+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42333"
time=2025-10-17T17:21:25.771+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-17T17:21:25.965+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:21:29.927+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:21:30.056+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:21:30.056+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:21:30.056+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:21:30.056+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:21:30.056+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:21:30.064+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T17:21:30.064+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:21:46.779+11:00 level=INFO source=server.go:637 msg="llama runner started in 21.08 seconds"
[GIN] 2025/10/17 - 17:21:52 | 200 | 28.299045287s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:21:55 | 200 |   931.58698ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:22:01 | 200 |  6.481450369s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:22:08 | 200 |  6.964195038s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T17:31:41.755+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T17:31:41.937+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="963.2 GiB" free_swap="0 B"
time=2025-10-17T17:31:41.938+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T17:31:42.036+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 33641"
time=2025-10-17T17:31:42.040+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:31:42.041+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:31:42.065+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:31:42.066+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33641"
time=2025-10-17T17:31:42.079+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:31:42.128+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:31:51.641+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:31:51.952+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:31:51.952+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:31:51.952+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:31:51.952+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:31:51.952+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:31:51.961+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T17:31:51.961+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:32:07.655+11:00 level=INFO source=server.go:637 msg="llama runner started in 25.61 seconds"
[GIN] 2025/10/17 - 17:32:26 | 200 | 46.353863316s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:37:34 | 200 | 22.504835535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:37:44 | 200 | 10.059232977s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:38:02 | 200 | 18.496858487s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:38:12 | 200 |  9.394878451s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T17:40:21.396+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T17:40:21.561+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="961.6 GiB" free_swap="0 B"
time=2025-10-17T17:40:21.561+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T17:40:21.627+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 33183"
time=2025-10-17T17:40:21.628+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:40:21.628+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:40:21.638+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:40:21.639+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:40:21.640+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33183"
time=2025-10-17T17:40:21.706+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:40:21.796+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:40:21.890+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:40:21.905+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:40:21.905+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:40:21.905+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:40:21.905+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:40:21.905+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:40:21.912+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T17:40:21.912+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:40:25.676+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 17:40:36 | 200 | 16.203951247s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:40:54 | 200 | 17.394585399s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:41:00 | 200 |  5.618351906s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:41:01 | 200 |  986.501278ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:41:20 | 200 | 19.366274436s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:41:23 | 200 |  2.984070381s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T17:55:58.669+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T17:55:58.834+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="961.7 GiB" free_swap="0 B"
time=2025-10-17T17:55:58.834+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T17:55:58.896+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43645"
time=2025-10-17T17:55:58.897+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:55:58.897+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:55:58.916+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:55:58.909+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:55:58.909+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43645"
time=2025-10-17T17:55:58.971+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-17T17:55:59.167+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:55:59.306+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:55:59.570+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:55:59.571+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:55:59.571+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:55:59.571+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:55:59.571+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:55:59.580+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T17:55:59.580+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:56:03.184+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.29 seconds"
[GIN] 2025/10/17 - 17:56:12 | 200 | 14.529848912s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 17:56:43 | 200 | 30.432100963s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T17:56:44.390+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T17:56:44.562+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="961.6 GiB" free_swap="0 B"
time=2025-10-17T17:56:44.563+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T17:56:44.622+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39429"
time=2025-10-17T17:56:44.623+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T17:56:44.623+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T17:56:44.634+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T17:56:44.634+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39429"
time=2025-10-17T17:56:44.642+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T17:56:44.701+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T17:56:44.783+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T17:56:44.891+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T17:56:44.891+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T17:56:44.891+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T17:56:44.891+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T17:56:44.891+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T17:56:44.894+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T17:56:44.899+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T17:56:44.899+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T17:56:48.456+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.83 seconds"
[GIN] 2025/10/17 - 17:57:00 | 200 | 16.787823343s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:57:15 | 200 | 15.072518459s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:57:16 | 200 |  1.608182313s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:57:27 | 200 | 10.641885761s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 17:57:28 | 200 |  1.369404948s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:01:06.188+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340847104 required="21.3 GiB"
time=2025-10-17T18:01:06.360+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="958.7 GiB" free_swap="0 B"
time=2025-10-17T18:01:06.361+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:01:06.426+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42149"
time=2025-10-17T18:01:06.427+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:01:06.427+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:01:06.437+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:01:06.440+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:01:06.440+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42149"
time=2025-10-17T18:01:06.502+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:01:06.581+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:01:06.688+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:01:06.689+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:01:06.689+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:01:06.689+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:01:06.689+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:01:06.689+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:01:06.698+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:01:06.698+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:01:10.248+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.82 seconds"
[GIN] 2025/10/17 - 18:01:16 | 200 | 12.125224293s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:01:36 | 200 |       65.49s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/17 - 18:01:36 | 200 |  237.683808ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/17 - 18:01:36 | 200 |  105.718208ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:37 | 200 |  272.723966ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:37 | 200 |  199.464006ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:37 | 200 |  157.535873ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:37 | 200 |  155.245638ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:38 | 200 |  957.087904ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:39 | 200 |  489.717872ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:01:39 | 200 |  248.815438ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/17 - 18:03:20 | 200 | 13.829182325s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:03:28 | 200 |   8.23573532s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:03:35 | 200 |  6.321098604s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:07:36 | 200 |  3.256465952s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:07:37.812+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:07:37.980+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.4 GiB" free_swap="0 B"
time=2025-10-17T18:07:37.981+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:07:38.048+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39033"
time=2025-10-17T18:07:38.048+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:07:38.048+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:07:38.067+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:07:38.060+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:07:38.060+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39033"
time=2025-10-17T18:07:38.119+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:07:38.209+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:07:38.317+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:07:38.317+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:07:38.317+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:07:38.317+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:07:38.317+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:07:38.320+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:07:38.325+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:07:38.326+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:07:42.097+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 18:07:46 | 200 | 10.109672141s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:07:47 | 200 |   985.65193ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:07:54 | 200 |  6.654643927s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:07:58 | 200 |  4.020777507s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:08:02 | 200 |  3.734118628s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:10:17.427+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:10:17.595+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.3 GiB" free_swap="0 B"
time=2025-10-17T18:10:17.595+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:10:17.654+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39811"
time=2025-10-17T18:10:17.654+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:10:17.654+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:10:17.666+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:10:17.666+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39811"
time=2025-10-17T18:10:17.673+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:10:17.726+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:10:17.830+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:10:17.924+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:10:17.949+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:10:17.949+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:10:17.949+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:10:17.949+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:10:17.950+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:10:17.959+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:10:17.959+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:10:21.716+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 18:10:30 | 200 | 13.849391099s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:11:44 | 200 |  7.618280603s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:11:51 | 200 |  6.421343909s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:11:59 | 200 |  7.994307089s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:12:07 | 200 |  8.334832882s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:12:20.249+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:12:20.419+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.2 GiB" free_swap="0 B"
time=2025-10-17T18:12:20.419+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:12:20.483+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 34489"
time=2025-10-17T18:12:20.483+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:12:20.483+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:12:20.494+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:12:20.495+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34489"
time=2025-10-17T18:12:20.496+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:12:20.561+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:12:20.651+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:12:20.747+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:12:20.761+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:12:20.761+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:12:20.761+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:12:20.761+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:12:20.761+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:12:20.769+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:12:20.769+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:12:24.322+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.84 seconds"
[GIN] 2025/10/17 - 18:12:33 | 200 | 14.686361488s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:12:47 | 200 | 13.276863889s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:12:56 | 200 |  9.361493367s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:13:07 | 200 | 11.312645349s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:22:22.522+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:22:22.687+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.1 GiB" free_swap="0 B"
time=2025-10-17T18:22:22.688+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:22:22.746+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42273"
time=2025-10-17T18:22:22.746+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:22:22.746+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:22:22.759+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:22:22.759+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42273"
time=2025-10-17T18:22:22.766+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:22:22.821+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:22:22.913+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:22:23.018+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:22:23.022+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:22:23.022+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:22:23.022+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:22:23.022+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:22:23.022+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:22:23.031+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:22:23.031+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:22:26.587+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.84 seconds"
[GIN] 2025/10/17 - 18:22:39 | 200 | 17.056303407s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:27:23 | 200 |  7.332707306s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:27:31 | 200 |  8.332089122s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:27:39 | 200 |  7.885427146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:27:53 | 200 |  13.70513351s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:27:54.685+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:27:54.867+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.0 GiB" free_swap="0 B"
time=2025-10-17T18:27:54.868+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:27:54.926+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39201"
time=2025-10-17T18:27:54.927+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:27:54.927+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:27:54.938+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:27:54.939+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39201"
time=2025-10-17T18:27:54.956+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:27:55.020+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:27:55.110+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:27:55.207+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:27:55.219+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:27:55.219+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:27:55.219+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:27:55.219+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:27:55.219+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:27:55.227+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:27:55.227+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:27:58.771+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.84 seconds"
[GIN] 2025/10/17 - 18:28:09 | 200 | 15.612160557s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:28:11 | 200 |  2.053698684s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:28:27 | 200 | 16.241349185s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:28:29 | 200 |  1.650979495s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:28:45 | 200 | 16.619858528s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:28:47 | 200 |  1.669449882s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:29:16 | 200 | 28.856763616s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:29:19 | 200 |  2.807021864s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:33:02.690+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:33:02.857+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.0 GiB" free_swap="0 B"
time=2025-10-17T18:33:02.857+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:33:02.927+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 40409"
time=2025-10-17T18:33:02.927+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:33:02.927+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:33:02.946+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:33:02.940+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:33:02.940+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40409"
time=2025-10-17T18:33:03.003+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:33:03.094+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:33:03.198+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:33:03.202+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:33:03.202+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:33:03.202+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:33:03.202+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:33:03.202+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:33:03.212+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:33:03.212+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:33:06.751+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.82 seconds"
[GIN] 2025/10/17 - 18:33:13 | 200 | 12.191335807s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:35:21 | 200 | 15.982861527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:35:27 | 200 |  5.868616119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:35:33 | 200 |  5.839346626s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:36:41 | 200 |   5.28485654s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:36:42.123+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:36:42.290+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.9 GiB" free_swap="0 B"
time=2025-10-17T18:36:42.291+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:36:42.360+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45865"
time=2025-10-17T18:36:42.361+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:36:42.361+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:36:42.380+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:36:42.372+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:36:42.372+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45865"
time=2025-10-17T18:36:42.439+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-17T18:36:42.631+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:36:42.688+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:36:42.797+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:36:42.797+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:36:42.797+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:36:42.797+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:36:42.797+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:36:42.805+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:36:42.805+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:36:46.438+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.08 seconds"
[GIN] 2025/10/17 - 18:36:52 | 200 | 11.237835891s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:36:58 | 200 |  6.337871134s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:37:04 | 200 |  5.558457926s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:37:18 | 200 |  14.53039888s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:39:53.380+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:39:53.554+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.9 GiB" free_swap="0 B"
time=2025-10-17T18:39:53.554+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:39:53.613+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42135"
time=2025-10-17T18:39:53.614+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:39:53.614+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:39:53.636+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:39:53.626+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:39:53.626+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42135"
time=2025-10-17T18:39:53.689+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:39:53.846+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:39:53.887+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:39:53.956+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:39:53.956+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:39:53.956+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:39:53.956+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:39:53.956+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:39:53.966+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:39:53.966+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:39:57.701+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.09 seconds"
[GIN] 2025/10/17 - 18:40:04 | 200 |  12.37033238s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:41:31 | 200 | 10.235954847s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:41:39 | 200 |  8.404438244s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:41:44 | 200 |  4.518550743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:41:47 | 200 |  3.135950639s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:41:48.752+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:41:48.915+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.8 GiB" free_swap="0 B"
time=2025-10-17T18:41:48.916+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:41:48.971+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45205"
time=2025-10-17T18:41:48.971+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:41:48.971+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:41:48.986+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:41:48.989+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:41:48.989+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45205"
time=2025-10-17T18:41:49.056+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:41:49.149+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:41:49.238+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:41:49.257+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:41:49.257+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:41:49.257+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:41:49.257+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:41:49.257+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:41:49.265+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:41:49.265+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:41:52.798+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.83 seconds"
[GIN] 2025/10/17 - 18:42:00 | 200 | 12.682871101s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:09 | 200 |  9.264766208s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:11 | 200 |  1.775612795s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:26 | 200 | 14.831488516s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:28 | 200 |  2.272227577s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:31 | 200 |  3.029379043s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:42:32 | 200 |  802.283068ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:47:05.673+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:47:05.840+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.0 GiB" free_swap="0 B"
time=2025-10-17T18:47:05.840+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:47:05.908+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 38011"
time=2025-10-17T18:47:05.908+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:47:05.908+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:47:05.920+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:47:05.921+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38011"
time=2025-10-17T18:47:05.927+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:47:05.983+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:47:06.075+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:47:06.178+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:47:06.186+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:47:06.186+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:47:06.186+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:47:06.186+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:47:06.186+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:47:06.196+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:47:06.196+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:47:15.258+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.35 seconds"
[GIN] 2025/10/17 - 18:47:25 | 200 | 20.963938768s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:49:52 | 200 | 11.567743695s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:49:58 | 200 |  5.627976797s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:50:03 | 200 |  4.608306149s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 18:50:10 | 200 |   7.73862543s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T18:50:12.482+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T18:50:12.651+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.8 GiB" free_swap="0 B"
time=2025-10-17T18:50:12.651+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T18:50:12.718+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 34903"
time=2025-10-17T18:50:12.718+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:50:12.718+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:50:12.718+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:50:12.729+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:50:12.729+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34903"
time=2025-10-17T18:50:12.796+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:50:12.886+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:50:12.969+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:50:12.996+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:50:12.996+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:50:12.996+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:50:12.996+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:50:12.996+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:50:13.004+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T18:50:13.004+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:50:16.762+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.04 seconds"
[GIN] 2025/10/17 - 18:50:23 | 200 | 12.159158155s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:50:24 | 200 |  992.102432ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:50:29 | 200 |   4.92762918s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 18:50:34 | 200 |  4.582212247s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T18:55:47.573+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T18:55:47.743+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.6 GiB" free_swap="0 B"
time=2025-10-17T18:55:47.743+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T18:55:47.811+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 37473"
time=2025-10-17T18:55:47.811+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T18:55:47.811+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T18:55:47.831+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T18:55:47.823+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T18:55:47.823+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37473"
time=2025-10-17T18:55:47.881+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T18:55:47.983+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T18:55:48.081+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T18:55:48.093+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T18:55:48.094+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T18:55:48.094+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T18:55:48.094+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T18:55:48.094+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T18:55:48.103+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T18:55:48.103+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T18:55:51.865+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 18:56:07 | 200 | 20.677607742s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:01:23.286+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:01:23.460+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.5 GiB" free_swap="0 B"
time=2025-10-17T19:01:23.461+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:01:23.529+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45429"
time=2025-10-17T19:01:23.529+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:01:23.529+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:01:23.548+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:01:23.541+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:01:23.541+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45429"
time=2025-10-17T19:01:23.600+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:01:23.750+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:01:23.807+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:01:23.862+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:01:23.862+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:01:23.862+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:01:23.862+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:01:23.862+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:01:23.871+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:01:23.872+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:01:27.636+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.11 seconds"
[GIN] 2025/10/17 - 19:02:02 | 200 |  39.53975598s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:02:23 | 200 | 21.126674432s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:02:27 | 200 |  3.611070909s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:02:32 | 200 |  5.530784146s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:02:33.870+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:02:34.037+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.5 GiB" free_swap="0 B"
time=2025-10-17T19:02:34.037+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:02:34.092+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43031"
time=2025-10-17T19:02:34.092+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:02:34.092+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:02:34.103+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:02:34.103+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43031"
time=2025-10-17T19:02:34.109+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:02:34.177+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:02:34.273+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:02:34.360+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:02:34.490+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:02:34.490+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:02:34.491+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:02:34.491+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:02:34.491+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:02:34.612+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:02:34.612+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:02:38.157+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 19:02:46 | 200 | 13.633300192s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:02:56 | 200 | 10.196427495s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:02:57 | 200 |  1.163502221s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:03:05 | 200 |  7.416157238s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:06:22.991+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:06:23.158+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.4 GiB" free_swap="0 B"
time=2025-10-17T19:06:23.159+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:06:23.227+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 46679"
time=2025-10-17T19:06:23.228+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:06:23.228+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:06:23.247+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:06:23.240+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:06:23.240+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46679"
time=2025-10-17T19:06:23.299+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-17T19:06:23.498+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:06:32.042+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:06:32.916+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:06:32.916+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:06:32.916+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:06:32.916+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:06:32.916+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:06:32.926+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:06:32.926+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:06:51.077+11:00 level=INFO source=server.go:637 msg="llama runner started in 27.85 seconds"
[GIN] 2025/10/17 - 19:07:17 | 200 | 55.513071091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:10:12 | 200 |  32.54354629s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:10:32 | 200 | 19.270671583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:10:36 | 200 |  4.270779866s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:10:51 | 200 | 15.189868772s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:10:52.748+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:10:52.914+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.7 GiB" free_swap="0 B"
time=2025-10-17T19:10:52.915+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:10:52.973+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42625"
time=2025-10-17T19:10:52.973+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:10:52.973+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:10:52.985+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:10:52.985+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42625"
time=2025-10-17T19:10:53.001+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:10:53.058+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:10:53.150+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:10:53.252+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:10:53.264+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:10:53.264+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:10:53.264+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:10:53.264+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:10:53.264+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:10:53.272+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:10:53.272+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:10:57.036+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 19:11:07 | 200 | 16.173580962s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:11:15 | 200 |  7.289019668s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:11:16 | 200 |  1.073886933s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:11:23 | 200 |  7.468985394s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:13:02.091+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:13:02.255+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.7 GiB" free_swap="0 B"
time=2025-10-17T19:13:02.255+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:13:02.317+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 45983"
time=2025-10-17T19:13:02.318+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:13:02.318+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:13:02.331+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:13:02.329+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:13:02.330+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45983"
time=2025-10-17T19:13:02.397+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:13:02.487+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:13:02.583+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:13:02.596+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:13:02.596+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:13:02.596+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:13:02.596+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:13:02.596+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:13:02.605+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:13:02.605+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:13:06.399+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.08 seconds"
[GIN] 2025/10/17 - 19:13:15 | 200 | 14.361282555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:15:20 | 200 | 25.907889437s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:15:35 | 200 | 15.072791067s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:15:58 | 200 | 23.275162517s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:16:07 | 200 |  8.422407719s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:16:08.297+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:16:08.468+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.7 GiB" free_swap="0 B"
time=2025-10-17T19:16:08.469+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:16:08.534+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 37483"
time=2025-10-17T19:16:08.535+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:16:08.535+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:16:08.555+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:16:08.547+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:16:08.547+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37483"
time=2025-10-17T19:16:08.605+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:16:08.708+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:16:08.806+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:16:08.823+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:16:08.823+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:16:08.823+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:16:08.823+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:16:08.823+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:16:08.831+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:16:08.831+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:16:12.603+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.07 seconds"
[GIN] 2025/10/17 - 19:16:20 | 200 | 13.810811087s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:16:22 | 200 |  1.573637126s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:16:27 | 200 |  5.099029666s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:16:28 | 200 |  900.405222ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:16:32 | 200 |  3.938896628s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:20:24.010+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:20:24.181+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="955.9 GiB" free_swap="0 B"
time=2025-10-17T19:20:24.181+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:20:24.255+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 38169"
time=2025-10-17T19:20:24.267+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:20:24.267+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:20:24.267+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:20:24.291+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:20:24.291+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38169"
time=2025-10-17T19:20:24.347+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-17T19:20:24.518+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:20:27.256+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:20:27.384+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:20:27.384+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:20:27.384+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:20:27.384+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:20:27.384+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:20:27.393+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:20:27.393+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:20:40.841+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.57 seconds"
[GIN] 2025/10/17 - 19:20:49 | 200 | 26.857247121s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:27:13.920+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:27:14.085+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.8 GiB" free_swap="0 B"
time=2025-10-17T19:27:14.086+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:27:14.168+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 40985"
time=2025-10-17T19:27:14.168+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:27:14.168+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:27:14.183+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:27:14.179+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:27:14.179+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40985"
time=2025-10-17T19:27:14.244+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:27:14.335+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:27:14.434+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:27:14.445+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:27:14.445+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:27:14.445+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:27:14.445+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:27:14.446+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:27:14.455+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:27:14.455+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:27:18.220+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 19:27:52 | 200 |  39.04584654s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:28:14 | 200 | 21.409442929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:28:28 | 200 | 14.608726438s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:28:47 | 200 | 19.319929973s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:28:49.083+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:28:49.248+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.7 GiB" free_swap="0 B"
time=2025-10-17T19:28:49.249+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:28:49.311+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 37729"
time=2025-10-17T19:28:49.311+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:28:49.311+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:28:49.314+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:28:49.323+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:28:49.324+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37729"
time=2025-10-17T19:28:49.390+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:28:49.470+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:28:49.565+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:28:49.584+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:28:49.584+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:28:49.584+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:28:49.584+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:28:49.584+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:28:49.592+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:28:49.592+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:28:53.324+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.01 seconds"
[GIN] 2025/10/17 - 19:29:12 | 200 | 24.322434932s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:29:15 | 200 |   2.76598492s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:29:25 | 200 |   9.94496309s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:29:26 | 200 |   1.09261274s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:29:38 | 200 | 12.483566778s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:29:39 | 200 |  1.283514219s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:34:05.908+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:34:06.073+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.6 GiB" free_swap="0 B"
time=2025-10-17T19:34:06.073+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:34:06.139+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 35339"
time=2025-10-17T19:34:06.140+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:34:06.140+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:34:06.159+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:34:06.152+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:34:06.153+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35339"
time=2025-10-17T19:34:06.216+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:34:06.319+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:34:06.411+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:34:06.428+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:34:06.428+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:34:06.428+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:34:06.428+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:34:06.428+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:34:06.437+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:34:06.437+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:34:10.191+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 19:34:24 | 200 | 20.118801421s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:34:34 | 200 |  9.231627804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:34:44 | 200 | 10.544227302s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:34:53 | 200 |  8.499653382s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:35:11 | 200 | 18.502484496s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:40:57.526+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:40:57.695+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.5 GiB" free_swap="0 B"
time=2025-10-17T19:40:57.695+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:40:57.764+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 36921"
time=2025-10-17T19:40:57.764+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:40:57.764+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:40:57.784+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:40:57.775+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:40:57.776+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36921"
time=2025-10-17T19:40:57.842+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:40:57.933+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:40:58.035+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:40:58.043+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:40:58.043+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:40:58.043+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:40:58.043+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:40:58.043+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:40:58.051+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:40:58.051+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:41:01.586+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.82 seconds"
[GIN] 2025/10/17 - 19:41:12 | 200 | 15.342717991s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:41:14 | 200 |  1.934966412s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:41:22 | 200 |  7.922906838s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:41:23 | 200 |   1.12849998s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:41:34 | 200 |  10.88246151s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:41:35 | 200 |   1.05401778s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:47:33.727+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:47:33.895+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.4 GiB" free_swap="0 B"
time=2025-10-17T19:47:33.896+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:47:33.963+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 33657"
time=2025-10-17T19:47:33.963+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:47:33.963+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:47:33.983+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:47:33.976+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:47:33.976+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33657"
time=2025-10-17T19:47:34.039+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-17T19:47:34.234+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:47:34.291+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:47:34.552+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:47:34.552+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:47:34.552+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:47:34.552+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:47:34.552+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:47:34.669+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:47:34.669+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:47:38.283+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.32 seconds"
[GIN] 2025/10/17 - 19:47:48 | 200 | 15.445488027s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:53:37.681+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:53:37.851+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.3 GiB" free_swap="0 B"
time=2025-10-17T19:53:37.851+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:53:37.916+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39323"
time=2025-10-17T19:53:37.916+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:53:37.916+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:53:37.927+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:53:37.927+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39323"
time=2025-10-17T19:53:37.928+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:53:37.994+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:53:38.097+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:53:38.180+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:53:38.207+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:53:38.207+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:53:38.207+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:53:38.208+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:53:38.208+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:53:38.217+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:53:38.217+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:53:41.749+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.83 seconds"
[GIN] 2025/10/17 - 19:54:16 | 200 | 39.223358705s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:54:43 | 200 | 27.069209537s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:55:17 | 200 | 33.553937569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 19:55:42 | 200 | 25.207152369s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T19:55:43.973+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T19:55:44.144+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.5 GiB" free_swap="0 B"
time=2025-10-17T19:55:44.145+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T19:55:44.221+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 36895"
time=2025-10-17T19:55:44.222+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:55:44.222+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:55:44.233+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:55:44.233+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36895"
time=2025-10-17T19:55:44.244+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:55:44.300+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:55:44.402+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:55:44.494+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:55:44.511+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:55:44.511+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:55:44.511+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:55:44.511+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:55:44.511+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:55:44.519+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T19:55:44.519+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:55:52.807+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.59 seconds"
[GIN] 2025/10/17 - 19:56:08 | 200 | 25.521886616s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:56:10 | 200 |  1.923455335s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:56:26 | 200 | 16.030848459s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:56:28 | 200 |  1.744453173s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:56:41 | 200 | 13.350155754s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 19:56:42 | 200 |  1.151331017s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T19:59:33.943+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T19:59:34.114+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.3 GiB" free_swap="0 B"
time=2025-10-17T19:59:34.114+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T19:59:34.182+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 46829"
time=2025-10-17T19:59:34.182+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T19:59:34.182+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T19:59:34.202+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T19:59:34.194+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T19:59:34.194+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46829"
time=2025-10-17T19:59:34.253+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T19:59:34.364+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T19:59:34.453+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T19:59:34.628+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T19:59:34.628+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T19:59:34.628+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T19:59:34.628+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T19:59:34.628+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T19:59:34.702+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T19:59:34.702+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T19:59:38.267+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.08 seconds"
[GIN] 2025/10/17 - 19:59:44 | 200 | 11.959442151s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:02:30 | 200 |  5.700425975s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:02:38 | 200 |  8.590885456s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:02:46 | 200 |  8.074621765s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:03:12 | 200 | 25.428571852s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:03:13.559+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:03:13.728+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.1 GiB" free_swap="0 B"
time=2025-10-17T20:03:13.729+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:03:13.783+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 36623"
time=2025-10-17T20:03:13.783+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:03:13.783+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:03:13.800+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:03:13.801+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:03:13.802+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36623"
time=2025-10-17T20:03:13.869+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:03:13.974+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:03:14.050+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:03:14.085+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:03:14.085+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:03:14.085+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:03:14.085+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:03:14.085+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:03:14.095+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:03:14.095+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:03:17.834+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:03:28 | 200 | 16.389251206s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:03:35 | 200 |  7.041856068s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:03:41 | 200 |  6.081713749s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:04:22.800+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:04:22.969+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.1 GiB" free_swap="0 B"
time=2025-10-17T20:04:22.969+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:04:23.034+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 46689"
time=2025-10-17T20:04:23.034+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:04:23.034+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:04:23.058+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:04:23.046+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:04:23.046+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46689"
time=2025-10-17T20:04:23.112+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:04:23.215+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:04:23.308+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:04:23.331+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:04:23.331+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:04:23.331+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:04:23.331+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:04:23.331+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:04:23.341+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:04:23.341+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:04:27.116+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.08 seconds"
[GIN] 2025/10/17 - 20:04:34 | 200 | 12.417422549s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:05:24 | 200 | 12.144551252s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:05:35 | 200 | 11.780944699s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:05:54 | 200 | 18.389048094s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:05:58 | 200 |  4.354915856s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:05:59.749+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:05:59.921+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.1 GiB" free_swap="0 B"
time=2025-10-17T20:05:59.921+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:05:59.987+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43213"
time=2025-10-17T20:05:59.987+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:05:59.987+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:05:59.988+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:05:59.998+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:05:59.999+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43213"
time=2025-10-17T20:06:00.066+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:06:00.177+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:06:00.239+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:06:00.415+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:06:00.415+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:06:00.415+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:06:00.415+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:06:00.415+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:06:00.471+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:06:00.471+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:06:04.046+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 20:06:09 | 200 | 10.369841038s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:06:12 | 200 |  3.115447841s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:06:15 | 200 |  3.621249299s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:11:34.681+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:11:34.858+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.0 GiB" free_swap="0 B"
time=2025-10-17T20:11:34.858+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:11:34.926+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43395"
time=2025-10-17T20:11:34.926+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:11:34.926+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:11:34.945+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:11:34.938+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:11:34.938+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43395"
time=2025-10-17T20:11:34.996+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:11:35.087+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:11:35.196+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:11:35.203+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:11:35.203+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:11:35.203+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:11:35.203+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:11:35.203+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:11:35.212+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:11:35.212+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:11:38.971+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:11:50 | 200 | 16.430013794s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:16:55.837+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:16:56.003+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="954.0 GiB" free_swap="0 B"
time=2025-10-17T20:16:56.003+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:16:56.075+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 35281"
time=2025-10-17T20:16:56.075+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:16:56.075+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:16:56.090+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:16:56.087+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:16:56.087+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35281"
time=2025-10-17T20:16:56.153+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:16:56.255+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:16:56.341+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:16:56.366+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:16:56.366+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:16:56.366+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:16:56.366+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:16:56.366+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:16:56.375+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:16:56.376+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:17:00.130+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:17:34 | 200 | 38.657040458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:17:54 | 200 | 20.678098756s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:18:16 | 200 | 21.945794394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:18:43 | 200 | 26.630821292s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:18:44.532+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:18:44.695+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.9 GiB" free_swap="0 B"
time=2025-10-17T20:18:44.696+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:18:44.753+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 40039"
time=2025-10-17T20:18:44.754+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:18:44.754+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:18:44.765+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:18:44.765+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40039"
time=2025-10-17T20:18:44.777+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:18:44.834+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:18:44.937+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:18:45.027+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:18:45.046+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:18:45.046+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:18:45.046+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:18:45.046+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:18:45.046+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:18:45.054+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:18:45.054+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:18:48.601+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.85 seconds"
[GIN] 2025/10/17 - 20:18:56 | 200 | 12.910524995s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:18:57 | 200 |  1.015834854s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:19:06 | 200 |  9.428680082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:19:07 | 200 |  1.131172797s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:19:17 | 200 |  9.721487325s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:22:51.030+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:22:51.201+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.9 GiB" free_swap="0 B"
time=2025-10-17T20:22:51.201+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:22:51.267+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42573"
time=2025-10-17T20:22:51.268+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:22:51.268+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:22:51.278+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:22:51.280+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:22:51.280+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42573"
time=2025-10-17T20:22:51.337+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:22:51.444+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:22:51.530+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:22:51.555+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:22:51.555+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:22:51.555+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:22:51.555+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:22:51.555+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:22:51.564+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:22:51.564+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:22:55.314+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:23:04 | 200 |  14.08213035s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:26:47 | 200 | 32.058686321s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:27:19 | 200 | 32.138634653s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:27:48 | 200 | 29.255014695s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:28:07 | 200 | 18.164134967s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:28:08.829+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:28:09.012+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.8 GiB" free_swap="0 B"
time=2025-10-17T20:28:09.013+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:28:09.083+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 39611"
time=2025-10-17T20:28:09.084+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:28:09.084+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:28:09.101+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:28:09.095+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:28:09.095+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39611"
time=2025-10-17T20:28:09.162+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:28:09.264+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:28:09.353+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:28:09.373+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:28:09.373+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:28:09.373+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:28:09.373+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:28:09.373+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:28:09.381+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:28:09.381+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:28:13.140+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 20:28:20 | 200 | 13.012270441s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:28:32 | 200 | 11.619706539s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:28:33 | 200 |  1.205397611s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:28:42 | 200 |  8.386333212s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:28:43 | 200 |  1.020079808s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:36:15.459+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:36:15.628+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.7 GiB" free_swap="0 B"
time=2025-10-17T20:36:15.629+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:36:15.697+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43557"
time=2025-10-17T20:36:15.697+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:36:15.697+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:36:15.716+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:36:15.710+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:36:15.711+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43557"
time=2025-10-17T20:36:15.774+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:36:15.864+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:36:15.967+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:36:15.974+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:36:15.974+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:36:15.974+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:36:15.974+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:36:15.974+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:36:15.984+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:36:15.984+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:36:19.750+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:36:34 | 200 | 19.511077039s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:41:32 | 200 | 32.473826833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:41:56 | 200 | 23.533692033s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:42:29 | 200 | 33.369373555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:42:58 | 200 | 28.675735612s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:42:59.578+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:42:59.751+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.6 GiB" free_swap="0 B"
time=2025-10-17T20:42:59.752+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:42:59.811+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 41399"
time=2025-10-17T20:42:59.811+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:42:59.811+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:42:59.822+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:42:59.823+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41399"
time=2025-10-17T20:42:59.834+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:42:59.889+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:42:59.982+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:43:00.084+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:43:00.092+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:43:00.092+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:43:00.092+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:43:00.092+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:43:00.092+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:43:00.100+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:43:00.100+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:43:03.865+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.05 seconds"
[GIN] 2025/10/17 - 20:43:12 | 200 | 14.445932081s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:43:13 | 200 |  1.021450713s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:43:29 | 200 | 15.225381538s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:43:30 | 200 |  1.540975378s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:43:39 | 200 |  8.871917461s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:43:40 | 200 |  954.434433ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:45:05.078+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:45:05.244+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.5 GiB" free_swap="0 B"
time=2025-10-17T20:45:05.244+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:45:05.316+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 42573"
time=2025-10-17T20:45:05.316+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:45:05.316+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:45:05.332+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:45:05.329+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:45:05.329+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42573"
time=2025-10-17T20:45:05.394+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:45:05.497+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:45:05.583+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:45:05.608+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:45:05.608+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:45:05.608+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:45:05.608+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:45:05.608+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:45:05.617+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:45:05.618+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:45:09.356+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.04 seconds"
[GIN] 2025/10/17 - 20:45:15 | 200 | 11.698877743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:46:39 | 200 | 10.136030588s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:46:44 | 200 |  5.121987559s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:50:16 | 200 |  5.512439166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:50:24 | 200 |  8.413933353s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T20:50:25.699+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T20:50:25.873+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.5 GiB" free_swap="0 B"
time=2025-10-17T20:50:25.873+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T20:50:25.934+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 40969"
time=2025-10-17T20:50:25.935+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:50:25.935+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:50:25.952+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:50:25.946+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:50:25.946+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40969"
time=2025-10-17T20:50:26.011+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:50:26.114+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:50:26.203+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T20:50:26.225+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:50:26.225+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:50:26.225+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:50:26.225+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:50:26.225+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:50:26.233+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T20:50:26.233+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:50:30.007+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.07 seconds"
[GIN] 2025/10/17 - 20:50:34 | 200 |  9.589954634s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:50:37 | 200 |  3.781892049s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 20:50:42 | 200 |  4.498812613s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T20:54:22.976+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T20:54:23.141+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.4 GiB" free_swap="0 B"
time=2025-10-17T20:54:23.141+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T20:54:23.217+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43737"
time=2025-10-17T20:54:23.217+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T20:54:23.217+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T20:54:23.229+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T20:54:23.229+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43737"
time=2025-10-17T20:54:23.232+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T20:54:23.296+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-17T20:54:23.483+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T20:54:23.609+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T20:54:23.718+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T20:54:23.718+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T20:54:23.718+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T20:54:23.718+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T20:54:23.718+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T20:54:23.727+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T20:54:23.727+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T20:54:27.507+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.29 seconds"
[GIN] 2025/10/17 - 20:54:52 | 200 | 31.022676458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:59:45 | 200 | 23.004433456s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 20:59:57 | 200 | 11.686542947s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:00:20 | 200 | 23.618588185s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:00:35 | 200 |    14.869151s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T21:00:36.728+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T21:00:36.894+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.2 GiB" free_swap="0 B"
time=2025-10-17T21:00:36.894+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T21:00:36.956+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43691"
time=2025-10-17T21:00:36.956+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T21:00:36.956+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T21:00:36.968+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T21:00:36.968+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43691"
time=2025-10-17T21:00:36.972+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T21:00:37.037+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T21:00:37.141+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T21:00:37.224+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T21:00:37.251+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T21:00:37.251+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T21:00:37.251+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T21:00:37.251+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T21:00:37.251+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T21:00:37.259+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T21:00:37.259+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T21:00:41.038+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.08 seconds"
[GIN] 2025/10/17 - 21:01:10 | 200 | 35.133992146s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:01:13 | 200 |  2.775055405s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:01:24 | 200 | 10.644833585s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:01:35 | 200 | 11.218678383s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:01:37 | 200 |  1.581942122s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T21:05:46.171+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T21:05:46.339+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.4 GiB" free_swap="0 B"
time=2025-10-17T21:05:46.340+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T21:05:46.409+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 38589"
time=2025-10-17T21:05:46.409+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T21:05:46.409+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T21:05:46.428+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T21:05:46.421+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T21:05:46.421+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38589"
time=2025-10-17T21:05:46.480+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T21:05:46.592+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T21:05:46.680+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T21:05:46.701+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T21:05:46.702+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T21:05:46.702+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T21:05:46.702+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T21:05:46.702+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T21:05:46.711+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T21:05:46.711+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T21:05:58.971+11:00 level=INFO source=server.go:637 msg="llama runner started in 12.56 seconds"
[GIN] 2025/10/17 - 21:06:09 | 200 | 23.987855002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:06:16 | 200 |  7.639677209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:06:25 | 200 |  8.454684466s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:06:30 | 200 |  5.727289731s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:06:34 | 200 |  3.080968982s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T21:06:35.248+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="14.9 GiB"
time=2025-10-17T21:06:35.422+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.2 GiB" free_swap="0 B"
time=2025-10-17T21:06:35.422+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-17T21:06:35.487+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43439"
time=2025-10-17T21:06:35.487+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T21:06:35.487+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T21:06:35.504+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T21:06:35.498+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T21:06:35.505+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43439"
time=2025-10-17T21:06:35.574+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T21:06:35.677+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T21:06:35.755+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T21:06:35.788+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T21:06:35.788+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T21:06:35.788+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T21:06:35.788+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T21:06:35.788+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T21:06:35.796+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-17T21:06:35.796+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T21:06:39.548+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.06 seconds"
[GIN] 2025/10/17 - 21:06:47 | 200 | 13.432404415s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:06:48 | 200 |  1.316865235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:06:54 | 200 |  5.292585364s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:07:01 | 200 |  7.258089849s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/17 - 21:07:02 | 200 |  933.274061ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-17T21:11:01.994+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T21:11:02.164+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.1 GiB" free_swap="0 B"
time=2025-10-17T21:11:02.164+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T21:11:02.234+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 41001"
time=2025-10-17T21:11:02.234+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T21:11:02.234+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T21:11:02.253+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T21:11:02.246+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T21:11:02.246+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41001"
time=2025-10-17T21:11:02.304+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T21:11:02.395+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T21:11:02.505+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T21:11:02.506+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T21:11:02.506+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T21:11:02.506+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T21:11:02.506+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T21:11:02.506+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T21:11:02.516+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T21:11:02.516+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T21:11:06.065+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.83 seconds"
[GIN] 2025/10/17 - 21:11:13 | 200 | 12.620340312s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-17T21:16:19.311+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-63f7ad9d-7aa9-7447-cb51-424903c0f4af parallel=1 available=47340912640 required="21.3 GiB"
time=2025-10-17T21:16:19.480+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.9 GiB" free="953.0 GiB" free_swap="0 B"
time=2025-10-17T21:16:19.480+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.3 GiB" memory.required.partial="21.3 GiB" memory.required.kv="858.0 MiB" memory.required.allocations="[21.3 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="7.8 GiB" memory.graph.partial="7.8 GiB"
time=2025-10-17T21:16:19.557+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 32000 --batch-size 512 --n-gpu-layers 25 --threads 64 --parallel 1 --port 43769"
time=2025-10-17T21:16:19.557+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-17T21:16:19.557+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-17T21:16:19.570+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-17T21:16:19.570+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-17T21:16:19.570+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43769"
time=2025-10-17T21:16:19.627+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-17T21:16:19.735+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-17T21:16:19.821+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-17T21:16:19.846+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-17T21:16:19.846+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-17T21:16:19.846+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-17T21:16:19.846+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-17T21:16:19.846+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-17T21:16:19.856+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="7.9 GiB"
time=2025-10-17T21:16:19.856+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-17T21:16:23.592+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.04 seconds"
[GIN] 2025/10/17 - 21:16:32 | 200 | 13.840784143s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:16:39 | 200 |  6.641640502s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/17 - 21:16:48 | 200 |  9.441411865s |       127.0.0.1 | POST     "/api/chat"
