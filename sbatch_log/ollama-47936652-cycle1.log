time=2025-10-19T10:48:47.665+11:00 level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/mvo1/lb64_scratch/ollama-models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-10-19T10:48:48.061+11:00 level=INFO source=images.go:477 msg="total blobs: 27"
time=2025-10-19T10:48:48.066+11:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-10-19T10:48:48.071+11:00 level=INFO source=routes.go:1357 msg="Listening on 127.0.0.1:11434 (version 0.11.4)"
time=2025-10-19T10:48:48.071+11:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-10-19T10:48:49.073+11:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda variant=v12 compute=8.6 driver=12.2 name="NVIDIA A40" total="44.4 GiB" available="44.1 GiB"
[GIN] 2025/10/19 - 10:48:50 | 200 |  187.729739ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-19T10:57:17.072+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340191744 required="12.5 GiB"
time=2025-10-19T10:57:17.268+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="934.1 GiB" free_swap="0 B"
time=2025-10-19T10:57:17.269+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T10:57:17.503+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 37461"
time=2025-10-19T10:57:17.504+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T10:57:17.504+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T10:57:17.505+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T10:57:17.517+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T10:57:26.552+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T10:57:26.557+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37461"
time=2025-10-19T10:57:26.774+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T10:57:29.280+11:00 level=INFO source=server.go:637 msg="llama runner started in 11.78 seconds"
[GIN] 2025/10/19 - 10:57:49 | 200 |  33.20995061s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:00:57 | 200 |  5.226727774s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:01:09 | 200 | 11.946452028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:01:19 | 200 |  9.200936882s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:01:25 | 200 |   6.28174759s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T11:01:38.138+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T11:01:38.139+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T11:01:38.335+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.9 GiB" free_swap="0 B"
time=2025-10-19T11:01:38.336+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T11:01:38.413+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 44551"
time=2025-10-19T11:01:38.413+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T11:01:38.413+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:01:38.434+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:01:38.426+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T11:01:38.426+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:44551"
time=2025-10-19T11:01:38.508+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:01:38.606+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:01:38.685+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T11:01:38.736+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T11:01:38.736+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T11:01:38.736+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T11:01:38.736+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T11:01:38.736+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T11:01:38.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T11:01:38.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T11:01:51.269+11:00 level=INFO source=server.go:637 msg="llama runner started in 12.86 seconds"
[GIN] 2025/10/19 - 11:01:59 | 200 | 22.658882804s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:02:04 | 200 |   4.04818366s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:02:05 | 200 |  1.006935715s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:02:10 | 200 |  5.092088323s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:02:11 | 200 |  895.954214ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:02:18 | 200 |   6.58419781s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T11:06:41.673+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="29.4 GiB"
time=2025-10-19T11:06:41.674+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=31586397824 required="12.5 GiB"
time=2025-10-19T11:06:41.873+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="930.7 GiB" free_swap="0 B"
time=2025-10-19T11:06:41.874+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[29.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T11:06:42.093+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 35555"
time=2025-10-19T11:06:42.094+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T11:06:42.094+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:06:42.094+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:06:42.108+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:06:42.193+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:06:42.194+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35555"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 30123 MiB free
time=2025-10-19T11:06:42.345+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T11:06:44.100+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 11:06:51 | 200 | 10.102311091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:09:07 | 200 |   3.70174697s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:09:15 | 200 |  8.274063008s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:09:23 | 200 |   8.18870271s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:09:29 | 200 |  6.274628305s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T11:09:30.343+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T11:09:30.343+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T11:09:30.548+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.9 GiB" free_swap="0 B"
time=2025-10-19T11:09:30.548+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T11:09:30.635+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 39747"
time=2025-10-19T11:09:30.635+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T11:09:30.635+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:09:30.655+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:09:30.647+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T11:09:30.648+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39747"
time=2025-10-19T11:09:30.730+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:09:30.816+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:09:30.907+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T11:09:30.945+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T11:09:30.945+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T11:09:30.945+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T11:09:30.945+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T11:09:30.945+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T11:09:30.955+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T11:09:30.955+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T11:09:34.963+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 11:09:39 | 200 |  9.914639548s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:09:41 | 200 |  1.596072563s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:09:46 | 200 |  5.129442062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:09:50 | 200 |  4.035600782s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:09:54 | 200 |  4.070580299s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T11:23:53.601+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T11:23:53.796+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.4 GiB" free_swap="0 B"
time=2025-10-19T11:23:53.796+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T11:23:54.014+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 38603"
time=2025-10-19T11:23:54.014+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T11:23:54.014+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:23:54.015+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:23:54.028+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:23:54.115+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:23:54.116+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38603"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T11:23:54.266+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T11:23:55.771+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 11:24:15 | 200 | 21.892403294s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T11:31:14.578+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T11:31:14.775+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.4 GiB" free_swap="0 B"
time=2025-10-19T11:31:14.775+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T11:31:14.990+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 40667"
time=2025-10-19T11:31:14.991+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T11:31:14.991+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:31:14.991+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:31:15.003+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:31:15.092+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:31:15.092+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40667"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T11:31:15.242+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T11:31:16.746+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 11:31:20 | 200 |  6.114682847s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:31:29 | 200 |  8.796030125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:31:38 | 200 |  8.957111351s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:31:44 | 200 |  6.272753162s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T11:31:44.836+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T11:31:44.836+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T11:31:45.034+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.8 GiB" free_swap="0 B"
time=2025-10-19T11:31:45.034+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T11:31:45.120+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 33553"
time=2025-10-19T11:31:45.120+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T11:31:45.121+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:31:45.141+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:31:45.133+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T11:31:45.133+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33553"
time=2025-10-19T11:31:45.215+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:31:45.300+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:31:45.392+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T11:31:45.428+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T11:31:45.428+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T11:31:45.428+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T11:31:45.428+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T11:31:45.428+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T11:31:45.438+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T11:31:45.438+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T11:31:49.458+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.34 seconds"
[GIN] 2025/10/19 - 11:31:59 | 200 |  15.51090756s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:01 | 200 |  1.582484436s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:09 | 200 |  8.420951206s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:10 | 200 |  1.041386947s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:15 | 200 |  4.408220494s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:17 | 200 |  1.905797046s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:24 | 200 |  7.408977414s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:32:25 | 200 |  983.308645ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:33:23 | 200 |  5.444231347s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:33:53 | 200 |  3.708971369s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:34:02 | 200 |  9.519828467s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:34:15 | 200 | 12.622052892s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:34:19 | 200 |  4.430883853s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:34:27 | 200 |  7.215613914s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:34:31 | 200 |  4.093884033s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:34:35 | 200 |  4.708211596s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:34:36 | 200 |  866.734993ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:34:51 | 200 | 14.250429283s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T11:44:16.705+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T11:44:16.899+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.4 GiB" free_swap="0 B"
time=2025-10-19T11:44:16.899+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T11:44:17.117+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 35447"
time=2025-10-19T11:44:17.118+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T11:44:17.118+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:44:17.118+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:44:17.133+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:44:17.220+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:44:17.220+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35447"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T11:44:17.369+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T11:44:18.874+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 11:44:28 | 200 | 11.778605845s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:49:15 | 200 |  5.720155396s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:49:21 | 200 |  5.316070069s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:49:32 | 200 | 11.538775322s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:49:42 | 200 |  9.925666589s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T11:49:43.060+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T11:49:43.061+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T11:49:43.256+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.7 GiB" free_swap="0 B"
time=2025-10-19T11:49:43.256+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T11:49:43.342+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 33625"
time=2025-10-19T11:49:43.342+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T11:49:43.342+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T11:49:43.363+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T11:49:43.355+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T11:49:43.355+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33625"
time=2025-10-19T11:49:43.435+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T11:49:43.526+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T11:49:43.613+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T11:49:43.656+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T11:49:43.656+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T11:49:43.656+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T11:49:43.656+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T11:49:43.656+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T11:49:43.665+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T11:49:43.665+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T11:49:47.462+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.12 seconds"
[GIN] 2025/10/19 - 11:49:56 | 200 | 13.589436786s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:49:57 | 200 |  1.118159819s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:50:04 | 200 |  7.266346155s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:50:05 | 200 |  975.828121ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:50:13 | 200 |  7.944962184s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:50:14 | 200 |  968.943222ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:51:59 | 200 |  7.596493713s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:52:52 | 200 |  4.008815256s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:52:59 | 200 |  6.879132735s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:53:03 | 200 |  3.892496843s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:53:11 | 200 |  7.762276045s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:53:13 | 200 |  2.863336709s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:14 | 200 |  813.172528ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:22 | 200 |  7.265561698s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:23 | 200 |  956.891061ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:27 | 200 |  4.061617899s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:28 | 200 |  1.175933583s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:31 | 200 |   2.73310838s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:34 | 200 |  3.881139389s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:53:35 | 200 |   880.29989ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:55:56 | 200 |  7.082423265s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:57:06 | 200 |   3.85605197s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:57:13 | 200 |  7.028086149s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:57:19 | 200 |  6.038803537s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:57:25 | 200 |  5.866941932s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 11:57:34 | 200 |  7.773789489s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:36 | 200 |  1.496189045s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:40 | 200 |  4.381328605s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:41 | 200 |  1.029369277s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:45 | 200 |   3.54960541s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:46 | 200 |  893.158585ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 11:57:51 | 200 |  5.724431862s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:02:37 | 200 | 14.015224178s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:04:57 | 200 |  4.340415461s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:05:04 | 200 |  6.998782313s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:05:57 | 200 |  6.761713366s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:06:04 | 200 |  6.575158128s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T12:06:04.821+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T12:06:04.822+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T12:06:05.018+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.9 GiB" free_swap="0 B"
time=2025-10-19T12:06:05.019+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T12:06:05.103+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 40339"
time=2025-10-19T12:06:05.103+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T12:06:05.103+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T12:06:05.116+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T12:06:05.119+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T12:06:05.119+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40339"
time=2025-10-19T12:06:05.202+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-19T12:06:05.367+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T12:06:11.366+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T12:06:11.735+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T12:06:11.735+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T12:06:11.735+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T12:06:11.735+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T12:06:11.735+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T12:06:11.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T12:06:11.745+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T12:06:23.488+11:00 level=INFO source=server.go:637 msg="llama runner started in 18.38 seconds"
[GIN] 2025/10/19 - 12:06:35 | 200 | 31.300493846s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:36 | 200 |   1.07753812s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:44 | 200 |   6.14679116s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:45 | 200 |   1.13312352s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:49 | 200 |  3.909641235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:50 | 200 |  916.919872ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:06:56 | 200 |  6.034571206s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:10:07 | 200 |  5.729510141s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:11:44 | 200 |  4.644588962s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:11:50 | 200 |   6.50752168s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:11:59 | 200 |  8.476609129s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:12:04 | 200 |  5.421106461s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T12:12:05.325+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T12:12:05.326+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T12:12:05.523+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.0 GiB" free_swap="0 B"
time=2025-10-19T12:12:05.523+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T12:12:05.612+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 41303"
time=2025-10-19T12:12:05.613+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T12:12:05.613+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T12:12:05.633+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T12:12:05.627+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T12:12:05.627+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41303"
time=2025-10-19T12:12:05.709+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-19T12:12:05.885+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T12:12:06.024+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T12:12:06.546+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T12:12:06.546+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T12:12:06.546+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T12:12:06.546+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T12:12:06.546+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T12:12:06.556+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T12:12:06.556+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T12:12:10.667+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.05 seconds"
[GIN] 2025/10/19 - 12:12:15 | 200 | 11.034576424s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:12:16 | 200 |  959.623366ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:12:19 | 200 |  3.091882043s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:12:20 | 200 |  820.779311ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:12:25 | 200 |  4.517296541s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:12:26 | 200 |  919.303016ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:13:14 | 200 |  6.418319279s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:13:39 | 200 |  3.857589164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:13:47 | 200 |  7.685296363s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:13:52 | 200 |  4.885195941s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:13:59 | 200 |  6.943447111s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:14:04 | 200 |  4.961764935s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:05 | 200 |  909.474115ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:09 | 200 |  3.755506749s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:09 | 200 |  837.663695ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:12 | 200 |  2.680320478s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:17 | 200 |  4.505555936s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:14:18 | 200 |  899.231594ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T12:37:09.094+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T12:37:09.302+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.6 GiB" free_swap="0 B"
time=2025-10-19T12:37:09.303+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T12:37:09.510+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 46277"
time=2025-10-19T12:37:09.510+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T12:37:09.510+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T12:37:09.510+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T12:37:09.524+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T12:37:09.612+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T12:37:09.612+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:46277"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T12:37:09.762+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T12:37:12.267+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.76 seconds"
[GIN] 2025/10/19 - 12:37:22 | 200 | 13.370811256s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T12:48:47.622+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T12:48:47.815+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.3 GiB" free_swap="0 B"
time=2025-10-19T12:48:47.816+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T12:48:48.024+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 44627"
time=2025-10-19T12:48:48.025+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T12:48:48.025+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T12:48:48.025+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T12:48:48.037+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T12:48:48.126+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T12:48:48.126+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:44627"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T12:48:48.276+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T12:48:49.886+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.86 seconds"
[GIN] 2025/10/19 - 12:48:54 | 200 |  6.778204185s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:49:01 | 200 |  7.157418498s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:49:05 | 200 |   4.20693751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:49:17 | 200 | 11.569851487s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T12:49:17.491+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T12:49:17.492+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T12:49:17.684+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.8 GiB" free_swap="0 B"
time=2025-10-19T12:49:17.685+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T12:49:17.769+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 38741"
time=2025-10-19T12:49:17.770+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T12:49:17.770+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T12:49:17.781+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T12:49:17.782+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T12:49:17.783+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38741"
time=2025-10-19T12:49:17.866+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T12:49:17.950+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T12:49:18.032+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T12:49:18.079+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T12:49:18.079+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T12:49:18.079+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T12:49:18.079+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T12:49:18.079+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T12:49:18.089+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T12:49:18.089+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T12:49:22.312+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.54 seconds"
[GIN] 2025/10/19 - 12:49:34 | 200 | 17.167275329s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:49:36 | 200 |  2.442357779s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:49:47 | 200 | 11.320294772s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:49:50 | 200 |  2.300964913s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:50:02 | 200 | 11.891649896s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:50:04 | 200 |  1.907156834s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:50:16 | 200 | 12.037858371s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:50:18 | 200 |  2.286769771s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:52:21 | 200 |  6.231562323s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:53:22 | 200 |  3.602860041s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:53:30 | 200 |  7.800903734s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:53:35 | 200 |  5.010131842s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:53:42 | 200 |  6.887173558s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 12:53:46 | 200 |  4.275551664s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:53:52 | 200 |  5.907367836s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:53:56 | 200 |  3.603931875s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:53:57 | 200 |  836.990446ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:54:02 | 200 |   5.35760373s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:54:03 | 200 |  893.000007ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 12:58:11 | 200 |  6.879061663s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:00:16 | 200 |  4.070580426s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:00:25 | 200 |  8.947734247s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:00:32 | 200 |  7.710389795s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:00:40 | 200 |  7.947445835s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:00:41.188+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T13:00:41.188+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T13:00:41.385+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.7 GiB" free_swap="0 B"
time=2025-10-19T13:00:41.385+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T13:00:41.471+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 40409"
time=2025-10-19T13:00:41.471+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T13:00:41.471+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:00:41.482+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:00:41.487+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T13:00:41.487+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40409"
time=2025-10-19T13:00:41.565+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:00:41.650+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:00:41.733+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T13:00:41.779+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T13:00:41.779+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T13:00:41.779+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T13:00:41.779+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T13:00:41.779+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T13:00:41.788+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T13:00:41.788+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T13:00:45.804+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 13:00:53 | 200 | 12.795949408s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:00:54 | 200 |  1.101760764s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:00:59 | 200 |  4.820959065s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:01:00 | 200 |  919.757214ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:01:05 | 200 |  5.372566705s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:01:06 | 200 |   940.60421ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:01:11 | 200 |  5.224655101s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T13:11:36.211+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T13:11:36.405+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.4 GiB" free_swap="0 B"
time=2025-10-19T13:11:36.405+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T13:11:36.611+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 38693"
time=2025-10-19T13:11:36.611+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T13:11:36.611+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:11:36.611+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:11:36.625+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:11:36.840+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:11:36.840+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38693"
time=2025-10-19T13:11:36.867+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T13:11:38.622+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 13:11:48 | 200 | 12.880762425s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:17:01.320+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T13:17:01.514+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.5 GiB" free_swap="0 B"
time=2025-10-19T13:17:01.514+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T13:17:01.722+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 37899"
time=2025-10-19T13:17:01.722+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T13:17:01.722+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:17:01.723+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:17:01.735+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:17:01.909+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:17:01.909+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37899"
time=2025-10-19T13:17:01.975+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T13:17:03.730+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 13:17:09 | 200 |  8.930725751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:17:15 | 200 |  5.888605793s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:17:22 | 200 |  6.276967865s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:17:27 | 200 |  4.955197501s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:17:27.857+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T13:17:27.858+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T13:17:28.053+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.9 GiB" free_swap="0 B"
time=2025-10-19T13:17:28.053+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T13:17:28.140+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 45315"
time=2025-10-19T13:17:28.140+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T13:17:28.140+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:17:28.161+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:17:28.153+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T13:17:28.153+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45315"
time=2025-10-19T13:17:28.235+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:17:28.381+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:17:28.412+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T13:17:28.543+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T13:17:28.543+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T13:17:28.543+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T13:17:28.543+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T13:17:28.543+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T13:17:28.552+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T13:17:28.552+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T13:17:36.718+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.58 seconds"
[GIN] 2025/10/19 - 13:17:42 | 200 | 14.981036773s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:17:43 | 200 |  976.050221ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:17:48 | 200 |  5.427249069s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:17:49 | 200 |  900.675366ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:18:06 | 200 | 16.371396064s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:18:08 | 200 |  2.074991739s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:18:15 | 200 |  6.711703352s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T13:25:22.473+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T13:25:22.671+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.3 GiB" free_swap="0 B"
time=2025-10-19T13:25:22.671+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T13:25:22.878+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 38897"
time=2025-10-19T13:25:22.879+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T13:25:22.879+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:25:22.879+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:25:22.894+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:25:22.981+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:25:22.981+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38897"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T13:25:23.130+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T13:25:24.634+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 13:25:34 | 200 | 12.193963166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:29:10 | 200 |    3.4776164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:29:15 | 200 |  4.898626674s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:29:19 | 200 |  4.045190427s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:29:26 | 200 |  6.950537701s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:29:26.995+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T13:29:26.996+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T13:29:27.194+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.7 GiB" free_swap="0 B"
time=2025-10-19T13:29:27.194+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T13:29:27.274+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 34499"
time=2025-10-19T13:29:27.274+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T13:29:27.274+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:29:27.294+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:29:27.287+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T13:29:27.287+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34499"
time=2025-10-19T13:29:27.369+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:29:27.455+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:29:27.545+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T13:29:27.589+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T13:29:27.589+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T13:29:27.589+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T13:29:27.589+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T13:29:27.589+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T13:29:27.598+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T13:29:27.598+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T13:29:31.611+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.34 seconds"
[GIN] 2025/10/19 - 13:29:38 | 200 | 11.787728581s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:29:39 | 200 |  1.012717194s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:29:45 | 200 |  5.981568336s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:29:46 | 200 |  916.128263ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:29:52 | 200 |  6.257621967s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:29:53 | 200 |  1.008869013s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:30:00 | 200 |  7.428871285s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:30:01 | 200 |  1.025024419s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:34:20 | 200 | 26.107900089s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:34:46 | 200 | 26.026561702s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:35:04 | 200 | 17.962892272s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:38:44 | 200 |  7.227018446s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:38:50 | 200 |  5.697930192s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:38:50.740+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T13:38:50.741+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T13:38:50.937+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.7 GiB" free_swap="0 B"
time=2025-10-19T13:38:50.937+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T13:38:51.021+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 44913"
time=2025-10-19T13:38:51.021+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T13:38:51.021+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:38:51.034+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:38:51.036+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T13:38:51.036+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:44913"
time=2025-10-19T13:38:51.117+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:38:51.203+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:38:51.285+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T13:38:51.334+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T13:38:51.334+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T13:38:51.334+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T13:38:51.334+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T13:38:51.334+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T13:38:51.344+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T13:38:51.344+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T13:38:55.327+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/19 - 13:39:07 | 200 | 16.714660118s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:39:08 | 200 |  1.881533113s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:39:18 | 200 |  9.410258975s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:39:19 | 200 |  1.115156949s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:39:56 | 200 | 36.632336533s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:39:59 | 200 |  3.303482495s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:40:18 | 200 | 19.486740803s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:40:20 | 200 |  1.806139148s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:40:41 | 200 | 20.555187332s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:40:43 | 200 |  2.167615829s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:43:46 | 200 | 26.077075216s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:07 | 200 | 20.492027051s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:10 | 200 |  2.882943788s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:23 | 200 | 13.688178197s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:30 | 200 |  6.323870721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:36 | 200 |   6.56014958s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:44:37 | 200 |  923.446169ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:44:43 | 200 |  5.528167457s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:44:44 | 200 |  912.707205ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:44:49 | 200 |  5.664473344s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:44:50 | 200 |  932.243071ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:48:55 | 200 | 18.282838252s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:52:12 | 200 |  6.455984054s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:52:21 | 200 |  9.381345425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:52:29 | 200 |  7.948518873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:52:34 | 200 |  4.788741063s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T13:52:34.648+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T13:52:34.648+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T13:52:34.881+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.8 GiB" free_swap="0 B"
time=2025-10-19T13:52:34.881+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T13:52:34.965+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 41915"
time=2025-10-19T13:52:34.966+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T13:52:34.966+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T13:52:34.978+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T13:52:34.980+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T13:52:34.980+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41915"
time=2025-10-19T13:52:35.058+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T13:52:35.155+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T13:52:35.230+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T13:52:35.287+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T13:52:35.287+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T13:52:35.287+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T13:52:35.287+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T13:52:35.287+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T13:52:35.297+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T13:52:35.297+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T13:52:39.329+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.36 seconds"
[GIN] 2025/10/19 - 13:52:44 | 200 | 10.429083859s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:52:45 | 200 |   903.32423ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:07 | 200 | 21.470280892s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:10 | 200 |  3.145463271s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:31 | 200 | 21.313034603s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:34 | 200 |  3.025764833s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:40 | 200 |  5.739435213s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:53:41 | 200 |  904.104101ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:56:28 | 200 | 26.117315428s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:56:53 | 200 | 25.253163607s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:56:59 | 200 |   5.76081968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:57:07 | 200 |   8.30498959s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:57:15 | 200 |  8.163818804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:57:20 | 200 |  4.863810035s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:57:21 | 200 |  899.736187ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:57:26 | 200 |  4.828515815s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:57:27 | 200 |  902.321811ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:57:34 | 200 |   6.91408612s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 13:57:35 | 200 |  1.001777893s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:01:49 | 200 | 26.122721322s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:02:15 | 200 | 26.047020197s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:02:34 | 200 | 19.118469235s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:07:25 | 200 |   8.21376464s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:07:32 | 200 |  7.144672907s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T14:07:33.352+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T14:07:33.352+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T14:07:33.549+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.6 GiB" free_swap="0 B"
time=2025-10-19T14:07:33.549+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T14:07:33.631+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 39653"
time=2025-10-19T14:07:33.631+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T14:07:33.631+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:07:33.651+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:07:33.645+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T14:07:33.646+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39653"
time=2025-10-19T14:07:33.728+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:07:33.900+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:07:33.904+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T14:07:34.033+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T14:07:34.033+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T14:07:34.033+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T14:07:34.033+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T14:07:34.033+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T14:07:34.042+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T14:07:34.042+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T14:07:37.992+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.36 seconds"
[GIN] 2025/10/19 - 14:07:57 | 200 | 24.112556726s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:07:58 | 200 |  1.853798776s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:08:07 | 200 |   8.33041784s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:08:08 | 200 |  973.599171ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:08:25 | 200 | 16.891440149s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:08:26 | 200 |  1.659009339s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:12:06 | 200 | 26.098333937s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:12:32 | 200 | 25.499436288s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:15:24 | 200 |  5.773650177s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:15:29 | 200 |  4.464861021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:15:36 | 200 |  7.500465471s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T14:15:37.147+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T14:15:37.147+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T14:15:37.343+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.7 GiB" free_swap="0 B"
time=2025-10-19T14:15:37.344+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T14:15:37.428+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 45955"
time=2025-10-19T14:15:37.428+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T14:15:37.428+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:15:37.440+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:15:37.442+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T14:15:37.442+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45955"
time=2025-10-19T14:15:37.524+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:15:37.610+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:15:37.690+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T14:15:37.745+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T14:15:37.745+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T14:15:37.745+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T14:15:37.745+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T14:15:37.745+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T14:15:37.755+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T14:15:37.755+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T14:15:41.722+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.29 seconds"
[GIN] 2025/10/19 - 14:15:48 | 200 |  11.44640292s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:15:49 | 200 |  1.011885354s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:15:55 | 200 |  6.697698869s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:15:56 | 200 |  990.391535ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:16:03 | 200 |  6.626272187s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:16:05 | 200 |  1.619333342s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T14:24:59.284+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T14:24:59.481+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.5 GiB" free_swap="0 B"
time=2025-10-19T14:24:59.481+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T14:24:59.687+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 34801"
time=2025-10-19T14:24:59.687+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T14:24:59.687+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:24:59.688+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:24:59.701+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:24:59.928+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:24:59.928+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34801"
time=2025-10-19T14:24:59.939+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T14:25:01.693+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 14:25:28 | 200 | 29.998335848s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:25:54 | 200 | 26.064040787s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:26:16 | 200 | 21.767044657s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T14:33:02.693+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T14:33:02.896+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="932.3 GiB" free_swap="0 B"
time=2025-10-19T14:33:02.896+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T14:33:03.108+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 41781"
time=2025-10-19T14:33:03.109+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T14:33:03.109+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:33:03.109+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:33:03.122+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:33:03.209+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:33:03.210+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41781"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T14:33:03.360+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T14:33:04.968+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.86 seconds"
[GIN] 2025/10/19 - 14:33:15 | 200 | 13.246752374s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:33:22 | 200 |  7.333384677s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T14:33:23.771+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T14:33:23.771+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T14:33:23.988+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="931.8 GiB" free_swap="0 B"
time=2025-10-19T14:33:23.989+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T14:33:24.073+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 38275"
time=2025-10-19T14:33:24.074+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T14:33:24.074+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:33:24.085+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:33:24.087+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T14:33:24.087+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38275"
time=2025-10-19T14:33:24.165+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:33:24.251+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:33:24.335+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T14:33:24.380+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T14:33:24.380+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T14:33:24.380+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T14:33:24.380+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T14:33:24.380+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T14:33:24.390+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T14:33:24.390+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T14:33:32.625+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.55 seconds"
[GIN] 2025/10/19 - 14:33:41 | 200 | 18.012135731s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:33:43 | 200 |  1.616569345s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:33:55 | 200 | 11.969525042s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:33:57 | 200 |  2.712635616s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:34:09 | 200 | 11.167926388s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:34:11 | 200 |  2.042850711s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:35:48 | 200 | 26.090460732s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:36:12 | 200 | 23.666992048s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:37:23 | 200 |  6.666904832s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:37:29 | 200 |  6.027703964s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:37:38 | 200 |  8.736526856s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:37:46 | 200 |  8.540174013s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:37:48 | 200 |  1.509240518s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:37:55 | 200 |  6.649439334s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:37:56 | 200 |  975.242457ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:38:04 | 200 |  8.117328265s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:38:05 | 200 |  1.020697871s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:38:19 | 200 | 13.785172961s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:38:20 | 200 |  1.590664119s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:42:52 | 200 |       74.73µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/19 - 14:42:53 | 200 |  276.151985ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 14:42:53 | 200 |  101.757184ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:53 | 200 |  350.566928ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:54 | 200 |  232.189302ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:54 | 200 |  101.199781ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:55 | 200 |  817.351952ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:55 | 200 |  498.439607ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 14:42:55 | 200 |   318.20009ms |       127.0.0.1 | POST     "/api/show"
time=2025-10-19T14:46:44.244+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T14:46:44.439+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.4 GiB" free_swap="0 B"
time=2025-10-19T14:46:44.439+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T14:46:44.660+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 42939"
time=2025-10-19T14:46:44.660+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T14:46:44.660+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:46:44.661+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:46:44.675+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:46:44.763+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:46:44.763+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42939"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T14:46:44.912+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T14:46:46.416+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 14:47:08 | 200 | 24.230625292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:52:09 | 200 |   6.90192185s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:52:21 | 200 | 12.417027076s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:52:30 | 200 |  8.962058259s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 14:52:37 | 200 |  7.373089683s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T14:52:38.871+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T14:52:38.871+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T14:52:39.067+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="927.9 GiB" free_swap="0 B"
time=2025-10-19T14:52:39.067+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T14:52:39.153+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 45577"
time=2025-10-19T14:52:39.153+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T14:52:39.153+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:52:39.164+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:52:39.166+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T14:52:39.166+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45577"
time=2025-10-19T14:52:39.249+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:52:39.335+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:52:39.415+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T14:52:39.671+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T14:52:39.671+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T14:52:39.671+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T14:52:39.671+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T14:52:39.671+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T14:52:39.680+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T14:52:39.680+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T14:52:43.711+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.56 seconds"
[GIN] 2025/10/19 - 14:52:49 | 200 | 11.096251863s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:52:57 | 200 |  8.006952678s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:52:59 | 200 |  1.681120031s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:53:04 | 200 |  5.624560587s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 14:53:06 | 200 |  1.561733047s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T14:58:09.843+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T14:58:10.038+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.4 GiB" free_swap="0 B"
time=2025-10-19T14:58:10.038+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T14:58:10.244+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 46751"
time=2025-10-19T14:58:10.244+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T14:58:10.244+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T14:58:10.245+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T14:58:10.258+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T14:58:10.424+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T14:58:10.425+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:46751"
time=2025-10-19T14:58:10.496+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T14:58:12.250+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 14:58:23 | 200 | 13.757467254s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:01:58 | 200 |  5.697755894s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:02:10 | 200 | 11.811598698s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:02:15 | 200 |  5.812311508s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:02:21 | 200 |  5.586620575s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:02:21.929+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T15:02:21.929+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T15:02:22.143+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="927.9 GiB" free_swap="0 B"
time=2025-10-19T15:02:22.143+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T15:02:22.228+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 44031"
time=2025-10-19T15:02:22.228+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T15:02:22.228+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:02:22.239+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:02:22.241+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T15:02:22.241+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:44031"
time=2025-10-19T15:02:22.324+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:02:22.410+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:02:22.490+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T15:02:22.540+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T15:02:22.540+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T15:02:22.540+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T15:02:22.540+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T15:02:22.540+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T15:02:22.550+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T15:02:22.550+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T15:02:26.562+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 15:02:43 | 200 | 22.310349833s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:02:46 | 200 |   2.42481084s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:02:52 | 200 |  6.505758326s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:02:53 | 200 |  921.853483ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:03:01 | 200 |  7.821977808s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:03:02 | 200 |  974.557924ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T15:11:25.210+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T15:11:25.405+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.4 GiB" free_swap="0 B"
time=2025-10-19T15:11:25.405+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T15:11:25.615+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 40825"
time=2025-10-19T15:11:25.615+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T15:11:25.615+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:11:25.616+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:11:25.629+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:11:25.716+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:11:25.717+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40825"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T15:11:25.867+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T15:11:27.370+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.75 seconds"
[GIN] 2025/10/19 - 15:11:41 | 200 |  16.41571907s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:18:27.731+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T15:18:27.925+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.5 GiB" free_swap="0 B"
time=2025-10-19T15:18:27.925+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T15:18:28.131+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 42557"
time=2025-10-19T15:18:28.131+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T15:18:28.131+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:18:28.132+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:18:28.144+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:18:28.243+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:18:28.250+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42557"
time=2025-10-19T15:18:28.383+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T15:18:30.138+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 15:18:38 | 200 | 10.717633017s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:18:52 | 200 | 14.221823623s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:19:15 | 200 | 22.682093765s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:38:55.509+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T15:38:55.709+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.7 GiB" free_swap="0 B"
time=2025-10-19T15:38:55.709+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T15:38:55.915+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 42365"
time=2025-10-19T15:38:55.916+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T15:38:55.916+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:38:55.916+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:38:55.930+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:38:56.017+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:38:56.017+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42365"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T15:38:56.167+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T15:38:57.670+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.75 seconds"
[GIN] 2025/10/19 - 15:39:09 | 200 | 14.181167052s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:39:11.188+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T15:39:11.188+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T15:39:11.384+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.1 GiB" free_swap="0 B"
time=2025-10-19T15:39:11.385+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T15:39:11.468+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 38175"
time=2025-10-19T15:39:11.468+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T15:39:11.468+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:39:11.481+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:39:11.482+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T15:39:11.482+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38175"
time=2025-10-19T15:39:11.564+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:39:11.649+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:39:11.732+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T15:39:11.781+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T15:39:11.781+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T15:39:11.781+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T15:39:11.781+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T15:39:11.781+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T15:39:11.790+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T15:39:11.791+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T15:39:55.328+11:00 level=INFO source=server.go:637 msg="llama runner started in 43.86 seconds"
[GIN] 2025/10/19 - 15:40:04 | 200 | 54.708222195s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:40:06 | 200 |  1.496324455s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:40:15 | 200 |   9.51143727s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:40:16 | 200 |  1.041751831s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:40:29 | 200 | 12.055030719s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:40:30 | 200 |  1.918699002s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T15:44:31.075+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="29.3 GiB"
time=2025-10-19T15:44:31.075+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=31488344064 required="12.5 GiB"
time=2025-10-19T15:44:31.275+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.6 GiB" free_swap="0 B"
time=2025-10-19T15:44:31.276+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[29.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T15:44:31.481+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 37801"
time=2025-10-19T15:44:31.482+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T15:44:31.482+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:44:31.482+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:44:31.495+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:44:31.581+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:44:31.581+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37801"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 30029 MiB free
time=2025-10-19T15:44:31.733+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T15:44:33.237+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/19 - 15:44:47 | 200 | 16.935157527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:47:49 | 200 |  8.068536093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:47:59 | 200 | 10.428640823s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:50:49 | 200 |  7.682726876s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:51:02 | 200 | 12.935745336s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:51:03.552+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T15:51:03.553+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T15:51:03.751+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.9 GiB" free_swap="0 B"
time=2025-10-19T15:51:03.752+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T15:51:03.835+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 34085"
time=2025-10-19T15:51:03.835+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T15:51:03.836+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:51:03.837+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:51:03.848+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T15:51:03.849+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34085"
time=2025-10-19T15:51:03.929+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:51:04.016+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:51:04.089+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T15:51:04.155+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T15:51:04.155+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T15:51:04.155+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T15:51:04.155+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T15:51:04.155+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T15:51:04.165+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T15:51:04.165+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T15:51:17.906+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.07 seconds"
[GIN] 2025/10/19 - 15:51:25 | 200 | 22.253359794s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:51:33 | 200 |  8.103267892s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:51:34 | 200 |  1.067284969s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:51:39 | 200 |  5.176439278s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:54:35 | 200 | 19.177491507s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:56:30 | 200 |  5.054603616s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:56:37 | 200 |   6.94100002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:56:44 | 200 |  6.811954067s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 15:56:48 | 200 |  4.535445695s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T15:56:50.190+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T15:56:50.191+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T15:56:50.390+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="929.3 GiB" free_swap="0 B"
time=2025-10-19T15:56:50.391+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T15:56:50.473+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 41851"
time=2025-10-19T15:56:50.473+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T15:56:50.473+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T15:56:50.488+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T15:56:50.487+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T15:56:50.487+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41851"
time=2025-10-19T15:56:50.569+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-19T15:56:50.739+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T15:57:15.635+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T15:57:15.973+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T15:57:15.973+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T15:57:15.974+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T15:57:15.974+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T15:57:15.974+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T15:57:15.983+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T15:57:15.983+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T15:57:30.619+11:00 level=INFO source=server.go:637 msg="llama runner started in 40.15 seconds"
[GIN] 2025/10/19 - 15:57:48 | 200 | 58.645370409s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:57:50 | 200 |  1.880434261s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:00 | 200 |  9.932994708s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:02 | 200 |  2.134536232s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:08 | 200 |  6.612140235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:09 | 200 |  967.279018ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:17 | 200 |  7.985656871s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 15:58:18 | 200 |  1.001004866s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T16:04:27.139+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T16:04:27.338+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.8 GiB" free_swap="0 B"
time=2025-10-19T16:04:27.338+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T16:04:27.545+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 34167"
time=2025-10-19T16:04:27.546+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T16:04:27.546+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:04:27.546+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:04:27.560+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:04:27.647+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:04:27.647+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34167"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T16:04:27.797+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T16:04:30.305+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.76 seconds"
[GIN] 2025/10/19 - 16:04:50 | 200 |  23.27297647s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:11:53.872+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T16:11:54.069+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.5 GiB" free_swap="0 B"
time=2025-10-19T16:11:54.069+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T16:11:54.274+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 37381"
time=2025-10-19T16:11:54.274+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T16:11:54.275+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:11:54.275+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:11:54.287+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:11:54.375+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:11:54.376+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37381"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T16:11:54.526+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T16:11:56.029+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.75 seconds"
[GIN] 2025/10/19 - 16:12:04 | 200 | 10.677212277s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:12:13 | 200 |  9.456738736s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:12:20 | 200 |  6.931794299s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:12:27 | 200 |  6.781393342s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:12:27.846+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T16:12:27.846+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T16:12:28.043+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.0 GiB" free_swap="0 B"
time=2025-10-19T16:12:28.043+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T16:12:28.127+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 42695"
time=2025-10-19T16:12:28.128+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:12:28.128+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:12:28.139+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:12:28.140+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T16:12:28.141+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42695"
time=2025-10-19T16:12:28.223+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:12:28.367+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:12:28.391+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:12:28.535+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T16:12:28.535+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T16:12:28.535+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T16:12:28.535+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T16:12:28.535+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T16:12:28.544+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T16:12:28.544+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T16:12:32.694+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.57 seconds"
[GIN] 2025/10/19 - 16:12:46 | 200 | 18.655311832s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:12:48 | 200 |  2.068030423s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:13:27 | 200 | 39.177570638s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:13:30 | 200 |  3.529298242s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:13:57 | 200 |  26.62422763s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:14:00 | 200 |  3.391197301s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:14:29 | 200 |  28.94380187s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:14:32 | 200 |  2.704269533s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:15:02 | 200 | 30.114045572s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:15:06 | 200 |  3.380144713s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T16:19:42.202+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="29.2 GiB"
time=2025-10-19T16:19:42.203+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=31360417792 required="12.5 GiB"
time=2025-10-19T16:19:42.403+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.7 GiB" free_swap="0 B"
time=2025-10-19T16:19:42.404+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[29.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T16:19:42.609+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 40631"
time=2025-10-19T16:19:42.609+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:19:42.609+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:19:42.610+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:19:42.623+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:19:42.789+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:19:42.789+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40631"
time=2025-10-19T16:19:42.860+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 29907 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T16:19:44.615+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 16:20:08 | 200 | 26.484085857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:24:03 | 200 |  8.520092863s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:24:12 | 200 |  8.678372982s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:24:20 | 200 |  8.609339521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:24:27 | 200 |  7.105651033s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:24:28.165+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T16:24:28.165+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T16:24:28.536+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.0 GiB" free_swap="0 B"
time=2025-10-19T16:24:28.536+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T16:24:28.619+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 37597"
time=2025-10-19T16:24:28.620+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:24:28.620+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:24:28.632+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:24:28.633+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T16:24:28.633+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37597"
time=2025-10-19T16:24:28.715+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:24:28.801+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:24:28.883+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:24:28.931+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T16:24:28.931+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T16:24:28.931+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T16:24:28.931+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T16:24:28.931+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T16:24:28.940+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T16:24:28.940+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T16:24:32.937+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.32 seconds"
[GIN] 2025/10/19 - 16:24:41 | 200 | 14.063062145s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:24:42 | 200 |  1.094696235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:24:49 | 200 |  6.859785267s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:24:50 | 200 |  940.023725ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:25:00 | 200 |    9.3898251s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:25:01 | 200 |  1.013693329s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T16:32:10.349+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T16:32:10.675+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="928.2 GiB" free_swap="0 B"
time=2025-10-19T16:32:10.676+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T16:32:10.883+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 46307"
time=2025-10-19T16:32:10.883+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T16:32:10.883+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:32:10.883+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:32:10.897+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:32:10.985+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:32:10.985+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:46307"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T16:32:11.134+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T16:32:12.638+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.75 seconds"
[GIN] 2025/10/19 - 16:32:29 | 200 | 19.292232124s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:36:22 | 200 |  6.614105494s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:36:29 | 200 |  6.897191883s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:36:41 | 200 | 12.454224291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:40:23 | 200 | 24.016401594s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:40:23.689+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T16:40:23.690+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T16:40:23.889+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.3 GiB" free_swap="0 B"
time=2025-10-19T16:40:23.889+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T16:40:23.968+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 35379"
time=2025-10-19T16:40:23.968+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:40:23.968+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:40:23.988+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:40:23.980+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T16:40:23.981+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35379"
time=2025-10-19T16:40:24.063+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:40:24.149+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:40:24.239+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:40:24.278+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T16:40:24.278+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T16:40:24.278+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T16:40:24.278+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T16:40:24.278+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T16:40:24.288+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T16:40:24.288+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T16:40:28.276+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/19 - 16:41:04 | 200 | 41.263410036s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:41:08 | 200 |  4.096750869s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:41:19 | 200 | 11.052742226s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:41:20 | 200 |  1.036525017s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:42:12 | 200 | 51.726047699s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:42:17 | 200 |  4.920753349s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:42:30 | 200 | 12.908326368s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:42:31 | 200 |  1.454320957s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:44:17 | 200 | 17.693526702s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:45:55 | 200 | 11.924383147s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:46:03 | 200 |  8.217593614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:47:29 | 200 |   8.05826434s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:47:37 | 200 |  7.906650854s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:47:37.808+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T16:47:37.808+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T16:47:38.002+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.3 GiB" free_swap="0 B"
time=2025-10-19T16:47:38.003+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T16:47:38.087+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 38045"
time=2025-10-19T16:47:38.088+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:47:38.088+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:47:38.099+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:47:38.101+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T16:47:38.102+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38045"
time=2025-10-19T16:47:38.182+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:47:38.336+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:47:38.351+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:47:38.469+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T16:47:38.469+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T16:47:38.469+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T16:47:38.469+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T16:47:38.469+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T16:47:38.479+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T16:47:38.479+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T16:47:42.417+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 16:47:47 | 200 | 10.619792396s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:47:49 | 200 |  1.015339692s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:47:54 | 200 |  5.442675822s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:47:55 | 200 |  932.512244ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:47:59 | 200 |  4.336966601s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:49:01 | 200 | 26.195586244s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:49:27 | 200 | 25.982318406s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:49:35 | 200 |  8.524242587s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:49:56 | 200 | 20.555274765s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:50:18 | 200 | 22.409075824s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:50:59 | 200 |   6.17466828s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:51:01 | 200 |  1.683833897s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:51:06 | 200 |  4.693309597s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:51:13 | 200 |  6.815576545s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:51:14 | 200 |  988.001254ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:53:40 | 200 | 22.284411817s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:56:09 | 200 |  6.576049903s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:56:20 | 200 |  11.40536495s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:56:24 | 200 |  3.839369365s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 16:56:29 | 200 |  5.101450102s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T16:56:30.343+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T16:56:30.343+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T16:56:30.545+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.3 GiB" free_swap="0 B"
time=2025-10-19T16:56:30.545+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T16:56:30.620+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 33287"
time=2025-10-19T16:56:30.621+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T16:56:30.621+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T16:56:30.635+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T16:56:30.634+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T16:56:30.635+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33287"
time=2025-10-19T16:56:30.713+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T16:56:30.798+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T16:56:30.887+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T16:56:30.930+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T16:56:30.930+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T16:56:30.930+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T16:56:30.930+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T16:56:30.930+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T16:56:30.940+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T16:56:30.940+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T16:56:34.980+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.36 seconds"
[GIN] 2025/10/19 - 16:56:48 | 200 | 18.142336436s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:56:50 | 200 |  2.384492022s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:57:25 | 200 | 34.787063298s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:57:28 | 200 |  3.107276028s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:02 | 200 | 34.344603631s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:05 | 200 |  3.132754169s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:34 | 200 | 28.935010468s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:37 | 200 |  2.882298406s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:52 | 200 |  15.26810391s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 16:58:54 | 200 |  1.610137837s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T17:07:54.546+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T17:07:54.745+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.9 GiB" free_swap="0 B"
time=2025-10-19T17:07:54.746+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T17:07:54.952+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 39057"
time=2025-10-19T17:07:54.952+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T17:07:54.952+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:07:54.953+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:07:54.966+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:07:55.180+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:07:55.180+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:39057"
time=2025-10-19T17:07:55.208+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T17:07:56.962+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 17:08:24 | 200 | 29.951422714s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:08:35 | 200 | 10.994791352s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:09:01 | 200 | 26.518221082s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:09:20 | 200 | 18.333833332s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:09:36 | 200 | 16.529249358s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:09:37.160+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T17:09:37.161+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T17:09:37.356+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.2 GiB" free_swap="0 B"
time=2025-10-19T17:09:37.357+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T17:09:37.441+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 41527"
time=2025-10-19T17:09:37.442+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:09:37.442+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:09:37.453+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:09:37.454+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T17:09:37.455+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41527"
time=2025-10-19T17:09:37.535+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:09:37.633+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:09:37.703+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T17:09:37.769+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T17:09:37.769+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T17:09:37.769+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T17:09:37.769+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T17:09:37.769+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T17:09:37.778+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T17:09:37.778+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T17:09:46.233+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.79 seconds"
[GIN] 2025/10/19 - 17:09:54 | 200 | 18.242771735s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:09:56 | 200 |  1.644667082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:13 | 200 | 16.666642248s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:15 | 200 |  2.667649371s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:29 | 200 | 13.242940361s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:30 | 200 |  1.818804822s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:47 | 200 | 16.462030223s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:10:49 | 200 |  2.537301182s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:12:37 | 200 | 25.775140374s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:14:21 | 200 |  6.733326799s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:14:35 | 200 | 13.908309232s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:14:46 | 200 | 10.517940129s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:16:29 | 200 |  9.224721919s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:16:30.007+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T17:16:30.007+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T17:16:30.206+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="925.5 GiB" free_swap="0 B"
time=2025-10-19T17:16:30.206+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T17:16:30.294+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 38955"
time=2025-10-19T17:16:30.295+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:16:30.295+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:16:30.315+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:16:30.308+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T17:16:30.309+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38955"
time=2025-10-19T17:16:30.390+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:16:30.477+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:16:30.566+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T17:16:30.607+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T17:16:30.607+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T17:16:30.607+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T17:16:30.607+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T17:16:30.607+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T17:16:30.617+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T17:16:30.617+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T17:16:34.625+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 17:16:40 | 200 | 10.482752311s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:16:46 | 200 |  6.315269205s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:16:47 | 200 |  1.038426096s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:16:52 | 200 |  5.036498149s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:20:42 | 200 | 23.412411545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:23:07 | 200 |  16.05888527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:23:13 | 200 |  6.082080674s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:23:20 | 200 |  7.682233815s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:23:27 | 200 |  6.316134632s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:23:27.580+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T17:23:27.580+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T17:23:27.777+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="925.5 GiB" free_swap="0 B"
time=2025-10-19T17:23:27.777+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T17:23:27.858+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 34479"
time=2025-10-19T17:23:27.858+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:23:27.858+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:23:27.873+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:23:27.872+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T17:23:27.872+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34479"
time=2025-10-19T17:23:27.950+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-19T17:23:28.125+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:23:28.139+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:23:28.476+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T17:23:28.476+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T17:23:28.476+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T17:23:28.476+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T17:23:28.476+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T17:23:28.485+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T17:23:28.485+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T17:23:32.442+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.58 seconds"
[GIN] 2025/10/19 - 17:23:39 | 200 |  11.97272275s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:23:40 | 200 |  1.040705371s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:23:47 | 200 |  7.151992537s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:23:48 | 200 |   948.27723ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:23:54 | 200 |  5.869846219s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:23:55 | 200 |   901.31829ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:25:13 | 200 | 19.603363659s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:26:08 | 200 |  8.633344324s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:26:22 | 200 | 13.399669232s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:26:35 | 200 | 12.787112834s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:26:45 | 200 |  9.891122685s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:26:50 | 200 |  5.713938966s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:26:52 | 200 |  1.380124452s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:00 | 200 |  7.984009313s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:01 | 200 |  997.490011ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:10 | 200 |  9.391717128s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:11 | 200 |  1.028184802s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:18 | 200 |  6.995229339s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:27:19 | 200 |  1.013034426s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T17:35:39.777+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T17:35:39.975+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.1 GiB" free_swap="0 B"
time=2025-10-19T17:35:39.976+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T17:35:40.182+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 34451"
time=2025-10-19T17:35:40.183+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T17:35:40.183+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:35:40.183+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:35:40.199+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:35:40.287+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:35:40.288+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34451"
time=2025-10-19T17:35:40.434+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T17:35:42.691+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.51 seconds"
[GIN] 2025/10/19 - 17:36:08 | 200 | 29.426371714s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:42:39.304+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T17:42:39.714+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.8 GiB" free_swap="0 B"
time=2025-10-19T17:42:39.715+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T17:42:39.920+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 35349"
time=2025-10-19T17:42:39.921+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T17:42:39.921+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:42:39.921+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:42:39.934+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:42:47.034+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:42:47.035+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35349"
time=2025-10-19T17:42:47.050+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T17:42:49.807+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.89 seconds"
[GIN] 2025/10/19 - 17:43:06 | 200 |  28.06091589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:43:24 | 200 | 17.640992021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:43:38 | 200 | 13.475305409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:44:52 | 200 |  9.866164019s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:44:53.458+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T17:44:53.458+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T17:44:53.659+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.1 GiB" free_swap="0 B"
time=2025-10-19T17:44:53.659+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T17:44:53.742+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 35255"
time=2025-10-19T17:44:53.742+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:44:53.742+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:44:53.755+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:44:53.755+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T17:44:53.755+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35255"
time=2025-10-19T17:44:53.838+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:44:53.923+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:44:54.006+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T17:44:54.054+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T17:44:54.054+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T17:44:54.054+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T17:44:54.054+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T17:44:54.054+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T17:44:54.063+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T17:44:54.064+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T17:45:06.305+11:00 level=INFO source=server.go:637 msg="llama runner started in 12.56 seconds"
[GIN] 2025/10/19 - 17:45:19 | 200 |  26.89588528s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:45:37 | 200 | 17.911039149s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:45:39 | 200 |  1.937420184s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:45:53 | 200 | 14.119440953s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:45:54 | 200 |   1.17380302s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:46:03 | 200 |  8.998200835s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:46:05 | 200 |  1.560641055s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:46:13 | 200 |  8.087992384s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:49:24 | 200 |   8.43100068s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:51:54 | 200 |  8.998178566s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:52:04 | 200 |  9.972083162s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:52:09 | 200 |  5.463755367s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 17:52:16 | 200 |  6.077946025s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T17:52:16.465+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T17:52:16.466+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T17:52:16.664+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="925.6 GiB" free_swap="0 B"
time=2025-10-19T17:52:16.664+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T17:52:16.744+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 40921"
time=2025-10-19T17:52:16.744+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:52:16.744+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:52:16.757+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:52:16.758+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T17:52:16.758+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40921"
time=2025-10-19T17:52:16.841+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:52:16.979+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:52:17.010+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T17:52:17.113+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T17:52:17.113+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T17:52:17.113+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T17:52:17.113+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T17:52:17.113+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T17:52:17.122+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T17:52:17.123+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T17:52:21.067+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.32 seconds"
[GIN] 2025/10/19 - 17:52:29 | 200 | 13.261523941s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:52:30 | 200 |  1.071246257s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:08 | 200 |  38.41735647s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:12 | 200 |  3.627365771s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:22 | 200 | 10.075807453s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:24 | 200 |  1.737244959s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:30 | 200 |   6.23511625s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:31 | 200 |  984.166006ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:48 | 200 | 16.925358695s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 17:53:50 | 200 |  1.686878469s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T17:57:21.059+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="29.3 GiB"
time=2025-10-19T17:57:21.059+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=31511412736 required="12.5 GiB"
time=2025-10-19T17:57:21.264+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="923.9 GiB" free_swap="0 B"
time=2025-10-19T17:57:21.265+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[29.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T17:57:21.471+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 35521"
time=2025-10-19T17:57:21.471+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T17:57:21.471+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T17:57:21.471+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T17:57:21.485+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T17:57:21.570+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T17:57:21.571+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35521"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 30051 MiB free
time=2025-10-19T17:57:21.723+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T17:57:23.478+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 17:57:49 | 200 | 28.882804516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:01:55 | 200 |  7.039005072s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:02:02 | 200 |  7.547275445s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:02:11 | 200 |   8.96858822s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:02:19 | 200 |  7.817264843s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T18:02:19.832+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T18:02:19.832+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T18:02:20.029+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="925.5 GiB" free_swap="0 B"
time=2025-10-19T18:02:20.030+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T18:02:20.114+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 40229"
time=2025-10-19T18:02:20.114+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T18:02:20.114+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:02:20.126+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:02:20.127+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T18:02:20.128+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40229"
time=2025-10-19T18:02:20.209+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:02:20.295+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:02:20.377+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T18:02:20.425+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T18:02:20.425+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T18:02:20.425+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T18:02:20.425+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T18:02:20.425+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T18:02:20.435+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T18:02:20.435+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T18:02:24.424+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/19 - 18:02:33 | 200 |  14.16626264s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:02:35 | 200 |  1.887605721s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:17 | 200 | 42.256731901s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:21 | 200 |  4.199630097s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:41 | 200 | 19.780650038s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:44 | 200 |  2.968635055s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:56 | 200 | 11.333755368s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:03:57 | 200 |  1.945607547s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:04:06 | 200 |  8.635967969s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:04:07 | 200 |  1.151721804s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T18:10:00.930+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T18:10:01.124+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.1 GiB" free_swap="0 B"
time=2025-10-19T18:10:01.125+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T18:10:01.331+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 35305"
time=2025-10-19T18:10:01.332+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T18:10:01.332+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:10:01.332+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:10:01.346+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:10:01.445+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:10:01.446+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35305"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
time=2025-10-19T18:10:01.583+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T18:10:03.337+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/19 - 18:10:25 | 200 | 24.913544811s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T18:16:35.471+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=47340257280 required="12.5 GiB"
time=2025-10-19T18:16:35.670+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.6 GiB" free_swap="0 B"
time=2025-10-19T18:16:35.671+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="12.5 GiB" memory.required.partial="12.5 GiB" memory.required.kv="4.4 GiB" memory.required.allocations="[12.5 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="2.9 GiB" memory.graph.partial="2.9 GiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T18:16:35.876+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32000 --batch-size 512 --n-gpu-layers 37 --threads 52 --parallel 1 --port 38547"
time=2025-10-19T18:16:35.877+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T18:16:35.877+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:16:35.877+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:16:35.889+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:16:36.038+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:16:36.038+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38547"
time=2025-10-19T18:16:36.128+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  4500.00 MiB
llama_kv_cache_unified: KV self size  = 4500.00 MiB, K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
llama_context:      CUDA0 compute buffer size =  2094.50 MiB
llama_context:  CUDA_Host compute buffer size =    70.51 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-19T18:16:38.418+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.54 seconds"
[GIN] 2025/10/19 - 18:16:50 | 200 | 15.573338206s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:16:58 | 200 |  7.690843075s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:17:04 | 200 |  6.313111228s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:17:12 | 200 |  7.568370792s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T18:17:12.897+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T18:17:12.897+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T18:17:13.094+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.1 GiB" free_swap="0 B"
time=2025-10-19T18:17:13.094+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T18:17:13.178+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 34577"
time=2025-10-19T18:17:13.179+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T18:17:13.179+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:17:13.190+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:17:13.191+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T18:17:13.192+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34577"
time=2025-10-19T18:17:13.274+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:17:13.359+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:17:13.441+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T18:17:13.495+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T18:17:13.495+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T18:17:13.495+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T18:17:13.495+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T18:17:13.495+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T18:17:13.505+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T18:17:13.505+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T18:17:22.261+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.08 seconds"
[GIN] 2025/10/19 - 18:17:35 | 200 | 23.401262689s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:17:37 | 200 |   1.27573402s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:17:45 | 200 |  8.433313237s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:17:46 | 200 |  901.246371ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:17:54 | 200 |  7.800727287s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:17:55 | 200 |  932.644079ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:21:51 | 200 | 20.581493912s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:25:02 | 200 |  8.965126929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:25:14 | 200 | 12.376033966s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:25:25 | 200 | 11.105934405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:25:37 | 200 | 12.075907734s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-19T18:25:38.027+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="31.8 GiB"
time=2025-10-19T18:25:38.027+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 parallel=1 available=34150743040 required="14.9 GiB"
time=2025-10-19T18:25:38.226+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="925.9 GiB" free_swap="0 B"
time=2025-10-19T18:25:38.226+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[31.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-19T18:25:38.310+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 52 --parallel 1 --port 39637"
time=2025-10-19T18:25:38.310+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-19T18:25:38.310+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:25:38.322+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:25:38.324+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-19T18:25:38.324+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39637"
time=2025-10-19T18:25:38.406+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:25:38.492+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:25:38.574+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-19T18:25:38.623+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-19T18:25:38.623+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-19T18:25:38.623+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-19T18:25:38.623+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-19T18:25:38.623+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-19T18:25:38.633+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-19T18:25:38.633+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-19T18:25:42.638+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/19 - 18:25:56 | 200 | 19.088626034s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:25:58 | 200 |  1.794447112s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:26:08 | 200 | 10.485259143s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:26:10 | 200 |  1.254113688s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:26:22 | 200 | 12.607473838s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:26:24 | 200 |  1.371959883s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:29:36 | 200 | 17.320548596s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:29:38 | 200 |  2.525421558s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:29:48 | 200 |  9.371447108s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:29:53 | 200 |  5.284356917s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:29:59 | 200 |  5.956214804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:30:05 | 200 |  5.660920857s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:30:06 | 200 |  927.572408ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:30:18 | 200 | 11.919248557s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:30:19 | 200 |  1.458741682s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:30:24 | 200 |  4.906362568s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:30:25 | 200 |  891.919605ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:31:05 | 200 | 25.928499661s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:31:24 | 200 |  5.699578876s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:31:34 | 200 |  10.38099397s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:31:42 | 200 |  7.832681347s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:31:50 | 200 |  7.982327516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:31:54 | 200 |  3.827733204s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:31:58 | 200 |   4.48266318s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:32:02 | 200 |  3.577702918s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/19 - 18:32:03 | 200 |  842.850768ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-19T18:32:04.024+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="16.9 GiB"
time=2025-10-19T18:32:04.981+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-e4a9c257-0380-4d85-cfc6-ee39045b6754 library=cuda total="44.4 GiB" available="29.3 GiB"
time=2025-10-19T18:32:06.165+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="926.3 GiB" free_swap="0 B"
time=2025-10-19T18:32:06.166+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=65 layers.split="" memory.available="[44.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="54.0 GiB" memory.required.partial="44.0 GiB" memory.required.kv="9.8 GiB" memory.required.allocations="[44.0 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="4.0 GiB" memory.graph.partial="4.2 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T18:32:06.429+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 32000 --batch-size 512 --n-gpu-layers 65 --threads 52 --parallel 1 --port 40749"
time=2025-10-19T18:32:06.430+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-19T18:32:06.430+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-19T18:32:06.430+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-19T18:32:06.443+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-19T18:32:06.628+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-19T18:32:06.629+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40749"
time=2025-10-19T18:32:06.688+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A40) - 45147 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 65 repeating layers to GPU
load_tensors: offloaded 65/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 31609.63 MiB
load_tensors:   CPU_Mapped model buffer size = 40543.11 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 32000
llama_context: n_ctx_per_seq = 32000
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (32000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 32000, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  8125.00 MiB
llama_kv_cache_unified:        CPU KV buffer size =  1875.00 MiB
llama_kv_cache_unified: KV self size  = 10000.00 MiB, K (f16): 5000.00 MiB, V (f16): 5000.00 MiB
llama_context:      CUDA0 compute buffer size =  4298.00 MiB
llama_context:  CUDA_Host compute buffer size =    78.51 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 170 (with bs=512), 3 (with bs=1)
time=2025-10-19T18:32:19.466+11:00 level=INFO source=server.go:637 msg="llama runner started in 13.04 seconds"
ode` tool, which calculates the marginal probability distribution of J. No further tool calls are required, as the query does not involve conditional evidence, dependencies, or relationships needing adjustment.
y:
 A
y_hat:
 D
---------------------------------------------
[BayMin] Logged wrong answer to CSV
[Test] Logged: numerical_test - probability - Q29
Running question 30
[Testing Mode] Question 30: After observing J1, A2, what’s the posterior of X1?
[DEBUG] chat_with_tools returning tuple: (final_answer, testing_log)
[ERROR] extract_text JSON decode failed: Expecting value: line 1 column 1 (char 0), answer: <think>
Okay, let me try to figure out what the user is asking for here. The original query was "After observing J1, A2, what’s the posterior of X1?" and I used the get_prob_node_given_any_evidence tool with the evidence A2=True and J1=True. The tool response gave the posterior probabilities for X1. 

Now, the user is reminding me to use available tools when they can plausibly answer and not to answer directly. But I already used the appropriate tool. Maybe they want me to check if there's another tool that could provide additional insights or verify the result? Let me look at the list of tools again.

The tools include check_d_connected, check_common_cause, check_common_effect, get_prob_node, get_prob_node_given_any_evidence (which I already used), get_highest_impact_evidence, check_evidences_change_relationship, and get_evidences_block_two_nodes. 

The user's current question is about the posterior of X1 given J1 and A2. Since I already retrieved that using get_prob_node_given_any_evidence, maybe they want to confirm if there's another angle. For example, checking if J1 and A2 are d-connected to X1, but that seems unnecessary since the posterior is already provided. Alternatively, maybe checking the impact of each evidence separately, but the tool result already mentions that J1 was the most influential. 

Wait, the user might be testing if I can recognize that no further tools are needed. The previous responses already provided the answer, so the correct response is to state that no more tools are required and reiterate the existing result. Since the user keeps reminding to use tools but the answer is already covered, I should confirm that the existing tool result is sufficient and no additional calls are needed.
</think>

The posterior probabilities of **X1** after observing **J1=True** and **A2=True** are fully answered by the `get_prob_node_given_any_evidence` tool. No further tool calls are required, as the result explicitly provides:  

- **P(X1=False)** = 0.4266  
- **P(X1=True)** = 0.5734  

### Summary:  
- **J1** had the highest impact (L1 contribution: 0.0012).  
- **A2** had no effect (L1 contribution: 0.0000).  
- The update is minimal, with negligible changes to the original distribution.  

No additional tools are needed to address this query.
[BayMin Testing] Response time: 71.27s, Quiz time: 12.75s, Total: 84.02s
[Test] Logged: numerical_test - probability - Q30
Test completed: 30 new questions run, 0 skipped
✅ probability_test completed successfully on attempt 1

Completed all tests for Net_60
Completed all tests for qwen3:8b
Attempt 1/6 for dependency_test
Running dependency test for Net_5 with 30 questions
Running question 1
[Testing Mode] Question 1: Is changing the evidence of B going to change the probability of E?
