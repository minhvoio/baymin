time=2025-10-25T12:48:43.452+11:00 level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/mvo1/lb64_scratch/ollama-models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-10-25T12:48:43.560+11:00 level=INFO source=images.go:477 msg="total blobs: 34"
time=2025-10-25T12:48:43.566+11:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-10-25T12:48:43.570+11:00 level=INFO source=routes.go:1357 msg="Listening on 127.0.0.1:11434 (version 0.11.4)"
time=2025-10-25T12:48:43.570+11:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-10-25T12:48:44.433+11:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda variant=v12 compute=8.0 driver=12.2 name="NVIDIA A100 80GB PCIe" total="79.2 GiB" available="78.7 GiB"
[GIN] 2025/10/25 - 12:48:46 | 200 |   61.650527ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-25T12:57:53.648+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544192512 required="41.8 GiB"
time=2025-10-25T12:57:53.843+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="975.4 GiB" free_swap="0 B"
time=2025-10-25T12:57:53.844+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T12:57:54.162+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 45393"
time=2025-10-25T12:57:54.162+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T12:57:54.162+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T12:57:54.162+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T12:57:54.177+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T12:58:01.106+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T12:58:01.130+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:45393"
time=2025-10-25T12:58:01.178+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
time=2025-10-25T12:58:03.383+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T12:58:04.535+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T12:58:16.560+11:00 level=INFO source=server.go:637 msg="llama runner started in 22.40 seconds"
[GIN] 2025/10/25 - 12:58:26 | 200 | 32.837604876s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T12:58:26.938+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="37.1 GiB"
time=2025-10-25T12:58:26.938+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39854407680 required="14.9 GiB"
time=2025-10-25T12:58:27.141+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.3 GiB" free_swap="0 B"
time=2025-10-25T12:58:27.142+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[37.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T12:58:27.219+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 44733"
time=2025-10-25T12:58:27.219+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T12:58:27.219+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T12:58:27.240+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T12:58:27.232+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T12:58:27.232+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:44733"
time=2025-10-25T12:58:27.319+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T12:58:27.405+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T12:58:27.490+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T12:58:27.538+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T12:58:27.538+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T12:58:27.538+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T12:58:27.538+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T12:58:27.538+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T12:58:27.548+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T12:58:27.548+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T12:58:39.831+11:00 level=INFO source=server.go:637 msg="llama runner started in 12.61 seconds"
[GIN] 2025/10/25 - 12:58:47 | 200 | 21.430139937s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 12:58:56 | 200 |  8.556671953s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 12:58:59 | 200 |  2.905188569s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 12:58:59 | 200 |  788.585723ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 12:59:09 | 200 |  9.678286531s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 12:59:10 | 200 |  1.220598338s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:11:19.790+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T13:11:19.990+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.8 GiB" free_swap="0 B"
time=2025-10-25T13:11:19.990+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T13:11:20.289+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 34959"
time=2025-10-25T13:11:20.289+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T13:11:20.289+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:11:20.289+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:11:20.304+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:11:20.391+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:11:20.392+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34959"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T13:11:20.540+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T13:11:36.326+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.04 seconds"
[GIN] 2025/10/25 - 13:11:39 | 200 | 20.015689226s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:11:39.911+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="37.1 GiB"
time=2025-10-25T13:11:39.911+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39850213376 required="14.9 GiB"
time=2025-10-25T13:11:40.112+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.4 GiB" free_swap="0 B"
time=2025-10-25T13:11:40.112+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[37.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T13:11:40.197+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 43273"
time=2025-10-25T13:11:40.198+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:11:40.198+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:11:40.209+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:11:40.211+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T13:11:40.211+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43273"
time=2025-10-25T13:11:40.303+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:11:40.401+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:11:40.460+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T13:11:40.536+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T13:11:40.536+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T13:11:40.536+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T13:11:40.536+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T13:11:40.536+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T13:11:40.545+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T13:11:40.545+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T13:11:44.735+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.54 seconds"
[GIN] 2025/10/25 - 13:11:59 | 200 | 20.405577804s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:12:15 | 200 | 15.668241435s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:12:17 | 200 |  1.663986082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:12:25 | 200 |  8.260399243s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:12:31 | 200 |  5.690416173s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:18:33.685+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T13:18:33.884+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.1 GiB" free_swap="0 B"
time=2025-10-25T13:18:33.885+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T13:18:34.178+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 34813"
time=2025-10-25T13:18:34.179+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T13:18:34.179+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:18:34.179+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:18:34.193+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:18:34.280+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:18:34.280+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34813"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T13:18:34.430+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T13:18:48.209+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.03 seconds"
[GIN] 2025/10/25 - 13:18:51 | 200 | 17.710312781s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:18:51.519+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="37.1 GiB"
time=2025-10-25T13:18:51.519+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39854407680 required="14.9 GiB"
time=2025-10-25T13:18:51.722+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.2 GiB" free_swap="0 B"
time=2025-10-25T13:18:51.722+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[37.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T13:18:51.799+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 36467"
time=2025-10-25T13:18:51.800+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:18:51.800+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:18:51.820+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:18:51.812+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T13:18:51.813+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36467"
time=2025-10-25T13:18:51.900+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:18:51.985+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:18:52.071+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T13:18:52.119+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T13:18:52.119+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T13:18:52.119+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T13:18:52.119+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T13:18:52.119+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T13:18:52.128+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T13:18:52.128+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T13:18:56.361+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.56 seconds"
[GIN] 2025/10/25 - 13:19:10 | 200 | 19.836533254s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:19:19 | 200 |  8.175171799s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:19:32 | 200 | 12.936664931s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:19:40 | 200 |  8.826856451s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:19:51 | 200 | 10.321044486s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:23:16.852+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23296 keep=5 new=4096
[GIN] 2025/10/25 - 13:23:34 | 200 | 17.381090825s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:36 | 200 |  1.890220993s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:38 | 200 |  2.715344163s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:40 | 200 |  1.254062509s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:42 | 200 |  2.709834912s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:43 | 200 |  681.539472ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:46 | 200 |  3.573025496s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:23:47 | 200 |  754.002201ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:27:43.275+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23304 keep=5 new=4096
[GIN] 2025/10/25 - 13:27:57 | 200 | 14.486857868s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:00 | 200 |  3.253372228s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:03 | 200 |  2.829424983s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:07 | 200 |  3.579628077s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:08 | 200 |  674.967215ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:12 | 200 |  4.227805584s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:28:12 | 200 |  704.829737ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:35:09.489+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T13:35:09.690+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.8 GiB" free_swap="0 B"
time=2025-10-25T13:35:09.690+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T13:35:09.995+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 35449"
time=2025-10-25T13:35:09.996+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T13:35:09.996+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:35:09.996+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:35:10.010+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:35:10.291+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:35:10.292+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35449"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-10-25T13:35:10.498+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T13:35:21.270+11:00 level=INFO source=server.go:637 msg="llama runner started in 11.27 seconds"
time=2025-10-25T13:35:21.289+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23397 keep=5 new=4096
[GIN] 2025/10/25 - 13:35:34 | 200 | 25.252051904s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:35:37 | 200 |  2.890153646s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:35:37.800+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T13:35:37.800+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T13:35:38.002+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.3 GiB" free_swap="0 B"
time=2025-10-25T13:35:38.002+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T13:35:38.091+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35531"
time=2025-10-25T13:35:38.091+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:35:38.091+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:35:38.112+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:35:38.104+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T13:35:38.104+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35531"
time=2025-10-25T13:35:38.195+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:35:38.281+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:35:38.363+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T13:35:38.414+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T13:35:38.414+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T13:35:38.414+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T13:35:38.414+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T13:35:38.414+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T13:35:38.424+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T13:35:38.424+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T13:35:44.167+11:00 level=INFO source=server.go:637 msg="llama runner started in 6.08 seconds"
[GIN] 2025/10/25 - 13:35:49 | 200 | 12.415195886s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:35:50 | 200 |  812.796263ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:04 | 200 | 14.248724407s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:06 | 200 |  1.385622558s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:34 | 200 |  27.84187603s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:42 | 200 |  8.019306941s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:42 | 200 |  855.902188ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:49 | 200 |  6.114743854s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:36:49 | 200 |   788.75412ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:49:38.550+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T13:49:38.769+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.8 GiB" free_swap="0 B"
time=2025-10-25T13:49:38.770+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T13:49:39.103+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 41371"
time=2025-10-25T13:49:39.106+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T13:49:39.107+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:49:39.198+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:49:39.199+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:49:39.284+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:49:39.285+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41371"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T13:49:39.449+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T13:49:55.985+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.88 seconds"
time=2025-10-25T13:49:56.003+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23384 keep=5 new=4096
[GIN] 2025/10/25 - 13:50:04 | 200 |   26.8456299s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:07 | 200 |  2.194849433s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:50:07.575+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T13:50:07.576+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T13:50:07.779+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.3 GiB" free_swap="0 B"
time=2025-10-25T13:50:07.779+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T13:50:07.868+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 43321"
time=2025-10-25T13:50:07.869+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:50:07.869+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:50:07.885+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:50:07.882+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T13:50:07.882+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43321"
time=2025-10-25T13:50:07.969+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:50:08.052+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:50:08.136+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T13:50:08.184+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T13:50:08.184+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T13:50:08.184+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T13:50:08.184+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T13:50:08.184+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T13:50:08.193+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T13:50:08.194+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T13:50:12.179+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/25 - 13:50:18 | 200 | 11.729105889s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:19 | 200 |  933.285595ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:43 | 200 | 23.775495163s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:45 | 200 |  1.942324254s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:55 | 200 |  9.562282568s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:50:56 | 200 |  1.388324882s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:51:02 | 200 |  6.427859698s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:51:04 | 200 |  1.370411341s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:51:14 | 200 | 10.322394336s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:51:16 | 200 |  1.573668743s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:56:04.279+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="63.8 GiB"
time=2025-10-25T13:56:04.281+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=68478435328 required="41.8 GiB"
time=2025-10-25T13:56:04.520+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.8 GiB" free_swap="0 B"
time=2025-10-25T13:56:04.521+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[63.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T13:56:04.818+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 33401"
time=2025-10-25T13:56:04.818+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:56:04.818+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:56:04.818+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:56:04.832+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:56:05.109+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:56:05.109+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:33401"
time=2025-10-25T13:56:05.320+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65306 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T13:56:22.871+11:00 level=INFO source=server.go:637 msg="llama runner started in 18.05 seconds"
time=2025-10-25T13:56:22.890+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23327 keep=5 new=4096
[GIN] 2025/10/25 - 13:56:52 | 200 | 48.639452126s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:56:56 | 200 |  3.916181051s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T13:56:57.573+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T13:56:57.573+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39650983936 required="14.9 GiB"
time=2025-10-25T13:56:57.776+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.9 GiB" free_swap="0 B"
time=2025-10-25T13:56:57.776+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T13:56:57.865+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 39277"
time=2025-10-25T13:56:57.865+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T13:56:57.865+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T13:56:57.886+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T13:56:57.878+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T13:56:57.878+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39277"
time=2025-10-25T13:56:57.965+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T13:56:58.053+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T13:56:58.138+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T13:56:58.234+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T13:56:58.234+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T13:56:58.234+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T13:56:58.234+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T13:56:58.234+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T13:56:58.243+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T13:56:58.243+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T13:57:06.959+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.09 seconds"
[GIN] 2025/10/25 - 13:57:17 | 200 | 20.710808956s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:57:18 | 200 |  1.246978978s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:57:24 | 200 |  5.256245317s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:57:24 | 200 |  730.686005ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:57:29 | 200 |   4.70860004s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 13:57:30 | 200 |  767.101418ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:01:55.257+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23381 keep=5 new=4096
[GIN] 2025/10/25 - 14:02:24 | 200 | 29.209665768s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:26 | 200 |  2.018147722s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:38 | 200 | 12.077998688s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:39 | 200 |  1.310935554s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:45 | 200 |  6.026130547s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:46 | 200 |   821.32789ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:53 | 200 |  7.105539021s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:02:54 | 200 |  855.279208ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:10:11.183+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T14:10:11.378+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.1 GiB" free_swap="0 B"
time=2025-10-25T14:10:11.378+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T14:10:11.684+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 37633"
time=2025-10-25T14:10:11.685+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T14:10:11.685+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:10:11.685+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:10:11.699+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:10:18.453+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:10:18.466+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37633"
time=2025-10-25T14:10:18.699+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T14:10:34.733+11:00 level=INFO source=server.go:637 msg="llama runner started in 23.05 seconds"
time=2025-10-25T14:10:34.752+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23390 keep=5 new=4096
[GIN] 2025/10/25 - 14:11:00 | 200 | 49.476123823s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:02 | 200 |  2.575191514s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:11:03.599+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T14:11:03.599+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T14:11:03.816+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.7 GiB" free_swap="0 B"
time=2025-10-25T14:11:03.817+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T14:11:03.907+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 45725"
time=2025-10-25T14:11:03.907+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T14:11:03.907+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:11:03.923+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:11:03.920+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T14:11:03.921+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45725"
time=2025-10-25T14:11:04.008+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:11:04.094+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:11:04.173+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T14:11:04.227+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T14:11:04.227+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T14:11:04.227+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T14:11:04.227+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T14:11:04.228+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T14:11:04.237+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T14:11:04.237+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T14:11:11.473+11:00 level=INFO source=server.go:637 msg="llama runner started in 7.57 seconds"
[GIN] 2025/10/25 - 14:11:18 | 200 | 15.469954269s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:19 | 200 |  1.387969565s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:28 | 200 |  8.155983944s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:28 | 200 |  893.798793ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:44 | 200 | 15.869422954s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:11:46 | 200 |  1.479203666s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:12:00.156+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23261 keep=5 new=4096
[GIN] 2025/10/25 - 14:12:23 | 200 | 23.358639517s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:12:25 | 200 |  2.186396475s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:12:27 | 200 |  1.704815588s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:12:28 | 200 |  1.503449239s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:12:31 | 200 |  2.416826154s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:23:27.568+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T14:23:27.763+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.3 GiB" free_swap="0 B"
time=2025-10-25T14:23:27.764+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T14:23:28.056+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 46139"
time=2025-10-25T14:23:28.057+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T14:23:28.057+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:23:28.057+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:23:28.071+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:23:28.158+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:23:28.158+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:46139"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T14:23:28.308+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T14:23:43.841+11:00 level=INFO source=server.go:637 msg="llama runner started in 15.78 seconds"
time=2025-10-25T14:23:43.859+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23383 keep=5 new=4096
[GIN] 2025/10/25 - 14:24:11 | 200 | 44.353416815s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:24:13 | 200 |  2.112936419s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:24:14.148+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T14:24:14.148+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39650983936 required="14.9 GiB"
time=2025-10-25T14:24:14.356+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.3 GiB" free_swap="0 B"
time=2025-10-25T14:24:14.356+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T14:24:14.439+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 40835"
time=2025-10-25T14:24:14.439+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T14:24:14.439+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:24:14.452+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:24:14.454+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T14:24:14.454+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40835"
time=2025-10-25T14:24:14.540+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:24:14.626+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:24:14.703+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T14:24:14.760+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T14:24:14.760+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T14:24:14.760+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T14:24:14.760+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T14:24:14.760+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T14:24:14.770+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T14:24:14.770+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T14:24:18.988+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.55 seconds"
[GIN] 2025/10/25 - 14:24:52 | 200 | 39.074000678s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:24:55 | 200 |  2.927827931s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:03 | 200 |  7.663421834s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:04 | 200 |  881.575062ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:09 | 200 |  5.702255282s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:10 | 200 |  835.261142ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:18 | 200 |  7.629595087s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:25:19 | 200 |  878.850947ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:29:48.870+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="63.8 GiB"
time=2025-10-25T14:29:48.872+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=68451172352 required="41.8 GiB"
time=2025-10-25T14:29:49.104+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="969.6 GiB" free_swap="0 B"
time=2025-10-25T14:29:49.105+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[63.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T14:29:49.393+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 43617"
time=2025-10-25T14:29:49.394+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T14:29:49.394+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:29:49.394+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:29:49.408+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:29:49.494+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:29:49.494+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:43617"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65280 MiB free
time=2025-10-25T14:29:49.645+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T14:30:05.680+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.29 seconds"
time=2025-10-25T14:30:05.698+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23311 keep=5 new=4096
[GIN] 2025/10/25 - 14:30:25 | 200 | 37.402143018s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:30:27 | 200 |  2.028325403s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:30:28.390+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T14:30:28.390+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T14:30:28.595+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.1 GiB" free_swap="0 B"
time=2025-10-25T14:30:28.595+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T14:30:28.683+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 33621"
time=2025-10-25T14:30:28.683+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T14:30:28.683+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:30:28.698+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:30:28.696+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T14:30:28.697+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33621"
time=2025-10-25T14:30:28.783+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:30:28.870+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:30:28.949+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T14:30:29.003+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T14:30:29.003+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T14:30:29.003+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T14:30:29.003+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T14:30:29.003+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T14:30:29.013+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T14:30:29.013+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T14:30:32.998+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/25 - 14:30:37 | 200 |  9.599142765s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:30:38 | 200 |  859.720889ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:30:42 | 200 |  3.651132027s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:30:46 | 200 |  4.253519968s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:30:47 | 200 |  675.314779ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:33:39.641+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23316 keep=5 new=4096
[GIN] 2025/10/25 - 14:34:08 | 200 | 28.658937522s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:12 | 200 |  3.927790806s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:18 | 200 |  5.948920847s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:22 | 200 |  4.593451002s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:23 | 200 |  723.573906ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:28 | 200 |  5.100716258s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:34:29 | 200 |  725.762044ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:38:42.613+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23352 keep=5 new=4096
[GIN] 2025/10/25 - 14:38:57 | 200 | 15.244217199s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:39:00 | 200 |  2.681914844s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:39:04 | 200 |  4.028533363s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:39:10 | 200 |  5.820973347s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:39:16 | 200 |  6.660453421s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:39:18 | 200 |  1.393011069s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:41:20.221+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23296 keep=5 new=4096
[GIN] 2025/10/25 - 14:41:37 | 200 | 17.793049416s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:41:39 | 200 |  2.047428918s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:41:43 | 200 |  3.241964068s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:41:46 | 200 |  2.890713271s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:41:49 | 200 |  3.617001629s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:41:50 | 200 |  694.474868ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:45:28.669+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23322 keep=5 new=4096
[GIN] 2025/10/25 - 14:45:46 | 200 | 18.349226595s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:45:50 | 200 |  3.434176523s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:45:55 | 200 |  4.765906044s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:45:55 | 200 |  791.220855ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:46:01 | 200 |  5.326860839s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:46:02 | 200 |  830.007294ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:46:06 | 200 |  4.669469943s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:46:07 | 200 |   787.32342ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:56:23.229+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T14:56:23.431+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.6 GiB" free_swap="0 B"
time=2025-10-25T14:56:23.431+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T14:56:23.725+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 38279"
time=2025-10-25T14:56:23.726+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T14:56:23.726+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:56:23.726+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:56:23.741+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:56:23.841+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:56:23.845+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38279"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T14:56:23.978+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T14:56:40.263+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.54 seconds"
time=2025-10-25T14:56:40.281+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23375 keep=5 new=4096
[GIN] 2025/10/25 - 14:56:50 | 200 | 27.444385325s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:56:52 | 200 |  1.751377059s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:56:52.526+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T14:56:52.527+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T14:56:52.737+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.3 GiB" free_swap="0 B"
time=2025-10-25T14:56:52.738+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T14:56:52.826+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 37029"
time=2025-10-25T14:56:52.826+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T14:56:52.826+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T14:56:52.847+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T14:56:52.840+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T14:56:52.840+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37029"
time=2025-10-25T14:56:52.927+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T14:56:53.012+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T14:56:53.101+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T14:56:53.161+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T14:56:53.161+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T14:56:53.161+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T14:56:53.161+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T14:56:53.161+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T14:56:53.170+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T14:56:53.170+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T14:56:57.139+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/25 - 14:57:05 | 200 | 13.342695765s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:57:06 | 200 |  906.734063ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:57:11 | 200 |  5.332083747s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:57:12 | 200 |  793.878499ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:57:19 | 200 |  7.499050169s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:57:20 | 200 |  897.293127ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T14:57:35.183+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23264 keep=5 new=4096
[GIN] 2025/10/25 - 14:57:59 | 200 | 24.375147429s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:02 | 200 |  3.158178321s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:04 | 200 |  1.981239869s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:06 | 200 |  1.583066307s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:06 | 200 |  757.467066ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:10 | 200 |  3.174877366s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 14:58:10 | 200 |  778.900088ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:04:44.393+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T15:04:44.588+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.5 GiB" free_swap="0 B"
time=2025-10-25T15:04:44.588+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T15:04:44.883+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 38061"
time=2025-10-25T15:04:44.884+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T15:04:44.884+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:04:44.884+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:04:44.898+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:04:44.987+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:04:44.987+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38061"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T15:04:45.135+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T15:04:58.916+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.03 seconds"
time=2025-10-25T15:04:58.934+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23384 keep=5 new=4096
[GIN] 2025/10/25 - 15:05:13 | 200 | 29.275853439s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:05:15 | 200 |  1.812419739s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:05:15.940+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:05:15.940+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T15:05:16.143+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.0 GiB" free_swap="0 B"
time=2025-10-25T15:05:16.143+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:05:16.232+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 41307"
time=2025-10-25T15:05:16.232+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:05:16.232+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:05:16.238+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:05:16.245+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:05:16.246+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41307"
time=2025-10-25T15:05:16.337+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:05:16.422+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:05:16.489+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:05:16.559+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:05:16.559+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:05:16.559+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:05:16.559+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:05:16.559+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:05:16.569+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:05:16.569+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:05:23.528+11:00 level=INFO source=server.go:637 msg="llama runner started in 7.30 seconds"
[GIN] 2025/10/25 - 15:05:28 | 200 | 13.234841613s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:05:37 | 200 |   8.95912949s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:05:38 | 200 |  982.510089ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:05:46 | 200 |  7.797251259s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:05:47 | 200 |  823.599321ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:10:51.263+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T15:10:51.464+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.6 GiB" free_swap="0 B"
time=2025-10-25T15:10:51.465+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T15:10:51.759+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 34635"
time=2025-10-25T15:10:51.760+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T15:10:51.760+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:10:51.760+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:10:51.773+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:10:51.918+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:10:51.919+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34635"
time=2025-10-25T15:10:52.011+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T15:11:06.042+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.28 seconds"
time=2025-10-25T15:11:06.061+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23385 keep=5 new=4096
[GIN] 2025/10/25 - 15:11:16 | 200 | 25.298065467s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:19 | 200 |  2.871688189s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:11:19.569+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:11:19.570+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T15:11:19.771+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.9 GiB" free_swap="0 B"
time=2025-10-25T15:11:19.771+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:11:19.862+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 45309"
time=2025-10-25T15:11:19.862+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:11:19.862+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:11:19.877+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:11:19.893+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:11:19.894+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45309"
time=2025-10-25T15:11:19.981+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:11:20.068+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:11:20.129+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:11:20.205+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:11:20.205+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:11:20.205+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:11:20.205+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:11:20.205+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:11:20.215+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:11:20.215+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:11:24.422+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.56 seconds"
[GIN] 2025/10/25 - 15:11:29 | 200 | 10.576650087s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:30 | 200 |  866.136456ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:36 | 200 |  5.889477818s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:37 | 200 |  829.449421ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:44 | 200 |  7.144377846s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:11:45 | 200 |  843.113654ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:18:46.020+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T15:18:46.212+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="971.2 GiB" free_swap="0 B"
time=2025-10-25T15:18:46.212+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T15:18:46.507+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 36713"
time=2025-10-25T15:18:46.508+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T15:18:46.508+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:18:46.508+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:18:46.522+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:18:46.696+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:18:46.706+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:36713"
time=2025-10-25T15:18:46.760+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T15:19:01.042+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.53 seconds"
time=2025-10-25T15:19:01.060+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23376 keep=5 new=4096
[GIN] 2025/10/25 - 15:19:10 | 200 | 24.620484224s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:11 | 200 |  1.552507198s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:19:12.307+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:19:12.307+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T15:19:12.508+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.2 GiB" free_swap="0 B"
time=2025-10-25T15:19:12.509+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:19:12.597+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 43461"
time=2025-10-25T15:19:12.598+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:19:12.598+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:19:12.619+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:19:12.611+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:19:12.611+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43461"
time=2025-10-25T15:19:12.698+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:19:12.783+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:19:12.870+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:19:12.916+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:19:12.916+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:19:12.916+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:19:12.916+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:19:12.916+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:19:12.925+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:19:12.925+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:19:17.152+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.55 seconds"
[GIN] 2025/10/25 - 15:19:22 | 200 |  10.34193823s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:23 | 200 |   1.35909347s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:32 | 200 |  9.254772919s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:34 | 200 |  1.458141041s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:42 | 200 |  8.660283304s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:19:43 | 200 |  927.158915ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:22:41.598+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23307 keep=5 new=4096
[GIN] 2025/10/25 - 15:23:00 | 200 | 19.340318046s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:04 | 200 |   3.60449748s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:11 | 200 |   7.16751098s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:14 | 200 |  3.202095625s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:18 | 200 |  4.068562723s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:19 | 200 |  802.978763ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:22 | 200 |  2.880256285s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:23:23 | 200 |  694.490297ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:28:42.873+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T15:28:43.072+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.5 GiB" free_swap="0 B"
time=2025-10-25T15:28:43.073+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T15:28:43.364+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 36795"
time=2025-10-25T15:28:43.364+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T15:28:43.364+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:28:43.365+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:28:43.380+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:28:43.469+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:28:43.470+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:36795"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T15:28:43.616+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T15:28:57.395+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.03 seconds"
time=2025-10-25T15:28:57.413+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23364 keep=5 new=4096
[GIN] 2025/10/25 - 15:29:15 | 200 | 32.637707746s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:29:17 | 200 |  1.886235652s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:29:17.620+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:29:17.621+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39650983936 required="14.9 GiB"
time=2025-10-25T15:29:17.822+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="969.4 GiB" free_swap="0 B"
time=2025-10-25T15:29:17.822+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:29:17.911+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 33829"
time=2025-10-25T15:29:17.911+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:29:17.911+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:29:17.932+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:29:17.924+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:29:17.925+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33829"
time=2025-10-25T15:29:18.012+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:29:18.099+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:29:18.184+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:29:18.240+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:29:18.240+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:29:18.240+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:29:18.240+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:29:18.240+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:29:18.250+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:29:18.250+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:29:22.232+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.32 seconds"
[GIN] 2025/10/25 - 15:29:27 | 200 |  9.866791771s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:29:33 | 200 |  6.453316666s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:29:38 | 200 |  5.015472496s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:29:39 | 200 |  820.126902ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:29:44.001+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23260 keep=5 new=4096
[GIN] 2025/10/25 - 15:30:09 | 200 | 25.588439519s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:30:13 | 200 |  4.165558749s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:30:16 | 200 |  2.581882108s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:30:19 | 200 |  2.651821921s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:30:19 | 200 |   727.95413ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:30:22 | 200 |  2.723501059s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:32:26.615+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23291 keep=5 new=4096
[GIN] 2025/10/25 - 15:32:48 | 200 | 21.800430402s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:32:50 | 200 |   2.04896085s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:32:54 | 200 |  4.326331599s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:32:55 | 200 |  710.824937ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:32:59 | 200 |  3.750340667s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:32:59 | 200 |  696.844673ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:33:04 | 200 |   4.44497439s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:33:05 | 200 |  724.313136ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:37:45.002+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23371 keep=5 new=4096
[GIN] 2025/10/25 - 15:38:18 | 200 | 33.503905584s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:38:21 | 200 |  3.540445048s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:38:22.431+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:38:22.431+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T15:38:22.634+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.1 GiB" free_swap="0 B"
time=2025-10-25T15:38:22.635+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:38:22.715+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 42453"
time=2025-10-25T15:38:22.716+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:38:22.716+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:38:22.731+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:38:22.796+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:38:22.797+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42453"
time=2025-10-25T15:38:22.883+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:38:22.971+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:38:22.982+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:38:23.104+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:38:23.104+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:38:23.104+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:38:23.104+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:38:23.104+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:38:23.114+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:38:23.114+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:38:27.293+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.58 seconds"
[GIN] 2025/10/25 - 15:38:32 | 200 | 10.064692054s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:38:38 | 200 |  6.624719229s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:38:40 | 200 |  1.333507902s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:38:46 | 200 |   6.94904802s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:38:48 | 200 |  1.458335984s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:40:39.935+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23286 keep=5 new=4096
[GIN] 2025/10/25 - 15:41:00 | 200 | 20.944481421s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:41:03 | 200 |  2.864252433s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:41:06 | 200 |  2.847283428s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:41:09 | 200 |  2.917092253s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:41:13 | 200 |  3.734728008s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:41:13 | 200 |   685.08052ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:42:00.950+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23270 keep=5 new=4096
[GIN] 2025/10/25 - 15:43:02 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:06 | 200 |  4.698571815s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:11 | 200 |  4.682056647s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:15 | 200 |   3.67481071s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:16 | 200 |  803.869764ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:20 | 200 |    4.2742865s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:43:21 | 200 |  751.955134ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:45:49.571+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23313 keep=5 new=4096
[GIN] 2025/10/25 - 15:46:17 | 200 | 28.033360691s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:20 | 200 |  3.499903621s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:25 | 200 |  4.237052488s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:26 | 200 |  1.449283072s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:29 | 200 |  3.259107451s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:30 | 200 |  721.989447ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:33 | 200 |  3.165673672s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:46:34 | 200 |  725.883611ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:47:51.657+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23289 keep=5 new=4096
[GIN] 2025/10/25 - 15:48:27 | 200 | 36.078218915s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:31 | 200 |  3.363715972s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:33 | 200 |  2.446506835s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:36 | 200 |  3.292628412s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:37 | 200 |  719.240622ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:41 | 200 |  3.683370265s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:48:41 | 200 |  678.663005ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:58:41.113+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T15:58:41.309+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.8 GiB" free_swap="0 B"
time=2025-10-25T15:58:41.310+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T15:58:41.632+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 39079"
time=2025-10-25T15:58:41.644+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T15:58:41.658+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:58:41.658+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:58:41.668+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:58:41.772+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:58:41.773+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:39079"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T15:58:41.909+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T15:58:56.941+11:00 level=INFO source=server.go:637 msg="llama runner started in 15.30 seconds"
time=2025-10-25T15:58:56.960+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23380 keep=5 new=4096
[GIN] 2025/10/25 - 15:59:15 | 200 | 34.358235148s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:17 | 200 |  2.806151401s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T15:59:18.387+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T15:59:18.387+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39650983936 required="14.9 GiB"
time=2025-10-25T15:59:18.588+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="969.5 GiB" free_swap="0 B"
time=2025-10-25T15:59:18.589+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T15:59:18.677+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 33085"
time=2025-10-25T15:59:18.677+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T15:59:18.677+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T15:59:18.698+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T15:59:18.690+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T15:59:18.690+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33085"
time=2025-10-25T15:59:18.778+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T15:59:18.864+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T15:59:18.949+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T15:59:18.995+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T15:59:18.995+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T15:59:18.995+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T15:59:18.995+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T15:59:18.995+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T15:59:19.005+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T15:59:19.005+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T15:59:22.991+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.31 seconds"
[GIN] 2025/10/25 - 15:59:27 | 200 | 10.071227811s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:29 | 200 |  1.360765677s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:36 | 200 |  6.900333848s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:37 | 200 |  853.975055ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:43 | 200 |  6.266358218s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 15:59:44 | 200 |  788.983447ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T16:05:05.713+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="41.8 GiB"
time=2025-10-25T16:05:05.909+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="970.5 GiB" free_swap="0 B"
time=2025-10-25T16:05:05.910+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=81 layers.offload=81 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="41.8 GiB" memory.required.partial="41.8 GiB" memory.required.kv="1.2 GiB" memory.required.allocations="[41.8 GiB]" memory.weights.total="39.0 GiB" memory.weights.repeating="38.2 GiB" memory.weights.nonrepeating="822.0 MiB" memory.graph.full="584.0 MiB" memory.graph.partial="1.1 GiB"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T16:05:06.203+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 4096 --batch-size 512 --n-gpu-layers 81 --threads 56 --parallel 1 --port 42577"
time=2025-10-25T16:05:06.204+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T16:05:06.204+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T16:05:06.204+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T16:05:06.218+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T16:05:06.305+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T16:05:06.305+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42577"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T16:05:06.455+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 39.59 GiB (4.82 BPW) 
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 8192
print_info: n_layer          = 80
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 28672
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 70B
print_info: model params     = 70.55 B
print_info: general.name     = Meta Llama 3.1 70B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 80 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 81/81 layers to GPU
load_tensors:        CUDA0 model buffer size = 39979.48 MiB
load_tensors:   CPU_Mapped model buffer size =   563.62 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.52 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 80, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =  1280.00 MiB
llama_kv_cache_unified: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
llama_context:      CUDA0 compute buffer size =   584.00 MiB
llama_context:  CUDA_Host compute buffer size =    24.01 MiB
llama_context: graph nodes  = 2726
llama_context: graph splits = 2
time=2025-10-25T16:05:19.734+11:00 level=INFO source=server.go:637 msg="llama runner started in 13.53 seconds"
time=2025-10-25T16:05:19.752+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=23318 keep=5 new=4096
[GIN] 2025/10/25 - 16:05:40 | 200 | 35.204362377s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:05:42 | 200 |  2.053826001s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T16:05:43.106+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="36.9 GiB"
time=2025-10-25T16:05:43.106+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=39648886784 required="14.9 GiB"
time=2025-10-25T16:05:43.308+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="969.6 GiB" free_swap="0 B"
time=2025-10-25T16:05:43.309+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[36.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T16:05:43.391+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 42493"
time=2025-10-25T16:05:43.392+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T16:05:43.392+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T16:05:43.405+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T16:05:43.406+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T16:05:43.407+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42493"
time=2025-10-25T16:05:43.493+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T16:05:43.580+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T16:05:43.656+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T16:05:43.714+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T16:05:43.715+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T16:05:43.715+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T16:05:43.715+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T16:05:43.715+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T16:05:43.724+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T16:05:43.725+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T16:05:47.710+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.32 seconds"
[GIN] 2025/10/25 - 16:05:51 | 200 |  8.434265704s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:05:51 | 200 |  778.807779ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:05:55 | 200 |  3.279359011s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:05:55 | 200 |  717.626641ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:06:00 | 200 |  4.825600942s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:06:01 | 200 |  771.588403ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T16:06:02.539+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="22.1 GiB"
time=2025-10-25T16:06:02.539+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=23731961856 required="6.1 GiB"
time=2025-10-25T16:06:02.741+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="967.7 GiB" free_swap="0 B"
time=2025-10-25T16:06:02.742+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[22.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T16:06:02.987+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 35811"
time=2025-10-25T16:06:02.987+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=3
time=2025-10-25T16:06:02.987+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T16:06:02.988+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T16:06:03.001+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T16:06:03.088+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T16:06:03.088+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35811"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 22632 MiB free
time=2025-10-25T16:06:03.238+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T16:06:06.997+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.01 seconds"
[GIN] 2025/10/25 - 16:07:15 | 200 |         1m13s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:07:30 | 200 | 15.233168173s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:07:32 | 200 |  2.219931862s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:07:34 | 200 |  1.936981726s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:07:37 | 200 |  2.193030583s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:19 | 200 | 42.419844369s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:24 | 200 |  5.222528786s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:27 | 200 |  3.226724955s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:29 | 200 |  1.328637505s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:31 | 200 |  2.397150386s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:34 | 200 |  2.503821506s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:44 | 200 | 10.081515829s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:08:59 | 200 | 14.698803652s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:09:10 | 200 | 11.905784635s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:09:25 | 200 | 14.448318876s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:09:46 | 200 |   21.0364385s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:10:44 | 200 | 58.301731702s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:10:47 | 200 |  2.721985858s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:10:54 | 200 |  6.878154509s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:10:58 | 200 |  3.737719163s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:11:08 | 200 |  9.788984853s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:11:52 | 200 | 43.931974683s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:11:56 | 200 |  3.844928274s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:00 | 200 |  4.154616373s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:01 | 200 |  1.043661792s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:04 | 200 |  3.463776514s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:08 | 200 |  3.798053359s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:16 | 200 |  7.581362431s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:20 | 200 |  4.079064553s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:12:21 | 200 |  703.915545ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:21 | 200 |          1m0s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:24 | 200 |  3.311677942s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:31 | 200 |  7.249808134s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:43 | 200 | 11.070042009s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:44 | 200 |   1.18333516s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:51 | 200 |  7.308752314s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:13:59 | 200 |   7.82338793s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:14:15 | 200 | 15.278411075s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:14:19 | 200 |    4.6994455s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:14:24 | 200 |  4.781429517s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:14:27 | 200 |  3.344120339s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:15:41 | 200 |         1m13s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:15:46 | 200 |  5.464619354s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:15:51 | 200 |  4.945425404s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:15:55 | 200 |  3.558792753s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:02 | 200 |  6.577540649s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:02 | 200 |   831.29011ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:20 | 200 | 17.974608789s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:23 | 200 |  2.435193197s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:24 | 200 |  1.444715788s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:26 | 200 |  1.979069647s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:16:27 | 200 |  750.384511ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:17:52 | 200 |         1m25s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:17:56 | 200 |  3.747946042s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:17:58 | 200 |  1.974728572s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:17:59 | 200 |  1.315538347s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:18:01 | 200 |  2.053207981s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:18:10 | 200 |  8.430740627s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:18:13 | 200 |  2.641324808s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:18:15 | 200 |  2.123552815s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:18:18 | 200 |  2.928959833s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:10 | 200 | 52.373892295s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:13 | 200 |  2.646070269s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:19 | 200 |  6.138220887s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:26 | 200 |  7.153337246s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:39 | 200 | 12.717360046s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:19:45 | 200 |  5.999556618s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:20:07 | 200 |   22.4246594s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:20:22 | 200 | 14.883293753s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:20:26 | 200 |  4.104768775s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:20:27 | 200 |   795.56102ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:20:31 | 200 |  4.212547823s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:21:50 | 200 |         1m19s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:21:53 | 200 |  2.798561244s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:21:56 | 200 |  2.913495683s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:21:58 | 200 |  1.884168791s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:22:00 | 200 |  2.333788179s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:22:01 | 200 |  769.106031ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:22:51 | 200 | 50.120695026s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:22:59 | 200 |  7.826543945s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:23:11 | 200 | 12.055434619s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:23:34 | 200 | 22.849210486s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:23:36 | 200 |   1.99328536s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:23:44 | 200 |   7.79593206s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:23:49 | 200 |  5.409318767s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:24:53 | 200 |          1m4s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:17 | 200 | 23.876878377s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:19 | 200 |  2.142260637s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:21 | 200 |  1.701452624s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:23 | 200 |  1.714046572s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:37 | 200 | 14.358162243s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:40 | 200 |  2.797814743s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:41 | 200 |  756.681001ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:43 | 200 |  2.146399646s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:25:45 | 200 |  2.486501733s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:26:19 | 200 | 33.717777029s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:26:21 | 200 |  2.184224477s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:26:23 | 200 |  1.389670135s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:26:25 | 200 |  2.197158553s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:12 | 200 | 46.522638104s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:16 | 200 |  4.074751451s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:25 | 200 |  9.247063454s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:27 | 200 |  1.974153141s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:44 | 200 | 17.128763592s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:53 | 200 |   8.57218329s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:54 | 200 |  1.067404141s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:59 | 200 |  4.996424945s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:27:59 | 200 |  762.486013ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:04 | 200 |          1m4s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:16 | 200 | 11.378490044s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:23 | 200 |  7.644950814s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:30 | 200 |  6.698790688s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:31 | 200 |  808.647539ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:44 | 200 |  13.13801294s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:29:53 | 200 |   8.91311415s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:30:28 | 200 | 35.059316168s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:30:30 | 200 |  1.727953621s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:30:32 | 200 |  1.965695582s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:30:34 | 200 |  1.852002581s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:31:35 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:01 | 200 | 26.224399966s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:04 | 200 |  2.669964809s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:06 | 200 |  2.001398413s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:08 | 200 |  2.579219382s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:09 | 200 |  727.038903ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:24 | 200 | 15.397239956s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:32 | 200 |  7.863959747s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:34 | 200 |  1.338217108s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:39 | 200 |  4.932375156s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:39 | 200 |  801.866699ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:44 | 200 |  4.349362279s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:47 | 200 |  3.658911381s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:32:54 | 200 |  6.458705768s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:33:18 | 200 | 23.839942614s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:33:21 | 200 |  3.222454793s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:33:24 | 200 |  2.797598415s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:33:27 | 200 |  3.485230388s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:33:28 | 200 |  697.103847ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:34:15 | 200 |  46.70647044s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:34:22 | 200 |  7.032834992s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:34:24 | 200 |  2.413527537s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:34:27 | 200 |  2.776514022s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:34:29 | 200 |  1.786717543s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:35:35 | 200 |          1m6s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:36:29 | 200 | 53.897333649s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:36:54 | 200 | 20.081030055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 16:37:01 | 200 |  7.089943403s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:02 | 200 |  786.290253ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:13 | 200 | 10.497311061s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:22 | 200 |  9.733591657s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:23 | 200 |  1.188619173s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:31 | 200 |  7.089347136s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:37:32 | 200 |  1.311767062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:38:53 | 200 |         1m20s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:38:55 | 200 |  2.834690587s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:38:59 | 200 |  3.237111088s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:39:00 | 200 |  1.476486127s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:39:02 | 200 |  2.053648882s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:40:25 | 200 |         1m22s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:08 | 200 | 42.964456283s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:28 | 200 | 19.882650003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 16:41:34 | 200 |  5.941872705s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:35 | 200 |  776.822613ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:39 | 200 |   4.78391818s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:40 | 200 |   736.10164ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:46 | 200 |  6.155767142s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:41:47 | 200 |  770.025956ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:19 | 200 |         1m32s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:27 | 200 |  7.761126036s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:39 | 200 | 12.111837537s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:50 | 200 |  11.23248444s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:56 | 200 |  5.398915202s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:43:56 | 200 |  703.059667ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:44:14 | 200 | 17.566230576s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:10 | 200 | 55.687010022s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:14 | 200 |  4.050980405s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:19 | 200 |  4.998598268s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:22 | 200 |  3.174432024s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:23 | 200 |  789.106301ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:26 | 200 |  2.760845905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:45:26 | 200 |  740.460576ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:11 | 200 | 44.629269302s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:13 | 200 |  2.494681979s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:16 | 200 |  2.287646066s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:17 | 200 |  1.111747062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:18 | 200 |  1.543040702s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:46:20 | 200 |  1.634319275s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:47:18 | 200 |  58.32466904s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:47:24 | 200 |  5.794549856s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:47:25 | 200 |  1.249613179s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:47:27 | 200 |  1.399885282s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:47:28 | 200 |  1.305518023s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:20 | 200 | 52.165325668s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:25 | 200 |   4.32195576s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:32 | 200 |  7.241413972s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:40 | 200 |  8.461438582s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:41 | 200 |  1.097174235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:47 | 200 |    5.9360738s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:48 | 200 |  748.342302ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:48:54 | 200 |  5.724925682s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:02 | 200 |  7.596599648s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:05 | 200 |  3.807329347s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:06 | 200 |  738.299446ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:10 | 200 |   4.03954384s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:13 | 200 |  3.200024629s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:30 | 200 | 16.684843109s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:32 | 200 |   2.30046836s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:34 | 200 |  1.947749536s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:49:36 | 200 |  1.770681329s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:22 | 200 | 45.530576621s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:25 | 200 |  3.038428738s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:31 | 200 |   6.35453929s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:32 | 200 |  1.000681741s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:39 | 200 |  7.423789775s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:40 | 200 |  754.295202ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:50:53 | 200 | 12.965027361s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:51:06 | 200 | 12.977667631s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:51:47 | 200 | 41.287555693s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:51:58 | 200 | 10.888056694s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:00 | 200 |   1.92696868s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:02 | 200 |   1.32834768s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:04 | 200 |   2.29378938s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:05 | 200 |   721.27157ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:39 | 200 | 33.919875041s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:46 | 200 |  7.127825869s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:58 | 200 | 12.184474466s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:52:59 | 200 |  1.149904975s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:53:13 | 200 | 14.070994543s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:53:31 | 200 | 17.811558495s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:53:33 | 200 |  1.623587607s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:53:56 | 200 | 23.081133566s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:53:58 | 200 |   2.40335987s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:54:00 | 200 |  2.286289914s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:54:03 | 200 |  2.448995647s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:55:28 | 200 |         1m25s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:55:49 | 200 |  21.07286484s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:55:52 | 200 |  2.616093067s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:55:53 | 200 |  726.508391ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:00 | 200 |  6.994884758s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:05 | 200 |  5.699669866s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:47 | 200 | 41.307831022s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:51 | 200 |  3.853973455s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:55 | 200 |  4.155199386s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:56:56 | 200 |  740.461907ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:57:01 | 200 |  5.307692295s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:57:02 | 200 |  713.253178ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:57:05 | 200 |  3.347129986s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:24 | 200 |         1m19s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:27 | 200 |  2.957879642s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:33 | 200 |  6.034741071s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:34 | 200 |  728.046895ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:49 | 200 | 14.977116575s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:51 | 200 |  1.461334469s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:56 | 200 |  5.391154408s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:58:57 | 200 |  721.120585ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:59:40 | 200 | 43.697236197s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:59:45 | 200 |  4.536358161s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:59:47 | 200 |  1.705066799s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:59:48 | 200 |  1.517959613s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 16:59:50 | 200 |  2.192173836s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:01:30 | 200 |         1m39s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:18 | 200 | 47.734193627s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:38 | 200 | 19.889425898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 17:02:43 | 200 |  5.815728494s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:44 | 200 |  982.985978ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:50 | 200 |  5.444236048s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:51 | 200 |  826.850911ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:02:56 | 200 |  4.868899097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 17:03:00 | 200 |  4.526992719s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:03:01 | 200 |  788.727101ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:03:08 | 200 |  6.623413422s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:03:09 | 200 |  867.234032ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:09 | 200 |          1m0s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:17 | 200 |  7.748237157s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:20 | 200 |  3.219514025s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:24 | 200 |  3.615236671s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:27 | 200 |  3.336300809s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:04:28 | 200 |  700.626962ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:05:43 | 200 |         1m15s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:05:47 | 200 |  3.271913691s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:05:48 | 200 |  1.678054757s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:05:50 | 200 |   1.44906602s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:05:52 | 200 |   2.16506936s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:06:54 | 200 |          1m2s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:06:58 | 200 |  3.480668357s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:07:03 | 200 |  4.956791046s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:07:03 | 200 |  705.838726ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:07:10 | 200 |  6.472738235s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:07:11 | 200 |  753.661754ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:07:14 | 200 |  3.547574804s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:08:17 | 200 |          1m3s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:08:21 | 200 |  4.146780371s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:08:23 | 200 |  1.732111091s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:08:25 | 200 |  1.817292389s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:08:26 | 200 |  1.370706069s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:15 | 200 | 48.711002995s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:21 | 200 |  5.506537654s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:24 | 200 |   2.89129873s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:28 | 200 |   4.15334709s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:32 | 200 |  3.895550924s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:09:32 | 200 |  713.346282ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:08 | 200 | 35.456038747s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:12 | 200 |  4.175836107s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:13 | 200 |  743.224038ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:27 | 200 | 13.908452593s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:28 | 200 |  1.228607174s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:42 | 200 | 13.746419392s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:10:57 | 200 | 15.537577594s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:07 | 200 |  9.869722124s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:42 | 200 | 34.484847507s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:44 | 200 |  2.429660587s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:46 | 200 |  2.338865977s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:47 | 200 |  756.541682ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:11:49 | 200 |  2.252386347s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:09 | 200 |         1m19s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:12 | 200 |   3.26746607s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:15 | 200 |  3.399563344s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:16 | 200 |  737.978742ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:18 | 200 |  1.886057352s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:13:21 | 200 |  3.081945788s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:12 | 200 | 50.429862473s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:15 | 200 |  2.979810169s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:19 | 200 |  4.705937762s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:29 | 200 |   9.51193702s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:44 | 200 | 15.029415206s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:51 | 200 |  6.702698371s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:14:51 | 200 |  830.546401ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:15:08 | 200 | 16.257910383s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:15:12 | 200 |  4.207562934s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:15:27 | 200 | 14.914632203s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:15:31 | 200 |  4.522966377s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:39 | 200 |          1m8s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:43 | 200 |  3.416009813s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:48 | 200 |  5.284550833s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:49 | 200 |  1.003933948s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:54 | 200 |  5.188404169s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:16:55 | 200 |  805.859728ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:17:01 | 200 |  5.527408658s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:17:02 | 200 |  791.568032ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:18:03 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:18:13 | 200 |  9.937395076s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:18:15 | 200 |  1.834708018s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:18:17 | 200 |  2.438976465s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:18:19 | 200 |  2.101633342s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:22 | 200 |          1m2s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:30 | 200 |  7.932439073s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:32 | 200 |  1.846986292s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:35 | 200 |  2.771015032s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:38 | 200 |  3.011008296s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:19:38 | 200 |  700.470306ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:18 | 200 | 38.914632691s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:21 | 200 |  3.316780119s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:25 | 200 |   4.05597472s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:26 | 200 |  939.620462ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:37 | 200 | 11.525718547s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:43 | 200 |  5.740395014s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:20:44 | 200 |  783.763701ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:21:52 | 200 |          1m8s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:17 | 200 | 24.277735804s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:19 | 200 |  2.467591288s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:21 | 200 |  2.385280695s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:24 | 200 |  2.110800239s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:27 | 200 |  3.410415085s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:28 | 200 |  769.468298ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:31 | 200 |  3.178345623s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:22:32 | 200 |  766.933023ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:23:32 | 200 | 59.947063719s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:23:38 | 200 |  6.690319795s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:23:40 | 200 |  1.932713751s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:23:42 | 200 |   1.54318015s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:23:45 | 200 |  2.794428987s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:31:34 | 200 |         7m49s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:31:37 | 200 |  2.892788294s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T17:31:38.065+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T17:31:38.065+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78070808576 required="14.9 GiB"
time=2025-10-25T17:31:38.269+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.2 GiB" free_swap="0 B"
time=2025-10-25T17:31:38.270+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T17:31:38.362+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 37609"
time=2025-10-25T17:31:38.362+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T17:31:38.362+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T17:31:38.377+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T17:31:38.379+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T17:31:38.379+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37609"
time=2025-10-25T17:31:38.466+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-25T17:31:38.629+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T17:31:39.344+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T17:31:39.852+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T17:31:39.852+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T17:31:39.853+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T17:31:39.853+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T17:31:39.853+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T17:31:39.862+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T17:31:39.862+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T17:31:48.674+11:00 level=INFO source=server.go:637 msg="llama runner started in 10.31 seconds"
[GIN] 2025/10/25 - 17:31:51 | 200 | 14.125164542s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:31:53 | 200 |  1.738887905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:31:55 | 200 |  1.750719174s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:32:36 | 200 | 41.392785303s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:03 | 200 | 26.834908609s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:07 | 200 |   4.27721372s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:09 | 200 |  1.540013454s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:11 | 200 |  2.314315728s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:12 | 200 |  700.013162ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:14 | 200 |  2.510355119s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:33:15 | 200 |   681.54428ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:34:50 | 200 |         1m34s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:34:58 | 200 |  7.981855385s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:00 | 200 |  2.725253018s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:01 | 200 |  725.857205ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:03 | 200 |  1.528527473s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:05 | 200 |  1.959332088s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:09 | 200 |  4.486781506s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:42 | 200 | 32.591346857s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:43 | 200 |  1.557421102s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:45 | 200 |  1.554492851s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:47 | 200 |  2.594287655s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:56 | 200 |  8.360995437s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:35:57 | 200 |  1.591361021s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:00 | 200 |  3.081426138s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:02 | 200 |  1.552037492s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:24 | 200 | 22.282017908s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:35 | 200 | 10.816335029s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:36 | 200 |  1.174141852s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:40 | 200 |   4.10717622s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:41 | 200 |  771.940346ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:36:50 | 200 |  8.647977434s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:37:53 | 200 |          1m3s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:04 | 200 | 11.294138839s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:09 | 200 |  4.709683284s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:10 | 200 |  774.801462ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:14 | 200 |  4.331145979s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:18 | 200 |  4.395713559s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:38:19 | 200 |   723.48494ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:39:28 | 200 |          1m9s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:02 | 200 | 33.322494026s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:03 | 200 |  1.701629151s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:07 | 200 |   3.46992904s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:09 | 200 |  1.960137873s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:12 | 200 |  3.022222406s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:26 | 200 | 14.113239934s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:44 | 200 |  17.58153861s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:40:53 | 200 |  9.212993649s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:41:13 | 200 | 20.053337276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:41:15 | 200 |  1.589028064s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:41:34 | 200 | 19.098727603s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:42:42 | 200 |          1m7s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:43:18 | 200 | 35.503359991s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:43:38 | 200 | 20.051373126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 17:44:30 | 200 | 51.449470177s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:44:34 | 200 |   4.16422119s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:44:44 | 200 | 10.191565469s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:44:46 | 200 |  1.172847663s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:44:54 | 200 |  8.686038837s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:44:55 | 200 |  812.704447ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:15 | 200 | 19.555089197s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:16 | 200 |  1.684807309s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:28 | 200 |  11.88775196s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:32 | 200 |  3.173075973s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:36 | 200 |  3.969277545s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:45:39 | 200 |  3.415769273s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:20 | 200 |         1m41s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:31 | 200 | 10.810208515s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:33 | 200 |  1.974344576s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:37 | 200 |   3.87531669s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:39 | 200 |  1.994915389s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:41 | 200 |  1.745801987s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:45 | 200 |  4.114892156s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:47:46 | 200 |   726.68406ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:48:31 | 200 | 45.408563332s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:48:36 | 200 |  4.545059198s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:48:40 | 200 |  4.891608939s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:48:45 | 200 |  4.437760274s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:48:48 | 200 |  3.072046355s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:49:22 | 200 | 34.075749524s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:49:34 | 200 | 11.530394786s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:49:44 | 200 | 10.793111392s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:49:50 | 200 |  6.042727961s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:50:45 | 200 | 54.652705696s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:51:08 | 200 | 22.735352124s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:51:09 | 200 |  1.307439772s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:51:11 | 200 |  1.528037086s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:51:12 | 200 |  1.297496754s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:52:14 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:53:20 | 200 |          1m6s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:53:40 | 200 | 20.414399497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 17:53:51 | 200 | 10.235140488s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:53:52 | 200 |  1.136459365s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:54:23 | 200 | 31.567751276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:54:26 | 200 |  3.051463859s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:54:32 | 200 |  6.035238021s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:54:33 | 200 |    864.4637ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:55:02 | 200 | 28.797094413s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:55:06 | 200 |   3.58451707s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:55:06 | 200 |  672.223611ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:55:09 | 200 |  2.373339344s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:55:11 | 200 |  2.296678594s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:56:20 | 200 |          1m9s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:57:10 | 200 | 49.519616446s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:57:30 | 200 |  19.89185701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 17:57:35 | 200 |  5.792442297s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:57:37 | 200 |  1.187468077s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:14 | 200 | 37.465626646s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:17 | 200 |  2.783917238s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:23 | 200 |  6.411944336s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:24 | 200 |  827.370963ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:31 | 200 |  6.522965591s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:31 | 200 |  835.536609ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:42 | 200 | 10.146951018s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 17:58:43 | 200 |  1.144534757s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:00:12 | 200 |         1m29s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:09 | 200 |  57.26727412s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:29 | 200 | 19.867588983s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 18:01:37 | 200 |  7.436555322s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:38 | 200 |  1.600658818s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:44 | 200 |  5.967840714s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:45 | 200 |  791.741809ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:53 | 200 |  8.086502102s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:01:54 | 200 |  1.400827255s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:02:51 | 200 | 56.218655456s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:02:54 | 200 |   3.65368288s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:02:57 | 200 |  2.898618583s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:02:58 | 200 |  1.013955069s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:03:01 | 200 |  2.613470209s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:03:03 | 200 |  1.972072343s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:14 | 200 |         1m10s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:18 | 200 |  4.323281073s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:27 | 200 |  8.940210278s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:29 | 200 |  1.783506539s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:32 | 200 |  3.623903276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:04:33 | 200 |   686.63871ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:05:01 | 200 | 27.549503436s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:05:03 | 200 |  2.057353592s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:05:04 | 200 |  1.497851391s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:05:07 | 200 |  2.564185489s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:00 | 200 |  52.87754593s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:06 | 200 |  6.423903584s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:18 | 200 | 11.426595972s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:27 | 200 |  9.171768273s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:34 | 200 |  6.810984305s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:44 | 200 | 10.644201895s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:06:45 | 200 |  1.145561845s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:07:51 | 200 |          1m5s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:07:58 | 200 |  6.925228463s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:08:00 | 200 |  1.270257948s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:08:01 | 200 |  1.554744977s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:08:03 | 200 |  1.686718407s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:09 | 200 |          1m6s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:13 | 200 |  3.121051135s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:19 | 200 |  6.257987664s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:20 | 200 |  1.478659497s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:24 | 200 |  3.824222285s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:25 | 200 |  724.379184ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:29 | 200 |  3.790882127s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:09:29 | 200 |  723.800473ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:10:28 | 200 | 58.081491502s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:10:35 | 200 |  7.669470501s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:10:38 | 200 |  2.913357065s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:10:40 | 200 |  1.751507492s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:10:42 | 200 |  2.058188415s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:11:10 | 200 | 27.758357038s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:11:12 | 200 |  1.794128536s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:11:13 | 200 |  1.853039956s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:11:16 | 200 |  2.119832309s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:12:25 | 200 |          1m9s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:12:29 | 200 |  3.781516313s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:12:33 | 200 |   4.63349414s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:12:36 | 200 |  3.104430011s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:12:40 | 200 |  3.158487849s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:14:16 | 200 |         1m36s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:15:22 | 200 |          1m5s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:15:41 | 200 | 19.895505713s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 18:16:00 | 200 | 18.451864703s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:16:02 | 200 |  2.177999677s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:16:13 | 200 | 11.107207689s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:16:46 | 200 | 32.892911445s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:16:49 | 200 |  2.674009514s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:02 | 200 |         1m13s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:24 | 200 |  21.61221173s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:25 | 200 |  1.830240204s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:27 | 200 |  1.448954533s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:28 | 200 |   723.09959ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:18:29 | 200 |  1.315986006s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:20:48 | 200 |         1m17s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:07 | 200 | 18.210040201s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:09 | 200 |  2.784908223s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:14 | 200 |  4.806667575s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:15 | 200 |  804.685493ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:20 | 200 |  4.889843832s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:21:21 | 200 |   802.67332ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:23:30 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:24:41 | 200 |         1m10s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:25:01 | 200 | 19.891551711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 18:25:06 | 200 |  4.881330909s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:25:06 | 200 |  825.491441ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:25:11 | 200 |  4.201835772s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:25:16 | 200 |  5.720143331s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:25:18 | 200 |  1.413288569s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:28:54 | 200 |         1m16s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:06 | 200 | 12.487427079s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:08 | 200 |  2.215229296s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:09 | 200 |  758.863799ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:13 | 200 |   3.95934266s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:14 | 200 |  742.593338ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:29:16 | 200 |  2.260811976s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:33:54 | 200 |          1m1s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:34:00 | 200 |  6.156033683s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:34:02 | 200 |  2.167774971s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:34:06 | 200 |  3.856119428s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:34:07 | 200 |  809.001669ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:34:09 | 200 |  2.297243432s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:19 | 200 |  56.05403477s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:32 | 200 | 12.587739713s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:34 | 200 |  2.640704251s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:35 | 200 |  702.052923ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:37 | 200 |  2.282961166s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:38 | 200 |  712.504785ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:41 | 200 |  2.814515639s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:46 | 200 |  5.235521163s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:36:47 | 200 |  883.332644ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:41:50 | 200 |          1m5s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:05 | 200 | 14.667738495s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T18:42:05.581+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T18:42:05.581+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78068711424 required="14.9 GiB"
time=2025-10-25T18:42:05.784+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="974.5 GiB" free_swap="0 B"
time=2025-10-25T18:42:05.784+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T18:42:05.878+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 46447"
time=2025-10-25T18:42:05.878+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T18:42:05.878+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T18:42:05.894+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T18:42:05.895+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T18:42:05.896+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46447"
time=2025-10-25T18:42:05.982+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-25T18:42:06.145+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T18:42:13.157+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T18:42:13.823+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T18:42:13.823+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T18:42:13.823+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T18:42:13.823+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T18:42:13.823+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T18:42:13.833+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T18:42:13.833+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T18:42:24.705+11:00 level=INFO source=server.go:637 msg="llama runner started in 18.83 seconds"
[GIN] 2025/10/25 - 18:42:31 | 200 | 26.591248188s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:32 | 200 |  803.096312ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:34 | 200 |  2.237861281s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:35 | 200 |  667.976784ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:38 | 200 |  3.004311375s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:41 | 200 |  3.540707231s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:42:42 | 200 |  775.855969ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:45:51 | 200 |         1m26s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:02 | 200 | 10.575285942s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:04 | 200 |  1.831779883s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:05 | 200 |  1.405337522s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:09 | 200 |   3.21842727s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:09 | 200 |  690.514824ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:14 | 200 |  4.715934688s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:46:15 | 200 |  766.041119ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:09 | 200 |  54.25750579s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:16 | 200 |  7.285214717s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:20 | 200 |  3.232236582s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:24 | 200 |  4.584341444s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:25 | 200 |  789.326027ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:28 | 200 |  2.659993512s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:47:28 | 200 |  751.384816ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:17 | 200 |         1m11s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:30 | 200 | 12.638684074s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:32 | 200 |  2.764978829s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:37 | 200 |  4.446694089s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:37 | 200 |  738.402396ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:40 | 200 |  2.723648728s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:49:41 | 200 |  684.186515ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:50:55 | 200 |         1m14s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:12 | 200 | 17.027296596s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:17 | 200 |  4.330959283s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:18 | 200 |  1.146940572s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:22 | 200 |  3.759540539s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:22 | 200 |  680.645473ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:26 | 200 |  3.821237377s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:51:27 | 200 |  776.807661ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:54:24 | 200 |         1m30s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:55:24 | 200 |          1m0s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:55:44 | 200 | 20.017782311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 18:55:54 | 200 |  9.387847452s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:55:55 | 200 |  1.656026598s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:56:02 | 200 |  6.405013997s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:56:03 | 200 |  1.810949862s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:56:09 | 200 |  5.345329242s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:56:10 | 200 |  727.618793ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:35 | 200 |          1m2s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:38 | 200 |  3.406769082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:41 | 200 |  2.578985213s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:43 | 200 |  2.343012317s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:44 | 200 |  767.789131ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 18:58:45 | 200 |  1.519915082s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:10 | 200 | 57.597543285s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:19 | 200 |  9.917720468s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:22 | 200 |  2.898373752s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:25 | 200 |  2.504486952s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:26 | 200 |  723.873064ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:03:27 | 200 |  1.850733826s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:06:16 | 200 |         1m12s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:07:02 | 200 | 46.285018305s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:07:22 | 200 | 19.898832685s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 19:07:34 | 200 | 11.820467014s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:07:35 | 200 |  1.221269276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:07:44 | 200 |  9.350135093s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:07:45 | 200 |  1.093743442s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:01 | 200 | 15.871330738s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:02 | 200 |  1.293085363s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:13 | 200 | 10.519521472s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:14 | 200 |  1.188202508s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:22 | 200 |  7.728353466s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:08:23 | 200 |   854.01644ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:10:57 | 200 | 58.563333445s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:01 | 200 |  3.687087378s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:06 | 200 |  5.456106373s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:07 | 200 |  731.418069ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:11 | 200 |   4.44371277s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:16 | 200 |    4.6472622s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:17 | 200 |  785.918877ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:20 | 200 |  3.157095654s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:21 | 200 |  758.565779ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:29 | 200 |  8.215087065s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:11:30 | 200 |  1.098960555s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:13 | 200 |         1m18s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:26 | 200 | 12.685901268s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:29 | 200 |  3.746659646s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:33 | 200 |  3.183291764s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:33 | 200 |  806.585814ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:37 | 200 |  3.102232321s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:13:37 | 200 |  758.623071ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:16:45 | 200 |         1m16s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:16:49 | 200 |  4.422097155s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:16:53 | 200 |  3.593489203s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:16:57 | 200 |  3.729871334s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:16:59 | 200 |  2.690794311s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:17:00 | 200 |   677.03736ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:17:03 | 200 |  3.112414214s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:19:09 | 200 | 47.798070842s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:19:40 | 200 | 31.405546834s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:00 | 200 | 19.908846741s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 19:20:10 | 200 | 10.228308427s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:12 | 200 |  1.585450025s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:21 | 200 |  9.356008345s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:22 | 200 |  1.120623953s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:29 | 200 |  6.506210153s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:20:30 | 200 |  814.842073ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:21:55 | 200 |         1m25s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:22:31 | 200 | 35.760081144s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:22:51 | 200 | 19.899186329s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 19:22:57 | 200 |  6.202173058s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:22:58 | 200 |  762.669143ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:13 | 200 | 15.635794319s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:20 | 200 |  6.614350766s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:21 | 200 |  800.383794ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:27 | 200 |   5.91378943s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:27 | 200 |  742.671727ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:32 | 200 |  4.552457987s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:23:33 | 200 |  774.681126ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:23 | 200 | 24.494965196s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:25 | 200 |  2.225019612s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:26 | 200 |   1.18234887s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:29 | 200 |  2.592465256s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:30 | 200 |  700.125244ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:25:32 | 200 |  2.474161851s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:20 | 200 |          1m4s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:26 | 200 |  6.323202727s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T19:31:27.026+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T19:31:27.027+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78068711424 required="14.9 GiB"
time=2025-10-25T19:31:27.229+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.1 GiB" free_swap="0 B"
time=2025-10-25T19:31:27.230+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T19:31:27.321+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 36867"
time=2025-10-25T19:31:27.322+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T19:31:27.322+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T19:31:27.336+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T19:31:27.337+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T19:31:27.338+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36867"
time=2025-10-25T19:31:27.419+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T19:31:27.507+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T19:31:27.587+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T19:31:27.642+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T19:31:27.642+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T19:31:27.642+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T19:31:27.642+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T19:31:27.642+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T19:31:27.651+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T19:31:27.651+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T19:31:34.131+11:00 level=INFO source=server.go:637 msg="llama runner started in 6.81 seconds"
[GIN] 2025/10/25 - 19:31:38 | 200 | 11.759548254s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:39 | 200 |  1.424580723s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:42 | 200 |  2.550432111s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:42 | 200 |  709.292775ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:31:49 | 200 |  6.740001382s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:15 | 200 |         1m11s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:18 | 200 |  3.012605726s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:21 | 200 |  2.472041258s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:23 | 200 |  2.314078857s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:24 | 200 |    710.0745ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:26 | 200 |  2.622958987s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:36:27 | 200 |  678.427268ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:38:54 | 200 |          1m8s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:00 | 200 |  6.127443638s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:03 | 200 |   3.22690456s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:04 | 200 |  722.845007ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:06 | 200 |  2.217243062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:07 | 200 |  660.168431ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:10 | 200 |  2.838331096s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:10 | 200 |  693.151738ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:45 | 200 | 34.810572352s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:49 | 200 |  4.028880299s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:50 | 200 |  781.306951ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:54 | 200 |  3.952727109s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:55 | 200 |  733.524876ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:39:59 | 200 |  4.178174313s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:40:00 | 200 |  751.733317ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:43:53 | 200 |          1m2s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:02 | 200 |  9.585386402s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:05 | 200 |  2.519787865s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:08 | 200 |  2.852411758s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:08 | 200 |  777.853348ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:11 | 200 |   2.96337694s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:44:12 | 200 |   693.03868ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:10 | 200 | 52.672104433s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:21 | 200 | 10.878160106s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:26 | 200 |  5.079014043s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:27 | 200 |  969.547086ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:31 | 200 |  3.939120515s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:31 | 200 |  742.542959ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:37 | 200 |  6.070194514s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:46:38 | 200 |  814.441429ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:28 | 200 | 58.409573683s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:34 | 200 |  6.374649792s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:36 | 200 |  2.233456083s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:41 | 200 |  4.812801211s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:42 | 200 |  807.507058ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:49:46 | 200 |  3.687429616s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:50:55 | 200 |          1m9s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:51:35 | 200 | 39.677779253s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:51:55 | 200 | 19.945929195s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/25 - 19:52:17 | 200 | 22.142658603s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:53:36 | 200 |         1m18s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:53:44 | 200 |  7.539255407s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:53:50 | 200 |  6.465077761s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:53:51 | 200 |  805.318642ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:56:56 | 200 |         1m21s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:57:04 | 200 |   8.88657706s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:57:09 | 200 |   4.22880539s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:57:14 | 200 |  5.621204569s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:57:18 | 200 |  3.872893256s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:57:19 | 200 |  717.579673ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 19:59:37 | 200 |          1m0s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:02 | 200 |  24.19106436s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:05 | 200 |  3.161251588s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:05 | 200 |   680.98435ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:08 | 200 |  2.572026688s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:09 | 200 |  703.942015ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:00:11 | 200 |   1.87615265s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T20:11:47.392+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T20:11:47.594+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.5 GiB" free_swap="0 B"
time=2025-10-25T20:11:47.594+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T20:11:47.862+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 43553"
time=2025-10-25T20:11:47.863+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T20:11:47.863+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:11:47.863+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:11:47.880+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:11:48.585+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:11:48.586+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:43553"
time=2025-10-25T20:11:48.616+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T20:12:23.190+11:00 level=INFO source=server.go:637 msg="llama runner started in 35.33 seconds"
[GIN] 2025/10/25 - 20:13:10 | 200 |         1m23s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:13:14 | 200 |  3.721086217s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T20:13:15.132+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T20:13:15.132+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78049837056 required="14.9 GiB"
time=2025-10-25T20:13:15.334+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.7 GiB" free_swap="0 B"
time=2025-10-25T20:13:15.335+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T20:13:15.416+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35963"
time=2025-10-25T20:13:15.417+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T20:13:15.417+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:13:15.430+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T20:13:15.430+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35963"
time=2025-10-25T20:13:15.433+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:13:15.521+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:13:15.622+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:13:15.684+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T20:13:15.757+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T20:13:15.757+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T20:13:15.757+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T20:13:15.757+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T20:13:15.757+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T20:13:15.766+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T20:13:15.766+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T20:13:22.241+11:00 level=INFO source=server.go:637 msg="llama runner started in 6.82 seconds"
[GIN] 2025/10/25 - 20:13:41 | 200 | 26.379754143s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:13:42 | 200 |  1.694598176s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:14:04 | 200 | 21.275895251s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:14:05 | 200 |  1.652374009s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:14:10 | 200 |  5.121184684s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:14:17 | 200 |  6.749739339s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:14:18 | 200 |  802.374388ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T20:23:33.211+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T20:23:33.407+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.1 GiB" free_swap="0 B"
time=2025-10-25T20:23:33.407+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T20:23:33.640+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 37757"
time=2025-10-25T20:23:33.641+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T20:23:33.641+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:23:33.641+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:23:33.655+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:23:33.755+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:23:33.756+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37757"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T20:23:33.892+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T20:23:35.647+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/25 - 20:24:26 | 200 | 53.116469496s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:24:35 | 200 |  9.442492841s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T20:24:35.913+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T20:24:35.913+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78045642752 required="14.9 GiB"
time=2025-10-25T20:24:36.115+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.6 GiB" free_swap="0 B"
time=2025-10-25T20:24:36.115+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T20:24:36.203+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 40097"
time=2025-10-25T20:24:36.203+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T20:24:36.203+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:24:36.216+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:24:36.216+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T20:24:36.216+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40097"
time=2025-10-25T20:24:36.303+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:24:36.391+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:24:36.468+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T20:24:36.522+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T20:24:36.522+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T20:24:36.522+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T20:24:36.522+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T20:24:36.522+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T20:24:36.531+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T20:24:36.531+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T20:24:40.774+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.57 seconds"
[GIN] 2025/10/25 - 20:24:49 | 200 | 13.776481139s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:24:50 | 200 |  1.029869302s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:24:54 | 200 |  4.232445439s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:25:04 | 200 | 10.162994319s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:25:05 | 200 |  1.188286739s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:25:18 | 200 | 12.949641721s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:25:25 | 200 |  7.099227641s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:28:09 | 200 | 31.408443047s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:28:20 | 200 | 11.396432315s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:28:21 | 200 |  1.197590959s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:28:32 | 200 | 10.645920874s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:28:38 | 200 |  5.764739298s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:33:13 | 200 |          1m3s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:34:13 | 200 | 59.096611617s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:34:33 | 200 | 19.896689863s |       127.0.0.1 | POST     "/api/generate"
time=2025-10-25T20:34:33.658+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T20:34:33.658+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78043545600 required="14.9 GiB"
time=2025-10-25T20:34:33.867+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.3 GiB" free_swap="0 B"
time=2025-10-25T20:34:33.868+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T20:34:33.958+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35035"
time=2025-10-25T20:34:33.958+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T20:34:33.958+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:34:33.974+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:34:33.972+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T20:34:33.972+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35035"
time=2025-10-25T20:34:34.059+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:34:34.146+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:34:34.224+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T20:34:34.277+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T20:34:34.277+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T20:34:34.277+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T20:34:34.277+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T20:34:34.277+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T20:34:34.287+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T20:34:34.287+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T20:34:38.499+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.54 seconds"
[GIN] 2025/10/25 - 20:34:52 | 200 | 18.887923268s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:34:53 | 200 |  1.320104236s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:35:12 | 200 | 18.794438698s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:35:13 | 200 |  1.707710155s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:35:23 | 200 |  9.830547101s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:35:25 | 200 |  1.281140748s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:40 | 200 | 43.074653278s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:44 | 200 |   3.86547539s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:50 | 200 |  6.011023333s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:50 | 200 |   816.81513ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:52 | 200 |  1.716184772s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:55 | 200 |   3.30457527s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:36:57 | 200 |  1.566509421s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:19 | 200 |         1m10s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:27 | 200 |  7.849317062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:33 | 200 |  6.051018758s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:34 | 200 |  747.996544ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:37 | 200 |  3.064178474s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:38 | 200 |  678.963804ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:47 | 200 |    8.9716257s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:53 | 200 |  5.767163818s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:39:53 | 200 |   735.77723ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:43:58 | 200 | 25.841298189s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:05 | 200 |  6.671563605s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:10 | 200 |  4.412526203s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:15 | 200 |  5.396792199s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:16 | 200 |  1.105203174s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:21 | 200 |  4.578079966s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 20:44:28 | 200 |  7.322676986s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T20:58:59.843+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T20:59:00.039+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.3 GiB" free_swap="0 B"
time=2025-10-25T20:59:00.039+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T20:59:00.292+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 38885"
time=2025-10-25T20:59:00.292+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T20:59:00.292+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T20:59:00.292+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T20:59:00.306+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T20:59:01.327+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T20:59:01.327+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38885"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2025-10-25T20:59:01.545+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T20:59:06.556+11:00 level=INFO source=server.go:637 msg="llama runner started in 6.26 seconds"
[GIN] 2025/10/25 - 21:00:10 | 200 |         1m11s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:00:40 | 200 |  30.15625943s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:00:41.288+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T21:00:41.288+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78054031360 required="14.9 GiB"
time=2025-10-25T21:00:41.493+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.0 GiB" free_swap="0 B"
time=2025-10-25T21:00:41.493+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T21:00:41.583+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35715"
time=2025-10-25T21:00:41.583+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T21:00:41.583+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:00:41.600+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:00:41.597+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T21:00:41.597+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35715"
time=2025-10-25T21:00:41.683+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:00:41.772+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:00:41.851+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T21:00:41.902+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T21:00:41.902+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T21:00:41.902+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T21:00:41.902+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T21:00:41.902+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T21:00:41.912+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T21:00:41.912+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T21:00:49.380+11:00 level=INFO source=server.go:637 msg="llama runner started in 7.80 seconds"
[GIN] 2025/10/25 - 21:00:54 | 200 | 13.279419446s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:00:57 | 200 |  3.058605498s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:00:58 | 200 |  837.993721ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:01:01 | 200 |  3.375460285s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:01:02 | 200 |  783.497201ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:03:15 | 200 | 23.397911219s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:03:18 | 200 |  2.700439717s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:03:20 | 200 |  2.382178905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:03:23 | 200 |  2.981072001s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:10:16.502+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T21:10:16.713+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.1 GiB" free_swap="0 B"
time=2025-10-25T21:10:16.714+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T21:10:16.992+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 44587"
time=2025-10-25T21:10:16.995+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T21:10:16.996+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:10:17.017+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:10:17.032+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:10:17.122+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:10:17.122+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:44587"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T21:10:17.268+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T21:10:19.022+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.03 seconds"
[GIN] 2025/10/25 - 21:11:03 | 200 |  47.29894303s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:11:06 | 200 |  2.752459094s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:11:06.541+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T21:11:06.541+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78072905728 required="14.9 GiB"
time=2025-10-25T21:11:06.747+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.6 GiB" free_swap="0 B"
time=2025-10-25T21:11:06.747+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T21:11:06.833+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 34007"
time=2025-10-25T21:11:06.834+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T21:11:06.834+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:11:06.855+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:11:06.847+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T21:11:06.847+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34007"
time=2025-10-25T21:11:06.934+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:11:07.021+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:11:07.105+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T21:11:07.161+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T21:11:07.161+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T21:11:07.161+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T21:11:07.161+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T21:11:07.161+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T21:11:07.170+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T21:11:07.170+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T21:11:11.388+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.55 seconds"
[GIN] 2025/10/25 - 21:11:15 | 200 |  9.121783117s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:11:17 | 200 |  1.953619105s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:11:19 | 200 |  2.371864818s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:12:07 | 200 | 28.998981569s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:12:13 | 200 |  5.859268258s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:12:18 | 200 |  4.515247062s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:12:21 | 200 |  3.382208898s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:12:27 | 200 |  5.587947132s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:24:07.736+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T21:24:07.938+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.0 GiB" free_swap="0 B"
time=2025-10-25T21:24:07.939+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T21:24:08.166+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 46673"
time=2025-10-25T21:24:08.166+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T21:24:08.166+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:24:08.166+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:24:08.180+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:24:08.267+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:24:08.268+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:46673"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T21:24:08.418+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T21:24:10.172+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/25 - 21:24:53 | 200 | 46.457132072s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:24:56 | 200 |  2.433276134s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:24:56.771+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T21:24:56.771+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78056128512 required="14.9 GiB"
time=2025-10-25T21:24:56.975+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.5 GiB" free_swap="0 B"
time=2025-10-25T21:24:56.975+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T21:24:57.057+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 41913"
time=2025-10-25T21:24:57.057+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T21:24:57.057+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:24:57.070+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T21:24:57.071+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:41913"
time=2025-10-25T21:24:57.074+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:24:57.162+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:24:57.247+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:24:57.325+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T21:24:57.378+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T21:24:57.378+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T21:24:57.378+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T21:24:57.378+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T21:24:57.378+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T21:24:57.387+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T21:24:57.387+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T21:25:01.387+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.33 seconds"
[GIN] 2025/10/25 - 21:25:10 | 200 |  13.69372131s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:25:29 | 200 | 19.916850349s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:25:31 | 200 |  1.710705166s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:25:44 | 200 | 12.365758552s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:25:52 | 200 |  8.431653256s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:26:03 | 200 | 11.402991436s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:38:38.774+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T21:38:38.966+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.0 GiB" free_swap="0 B"
time=2025-10-25T21:38:38.966+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T21:38:39.195+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 40985"
time=2025-10-25T21:38:39.195+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T21:38:39.195+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:38:39.195+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:38:39.209+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:38:39.310+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:38:39.314+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40985"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T21:38:39.447+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T21:38:41.201+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/25 - 21:39:49 | 200 |         1m10s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:39:54 | 200 |  5.320401369s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:39:55.022+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T21:39:55.022+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78037254144 required="14.9 GiB"
time=2025-10-25T21:39:55.222+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.5 GiB" free_swap="0 B"
time=2025-10-25T21:39:55.222+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T21:39:55.312+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 38275"
time=2025-10-25T21:39:55.312+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T21:39:55.312+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:39:55.328+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:39:55.325+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T21:39:55.325+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38275"
time=2025-10-25T21:39:55.412+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-25T21:39:55.579+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:39:55.610+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:39:55.749+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T21:39:55.749+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T21:39:55.749+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T21:39:55.749+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T21:39:55.749+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T21:39:55.758+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T21:39:55.758+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T21:39:59.906+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.59 seconds"
[GIN] 2025/10/25 - 21:40:13 | 200 | 18.537249039s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:40:24 | 200 | 11.287335899s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:40:33 | 200 |  8.741019576s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:56:17.319+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T21:56:17.511+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="973.2 GiB" free_swap="0 B"
time=2025-10-25T21:56:17.511+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T21:56:17.731+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 43999"
time=2025-10-25T21:56:17.732+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T21:56:17.732+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:56:17.732+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:56:17.746+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:56:17.907+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:56:17.908+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:43999"
time=2025-10-25T21:56:17.983+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T21:56:19.738+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/25 - 21:57:37 | 200 |         1m20s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:57:40 | 200 |  3.231087104s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T21:57:41.039+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T21:57:41.040+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-6c53312f-fab3-d4ee-bf36-01096ef183c7 parallel=1 available=78054031360 required="14.9 GiB"
time=2025-10-25T21:57:41.250+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="972.9 GiB" free_swap="0 B"
time=2025-10-25T21:57:41.250+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T21:57:41.339+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 40585"
time=2025-10-25T21:57:41.339+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T21:57:41.339+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T21:57:41.360+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T21:57:41.353+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T21:57:41.353+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40585"
time=2025-10-25T21:57:41.440+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T21:57:41.527+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T21:57:41.612+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T21:57:41.658+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T21:57:41.658+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T21:57:41.658+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T21:57:41.658+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T21:57:41.658+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T21:57:41.667+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T21:57:41.667+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T21:57:49.386+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.05 seconds"
[GIN] 2025/10/25 - 21:57:54 | 200 | 13.703835676s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:58:01 | 200 |  7.282814975s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:58:05 | 200 |  4.012184599s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 21:58:08 | 200 |  3.303111635s |       127.0.0.1 | POST     "/v1/chat/completions"
[Raw Model Testing] Response time: 83.88s, Quiz time: 28.32s, Total: 112.20s
[Test] Logged: elementary_test - evidence_change_relationship - Q14
Running question 15
