time=2025-10-25T23:24:22.482+11:00 level=INFO source=routes.go:1304 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/mvo1/lb64_scratch/ollama-models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-10-25T23:24:22.619+11:00 level=INFO source=images.go:477 msg="total blobs: 34"
time=2025-10-25T23:24:22.625+11:00 level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-10-25T23:24:22.630+11:00 level=INFO source=routes.go:1357 msg="Listening on 127.0.0.1:11434 (version 0.11.4)"
time=2025-10-25T23:24:22.630+11:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-10-25T23:24:23.676+11:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda variant=v12 compute=8.0 driver=12.2 name="NVIDIA A100 80GB PCIe" total="79.2 GiB" available="78.7 GiB"
[GIN] 2025/10/25 - 23:24:25 | 200 |   71.198686ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-25T23:29:15.635+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544192512 required="6.1 GiB"
time=2025-10-25T23:29:15.851+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="985.0 GiB" free_swap="0 B"
time=2025-10-25T23:29:15.852+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T23:29:16.169+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 40601"
time=2025-10-25T23:29:16.172+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T23:29:16.173+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T23:29:16.210+11:00 level=INFO source=runner.go:815 msg="starting go runner"
time=2025-10-25T23:29:16.210+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T23:29:26.240+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T23:29:26.264+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40601"
time=2025-10-25T23:29:26.339+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T23:29:32.271+11:00 level=INFO source=server.go:637 msg="llama runner started in 16.10 seconds"
[GIN] 2025/10/25 - 23:30:41 | 200 |         1m26s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:07 | 200 |  26.11775272s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T23:31:07.979+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T23:31:07.979+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78081294336 required="14.9 GiB"
time=2025-10-25T23:31:08.178+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.3 GiB" free_swap="0 B"
time=2025-10-25T23:31:08.179+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T23:31:08.256+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 36609"
time=2025-10-25T23:31:08.256+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T23:31:08.256+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T23:31:08.277+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T23:31:08.269+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T23:31:08.270+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36609"
time=2025-10-25T23:31:08.356+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T23:31:08.441+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T23:31:08.529+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T23:31:08.573+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T23:31:08.573+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T23:31:08.573+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T23:31:08.573+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T23:31:08.573+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T23:31:08.583+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T23:31:08.583+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T23:31:22.240+11:00 level=INFO source=server.go:637 msg="llama runner started in 13.98 seconds"
[GIN] 2025/10/25 - 23:31:30 | 200 | 23.615739555s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:33 | 200 |  2.969735892s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:36 | 200 |  2.952371972s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:37 | 200 |  795.508369ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:43 | 200 |  6.219440752s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:31:57 | 200 | 13.209329903s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:35:28 | 200 |          1m2s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:35:45 | 200 | 17.484318602s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:02 | 200 | 16.876311193s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:12 | 200 |  9.959870936s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:15 | 200 |  3.446710148s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:34 | 200 | 19.072711953s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:37 | 200 |  2.399101983s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:36:41 | 200 |      45.534µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/25 - 23:36:41 | 200 |      91.948µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/10/25 - 23:36:43 | 200 |  6.490829987s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T23:52:45.467+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-25T23:52:45.668+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.8 GiB" free_swap="0 B"
time=2025-10-25T23:52:45.668+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-25T23:52:45.902+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 35281"
time=2025-10-25T23:52:45.902+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-25T23:52:45.902+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T23:52:45.903+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T23:52:45.917+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T23:52:46.016+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T23:52:46.016+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35281"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-25T23:52:46.154+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-25T23:52:48.160+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.26 seconds"
[GIN] 2025/10/25 - 23:53:37 | 200 |   52.8312713s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:53:42 | 200 |  4.562617924s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-25T23:53:42.988+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-25T23:53:42.988+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78037254144 required="14.9 GiB"
time=2025-10-25T23:53:43.184+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.3 GiB" free_swap="0 B"
time=2025-10-25T23:53:43.184+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-25T23:53:43.267+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 33617"
time=2025-10-25T23:53:43.267+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-25T23:53:43.267+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-25T23:53:43.288+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-25T23:53:43.280+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-25T23:53:43.281+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:33617"
time=2025-10-25T23:53:43.367+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-25T23:53:43.453+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-25T23:53:43.539+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-25T23:53:43.584+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-25T23:53:43.584+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-25T23:53:43.584+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-25T23:53:43.584+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-25T23:53:43.584+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-25T23:53:43.594+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-25T23:53:43.594+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-25T23:53:48.345+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.08 seconds"
[GIN] 2025/10/25 - 23:54:00 | 200 | 17.699109167s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:54:01 | 200 |  1.285196869s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:54:05 | 200 |  4.098673172s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:54:06 | 200 |  749.320544ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/25 - 23:54:15 | 200 |  9.155626602s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:02:35.426+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T00:02:35.622+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.7 GiB" free_swap="0 B"
time=2025-10-26T00:02:35.622+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:02:35.854+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 45733"
time=2025-10-26T00:02:35.854+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:02:35.854+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:02:35.855+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:02:35.869+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:02:35.969+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:02:35.970+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:45733"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T00:02:36.105+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:02:37.909+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.05 seconds"
[GIN] 2025/10/26 - 00:03:45 | 200 |         1m10s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:04:37 | 200 | 52.078738111s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:04:40.883+11:00 level=WARN source=types.go:626 msg="invalid option provided" option=low_vram
time=2025-10-26T00:04:42.182+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.7 GiB" free_swap="0 B"
time=2025-10-26T00:04:42.182+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=999 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:04:42.413+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 999 --threads 56 --parallel 1 --port 37981"
time=2025-10-26T00:04:42.413+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:04:42.414+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:04:42.414+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:04:42.429+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:04:42.518+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:04:42.519+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37981"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T00:04:42.665+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:04:44.499+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.09 seconds"
[GIN] 2025/10/26 - 00:05:04 | 200 | 23.461858705s |       127.0.0.1 | POST     "/api/generate"
time=2025-10-26T00:05:04.754+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:05:04.754+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78068711424 required="14.9 GiB"
time=2025-10-26T00:05:04.952+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T00:05:04.953+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:05:05.043+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 37049"
time=2025-10-26T00:05:05.044+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:05:05.044+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:05:05.059+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:05:05.058+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:05:05.058+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37049"
time=2025-10-26T00:05:05.150+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:05:05.247+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:05:05.310+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T00:05:05.376+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:05:05.376+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:05:05.376+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:05:05.376+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:05:05.376+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:05:05.385+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:05:05.385+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:05:10.104+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.06 seconds"
[GIN] 2025/10/26 - 00:05:21 | 200 | 17.401530369s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:22 | 200 |  1.265867122s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:32 | 200 |  9.285224326s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:33 | 200 |  845.338099ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:42 | 200 |   8.94596516s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:42 | 200 |   868.36888ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:50 | 200 |  7.895228092s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:05:51 | 200 |  1.091693466s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:20:56.330+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T00:20:56.523+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.1 GiB" free_swap="0 B"
time=2025-10-26T00:20:56.523+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:20:56.747+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 33295"
time=2025-10-26T00:20:56.748+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:20:56.748+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:20:56.748+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:20:56.767+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:20:57.259+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:20:57.259+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:33295"
time=2025-10-26T00:20:57.333+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:20:59.088+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.34 seconds"
[GIN] 2025/10/26 - 00:22:02 | 200 |          1m6s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:23:00 | 200 | 57.877003708s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:23:00.796+11:00 level=WARN source=types.go:626 msg="invalid option provided" option=low_vram
time=2025-10-26T00:23:01.909+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.1 GiB" free_swap="0 B"
time=2025-10-26T00:23:01.910+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=999 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:23:02.380+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 999 --threads 56 --parallel 1 --port 45457"
time=2025-10-26T00:23:02.380+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:23:02.380+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:23:02.380+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:23:02.394+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:23:02.472+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:23:02.473+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:45457"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T00:23:02.631+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:23:04.317+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.94 seconds"
[GIN] 2025/10/26 - 00:23:24 | 200 |  23.39342106s |       127.0.0.1 | POST     "/api/generate"
time=2025-10-26T00:23:24.965+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:23:24.966+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78098071552 required="14.9 GiB"
time=2025-10-26T00:23:25.168+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T00:23:25.168+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:23:25.257+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 42157"
time=2025-10-26T00:23:25.257+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:23:25.257+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:23:25.278+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:23:25.270+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:23:25.270+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:42157"
time=2025-10-26T00:23:25.361+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:23:25.435+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:23:25.530+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T00:23:25.566+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:23:25.566+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:23:25.566+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:23:25.566+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:23:25.566+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:23:25.575+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:23:25.575+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:23:34.186+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.93 seconds"
[GIN] 2025/10/26 - 00:23:45 | 200 | 20.934757507s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:23:46 | 200 |  1.716658646s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:23:57 | 200 | 10.291828185s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:23:57 | 200 |  836.719037ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:24:06 | 200 |  8.523349473s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:24:07 | 200 |   1.35895963s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:28:41.549+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="63.9 GiB"
time=2025-10-26T00:28:41.549+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=68600070144 required="6.1 GiB"
time=2025-10-26T00:28:42.009+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="981.0 GiB" free_swap="0 B"
time=2025-10-26T00:28:42.009+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[63.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:28:42.237+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 42807"
time=2025-10-26T00:28:42.237+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:28:42.237+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:28:42.238+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:28:42.251+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:28:42.325+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:28:42.326+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42807"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65422 MiB free
time=2025-10-26T00:28:42.489+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:28:43.993+11:00 level=INFO source=server.go:637 msg="llama runner started in 1.76 seconds"
[GIN] 2025/10/26 - 00:30:11 | 200 |         1m29s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:30:19 | 200 |  6.614166986s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:30:21.283+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:30:21.283+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78079197184 required="14.9 GiB"
time=2025-10-26T00:30:21.501+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.8 GiB" free_swap="0 B"
time=2025-10-26T00:30:21.502+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:30:21.591+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 34099"
time=2025-10-26T00:30:21.592+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:30:21.592+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:30:21.608+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:30:21.606+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:30:21.606+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34099"
time=2025-10-26T00:30:21.692+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-26T00:30:22.030+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:30:22.098+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:30:22.231+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:30:22.231+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:30:22.231+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:30:22.231+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:30:22.231+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:30:22.240+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:30:22.241+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:30:33.477+11:00 level=INFO source=server.go:637 msg="llama runner started in 11.88 seconds"
[GIN] 2025/10/26 - 00:30:40 | 200 | 19.471880135s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:30:46 | 200 |  5.932824849s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:30:50 | 200 |  4.880749234s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:31:04 | 200 | 13.103491993s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:36:11.976+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T00:36:12.171+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.9 GiB" free_swap="0 B"
time=2025-10-26T00:36:12.172+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:36:12.428+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 40651"
time=2025-10-26T00:36:12.429+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:36:12.429+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:36:12.429+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:36:12.444+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:36:12.788+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:36:12.798+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40651"
time=2025-10-26T00:36:12.931+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:36:17.176+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.75 seconds"
[GIN] 2025/10/26 - 00:44:00 | 200 |         7m48s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:06 | 200 |   5.94588096s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:44:07.864+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:44:07.865+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78093877248 required="14.9 GiB"
time=2025-10-26T00:44:08.072+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T00:44:08.072+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:44:08.161+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 32877"
time=2025-10-26T00:44:08.162+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:44:08.162+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:44:08.178+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:44:08.175+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:44:08.175+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:32877"
time=2025-10-26T00:44:08.263+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-26T00:44:08.430+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:44:08.854+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:44:09.522+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:44:09.522+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:44:09.522+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:44:09.522+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:44:09.522+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:44:09.531+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:44:09.531+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:44:22.280+11:00 level=INFO source=server.go:637 msg="llama runner started in 14.12 seconds"
[GIN] 2025/10/26 - 00:44:26 | 200 | 19.251728179s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:27 | 200 |  751.087745ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:29 | 200 |  2.631937822s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:30 | 200 |  959.118096ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:32 | 200 |  1.834777375s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:44:33 | 200 |  908.195045ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:52:25.921+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T00:52:26.119+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.8 GiB" free_swap="0 B"
time=2025-10-26T00:52:26.120+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:52:26.347+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 35049"
time=2025-10-26T00:52:26.347+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:52:26.347+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:52:26.348+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:52:26.362+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:52:26.460+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:52:26.464+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:35049"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T00:52:26.599+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:52:30.308+11:00 level=INFO source=server.go:637 msg="llama runner started in 3.96 seconds"
[GIN] 2025/10/26 - 00:53:12 | 200 | 46.569699843s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:53:17 | 200 |  5.537368078s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:53:18.408+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:53:18.409+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78089682944 required="14.9 GiB"
time=2025-10-26T00:53:18.607+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.3 GiB" free_swap="0 B"
time=2025-10-26T00:53:18.608+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:53:18.691+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 46637"
time=2025-10-26T00:53:18.691+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:53:18.691+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:53:18.704+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:53:18.705+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:53:18.705+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46637"
time=2025-10-26T00:53:18.792+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:53:18.891+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:53:18.955+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T00:53:19.021+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:53:19.021+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:53:19.021+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:53:19.021+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:53:19.021+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:53:19.030+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:53:19.030+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:53:23.680+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.99 seconds"
[GIN] 2025/10/26 - 00:53:34 | 200 | 16.331546162s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:53:36 | 200 |  2.717379974s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:53:39 | 200 |  2.352350809s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:58:46.374+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T00:58:46.577+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T00:58:46.578+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T00:58:46.797+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 34767"
time=2025-10-26T00:58:46.798+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T00:58:46.798+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:58:46.798+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:58:46.812+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:58:46.888+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:58:46.889+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34767"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T00:58:47.050+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T00:58:48.832+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.03 seconds"
[GIN] 2025/10/26 - 00:59:18 | 200 | 32.349939157s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T00:59:18.866+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T00:59:18.866+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78093877248 required="14.9 GiB"
time=2025-10-26T00:59:19.065+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T00:59:19.066+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T00:59:19.156+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35315"
time=2025-10-26T00:59:19.157+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T00:59:19.157+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T00:59:19.178+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T00:59:19.170+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T00:59:19.170+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35315"
time=2025-10-26T00:59:19.257+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T00:59:19.345+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T00:59:19.429+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T00:59:19.475+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T00:59:19.475+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T00:59:19.475+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T00:59:19.475+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T00:59:19.475+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T00:59:19.485+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T00:59:19.485+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T00:59:24.312+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.16 seconds"
[GIN] 2025/10/26 - 00:59:35 | 200 | 17.828643457s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:59:41 | 200 |  5.716047834s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 00:59:49 | 200 |  8.044746893s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:09:31.975+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T01:09:32.169+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.7 GiB" free_swap="0 B"
time=2025-10-26T01:09:32.170+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T01:09:32.634+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 38775"
time=2025-10-26T01:09:32.635+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T01:09:32.635+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:09:32.635+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:09:32.652+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:09:32.740+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:09:32.740+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:38775"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T01:09:32.886+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T01:09:34.641+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
[GIN] 2025/10/26 - 01:10:43 | 200 |         1m12s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:10:48 | 200 |  3.774424185s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:10:49.655+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T01:10:49.655+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78054031360 required="14.9 GiB"
time=2025-10-26T01:10:50.098+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.8 GiB" free_swap="0 B"
time=2025-10-26T01:10:50.098+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T01:10:50.192+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 43051"
time=2025-10-26T01:10:50.193+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T01:10:50.193+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:10:50.196+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:10:50.206+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T01:10:50.207+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:43051"
time=2025-10-26T01:10:50.293+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-26T01:10:50.448+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:10:50.668+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:10:50.801+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T01:10:50.801+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T01:10:50.801+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T01:10:50.801+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T01:10:50.801+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T01:10:50.811+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T01:10:50.811+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T01:11:00.009+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.82 seconds"
[GIN] 2025/10/26 - 01:11:04 | 200 |   15.3890421s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:11:10 | 200 |  5.684621911s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:11:17 | 200 |  7.014517563s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:11:18 | 200 |  884.578823ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:11:28 | 200 | 10.235741965s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:16:00 | 200 |         1m27s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:16:04 | 200 |  3.649097433s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:16:06 | 200 |   2.29175443s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:16:07 | 200 |  1.400620391s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:16:14 | 200 |  7.073574107s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:23:41.433+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T01:23:41.875+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.0 GiB" free_swap="0 B"
time=2025-10-26T01:23:41.876+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T01:23:42.126+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 43343"
time=2025-10-26T01:23:42.127+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T01:23:42.127+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:23:42.127+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:23:42.142+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:23:42.218+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:23:42.218+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:43343"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T01:23:42.378+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T01:23:48.091+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.96 seconds"
[GIN] 2025/10/26 - 01:24:34 | 200 | 53.094575721s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:24:37 | 200 |  3.453040898s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:24:38.126+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T01:24:38.127+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78089682944 required="14.9 GiB"
time=2025-10-26T01:24:38.324+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T01:24:38.325+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T01:24:38.413+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 45543"
time=2025-10-26T01:24:38.413+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T01:24:38.413+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:24:38.434+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:24:38.426+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T01:24:38.427+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45543"
time=2025-10-26T01:24:38.514+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:24:38.599+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:24:38.685+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T01:24:38.971+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T01:24:38.971+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T01:24:38.971+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T01:24:38.971+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T01:24:38.971+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T01:24:38.983+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T01:24:38.983+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T01:24:43.606+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.19 seconds"
[GIN] 2025/10/26 - 01:24:53 | 200 | 16.082956624s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:25:02 | 200 |  8.468851058s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:25:09 | 200 |  7.111050859s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:25:26 | 200 | 17.427780955s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:25:28 | 200 |  1.829414547s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:33:18.672+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T01:33:18.870+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T01:33:18.871+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T01:33:19.090+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 44025"
time=2025-10-26T01:33:19.091+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T01:33:19.091+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:33:19.091+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:33:19.105+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:33:19.193+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:33:19.193+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:44025"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T01:33:19.342+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T01:33:21.301+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.21 seconds"
[GIN] 2025/10/26 - 01:34:10 | 200 | 51.945773094s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:34:18 | 200 |  8.602690029s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:34:19.355+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.7 GiB"
time=2025-10-26T01:34:19.356+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=78054031360 required="14.9 GiB"
time=2025-10-26T01:34:19.554+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.1 GiB" free_swap="0 B"
time=2025-10-26T01:34:19.555+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T01:34:19.643+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 37183"
time=2025-10-26T01:34:19.644+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T01:34:19.644+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:34:19.664+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:34:19.657+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T01:34:19.657+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37183"
time=2025-10-26T01:34:19.744+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-26T01:34:20.072+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:34:20.089+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:34:20.221+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T01:34:20.221+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T01:34:20.221+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T01:34:20.221+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T01:34:20.221+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T01:34:20.230+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T01:34:20.230+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T01:34:24.880+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.24 seconds"
[GIN] 2025/10/26 - 01:34:33 | 200 | 14.581127729s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:34:44 | 200 |  10.56694381s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:34:45 | 200 |  1.444782873s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:34:52 | 200 |  7.394042056s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:38:50.701+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46210 keep=4 new=4096
[GIN] 2025/10/26 - 01:39:24 | 200 |  33.88433443s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:29 | 200 |  4.549620549s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:32 | 200 |  3.809015815s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:33 | 200 |  677.180736ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:40 | 200 |  6.755521434s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:41 | 200 |   780.59479ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:49 | 200 |  8.603153333s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:39:50 | 200 |  1.127133215s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:50:10.507+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T01:50:10.703+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.0 GiB" free_swap="0 B"
time=2025-10-26T01:50:10.703+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T01:50:10.923+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 41945"
time=2025-10-26T01:50:10.924+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T01:50:10.924+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:50:10.924+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:50:10.939+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:50:11.430+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:50:11.431+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41945"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T01:50:11.677+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T01:50:15.409+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.48 seconds"
time=2025-10-26T01:50:15.444+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46278 keep=4 new=4096
[GIN] 2025/10/26 - 01:50:40 | 200 | 30.884974778s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:50:44 | 200 |  3.702225771s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:50:45.641+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T01:50:45.641+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T01:50:45.840+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.5 GiB" free_swap="0 B"
time=2025-10-26T01:50:45.840+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T01:50:45.930+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 44835"
time=2025-10-26T01:50:45.930+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T01:50:45.930+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:50:45.946+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:50:45.943+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T01:50:45.944+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:44835"
time=2025-10-26T01:50:46.031+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:50:46.128+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:50:46.199+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T01:50:46.260+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T01:50:46.261+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T01:50:46.261+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T01:50:46.261+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T01:50:46.261+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T01:50:46.270+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T01:50:46.270+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T01:50:56.386+11:00 level=INFO source=server.go:637 msg="llama runner started in 10.46 seconds"
[GIN] 2025/10/26 - 01:51:03 | 200 | 19.140405502s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:51:05 | 200 |  1.347172697s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:51:10 | 200 |  5.347654992s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:51:11 | 200 |  1.006203455s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:51:19 | 200 |   7.78154905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 01:51:20 | 200 |  1.247838627s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T01:59:30.660+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T01:59:30.852+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T01:59:30.853+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T01:59:31.078+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 43647"
time=2025-10-26T01:59:31.079+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T01:59:31.079+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T01:59:31.079+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T01:59:31.094+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T01:59:31.180+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T01:59:31.181+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:43647"
time=2025-10-26T01:59:31.537+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T01:59:33.292+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.21 seconds"
time=2025-10-26T01:59:33.328+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46282 keep=4 new=4096
[GIN] 2025/10/26 - 02:08:13 | 200 |         8m43s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:08:17 | 200 |  3.686932272s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:08:17.976+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T02:08:17.976+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T02:08:18.177+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T02:08:18.178+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T02:08:18.269+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 36957"
time=2025-10-26T02:08:18.270+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:08:18.270+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:08:18.284+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:08:18.284+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T02:08:18.285+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:36957"
time=2025-10-26T02:08:18.371+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
time=2025-10-26T02:08:18.535+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:08:26.353+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:08:26.927+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T02:08:26.927+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T02:08:26.927+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T02:08:26.927+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T02:08:26.927+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T02:08:26.936+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T02:08:26.936+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T02:08:31.538+11:00 level=INFO source=server.go:637 msg="llama runner started in 13.27 seconds"
[GIN] 2025/10/26 - 02:09:06 | 200 | 49.453320276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:09 | 200 |  2.693226307s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:19 | 200 |    9.6463384s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:19 | 200 |  882.134673ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:27 | 200 |  7.062740034s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:27 | 200 |  808.796857ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:36 | 200 |   8.93364839s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:09:37 | 200 |  897.417712ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:16:43.513+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T02:16:43.709+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.1 GiB" free_swap="0 B"
time=2025-10-26T02:16:43.709+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T02:16:43.984+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 34037"
time=2025-10-26T02:16:43.984+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T02:16:43.984+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:16:43.985+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:16:43.999+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:16:44.373+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:16:44.374+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34037"
time=2025-10-26T02:16:44.487+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T02:16:48.999+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.01 seconds"
time=2025-10-26T02:16:49.035+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46287 keep=4 new=4096
[GIN] 2025/10/26 - 02:24:43 | 200 |          8m0s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:24:48 | 200 |  4.512908569s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:24:48.882+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T02:24:48.883+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T02:24:49.087+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.4 GiB" free_swap="0 B"
time=2025-10-26T02:24:49.087+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T02:24:49.177+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 45971"
time=2025-10-26T02:24:49.178+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:24:49.178+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:24:49.194+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:24:49.192+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T02:24:49.192+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:45971"
time=2025-10-26T02:24:49.276+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:24:49.380+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:24:49.445+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T02:24:49.514+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T02:24:49.514+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T02:24:49.514+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T02:24:49.514+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T02:24:49.514+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T02:24:49.523+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T02:24:49.523+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T02:24:58.975+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.80 seconds"
[GIN] 2025/10/26 - 02:25:10 | 200 | 21.924170437s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:25:25 | 200 | 15.453101155s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:25:30 | 200 |  4.832401534s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:25:41 | 200 | 11.095929205s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:25:47 | 200 |  5.455291132s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:25:58 | 200 | 11.379341586s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:26:01 | 200 |   3.45244343s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:26:12 | 200 | 10.704118604s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:26:13 | 200 |  1.220084391s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:26:23 | 200 |  9.698974417s |       127.0.0.1 | POST     "/api/generate"
time=2025-10-26T02:30:17.230+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="63.8 GiB"
time=2025-10-26T02:30:17.231+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=68482629632 required="6.1 GiB"
time=2025-10-26T02:30:17.431+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="980.7 GiB" free_swap="0 B"
time=2025-10-26T02:30:17.431+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[63.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T02:30:17.654+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 33515"
time=2025-10-26T02:30:17.655+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:30:17.655+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:30:17.655+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:30:17.669+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:30:17.743+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:30:17.743+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:33515"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65310 MiB free
time=2025-10-26T02:30:17.906+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T02:30:19.661+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
time=2025-10-26T02:30:19.696+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46204 keep=4 new=4096
[GIN] 2025/10/26 - 02:30:37 | 200 | 20.587450476s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:42 | 200 |  5.064718948s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:45 | 200 |  3.327464204s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:47 | 200 |  1.270701363s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:52 | 200 |  5.240246566s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:53 | 200 |  781.624699ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:57 | 200 |  4.490377434s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:30:58 | 200 |  714.093825ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:38:03.332+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T02:38:03.532+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T02:38:03.532+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T02:38:03.753+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 42245"
time=2025-10-26T02:38:03.754+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T02:38:03.754+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:38:03.754+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:38:03.769+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:38:03.867+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:38:03.868+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:42245"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T02:38:04.006+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T02:38:05.760+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
time=2025-10-26T02:38:05.796+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46207 keep=4 new=4096
[GIN] 2025/10/26 - 02:38:21 | 200 | 18.590196226s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:38:25 | 200 |  3.948229875s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:38:25.994+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T02:38:25.995+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T02:38:26.197+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.1 GiB" free_swap="0 B"
time=2025-10-26T02:38:26.197+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T02:38:26.286+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35549"
time=2025-10-26T02:38:26.286+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:38:26.286+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:38:26.307+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:38:26.299+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T02:38:26.300+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35549"
time=2025-10-26T02:38:26.387+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:38:26.485+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:38:26.558+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T02:38:26.624+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T02:38:26.624+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T02:38:26.624+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T02:38:26.624+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T02:38:26.624+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T02:38:26.633+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T02:38:26.633+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T02:38:30.844+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.56 seconds"
[GIN] 2025/10/26 - 02:38:34 | 200 |  9.246542086s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:38:35 | 200 |  788.593613ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:38:38 | 200 |  3.409625763s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:38:43 | 200 |  4.554714751s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:42:00.933+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46211 keep=4 new=4096
[GIN] 2025/10/26 - 02:42:17 | 200 | 16.949462051s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:25 | 200 |  7.352440893s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:28 | 200 |  3.609157228s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:29 | 200 |  936.491433ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:33 | 200 |  3.722849638s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:34 | 200 |  747.950692ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:37 | 200 |  3.744723005s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:42:38 | 200 |  751.845213ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:44:20.681+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46166 keep=4 new=4096
[GIN] 2025/10/26 - 02:45:11 | 200 | 50.875875538s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:45:14 | 200 |  2.897960127s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:45:17 | 200 |  3.389671446s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:45:19 | 200 |   1.90803981s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:45:21 | 200 |  1.979553874s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:45:22 | 200 |  657.632244ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:49:24.727+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46251 keep=4 new=4096
[GIN] 2025/10/26 - 02:49:42 | 200 | 17.385538332s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:49:43 | 200 |  1.741451276s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:49:47 | 200 |  3.837986215s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:49:51 | 200 |   4.08565396s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:49:55 | 200 |  4.218685251s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:49:56 | 200 |  732.048062ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:54:46.043+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="63.9 GiB"
time=2025-10-26T02:54:46.043+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=68606361600 required="6.1 GiB"
time=2025-10-26T02:54:46.246+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="980.9 GiB" free_swap="0 B"
time=2025-10-26T02:54:46.246+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[63.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T02:54:46.467+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 36911"
time=2025-10-26T02:54:46.468+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:54:46.468+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:54:46.468+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:54:46.483+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:54:48.349+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:54:48.350+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:36911"
time=2025-10-26T02:54:48.472+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65428 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T02:54:51.731+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.26 seconds"
time=2025-10-26T02:54:51.768+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46226 keep=4 new=4096
[GIN] 2025/10/26 - 02:55:32 | 200 |  46.90266851s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:55:36 | 200 |  3.233720425s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:55:37.018+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T02:55:37.019+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T02:55:37.218+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T02:55:37.219+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T02:55:37.308+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 46005"
time=2025-10-26T02:55:37.308+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T02:55:37.308+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T02:55:37.329+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T02:55:37.322+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T02:55:37.322+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46005"
time=2025-10-26T02:55:37.432+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T02:55:37.506+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T02:55:37.580+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T02:55:37.638+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T02:55:37.638+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T02:55:37.638+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T02:55:37.638+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T02:55:37.638+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T02:55:37.647+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T02:55:37.647+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T02:55:46.646+11:00 level=INFO source=server.go:637 msg="llama runner started in 9.34 seconds"
[GIN] 2025/10/26 - 02:55:52 | 200 | 15.815805084s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:55:53 | 200 |  864.055638ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:56:02 | 200 |  9.363271901s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:56:03 | 200 |  905.880753ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:56:07 | 200 |  4.193525337s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:56:08 | 200 |  754.141699ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T02:57:04.070+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46164 keep=4 new=4096
[GIN] 2025/10/26 - 02:57:20 | 200 | 16.504327572s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:57:23 | 200 |  3.110765245s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:57:25 | 200 |  1.888458209s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:57:27 | 200 |  1.955090798s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 02:57:29 | 200 |   2.34885925s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:01:49.921+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46213 keep=4 new=4096
[GIN] 2025/10/26 - 03:02:09 | 200 | 19.371996498s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:02:14 | 200 |  5.315183849s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:02:19 | 200 |  5.337768774s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:02:24 | 200 |  4.823338724s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:02:25 | 200 |  794.202094ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:02:30 | 200 |  4.614385475s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:12:31.465+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T03:12:31.662+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.7 GiB" free_swap="0 B"
time=2025-10-26T03:12:31.663+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T03:12:31.906+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 41807"
time=2025-10-26T03:12:31.907+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T03:12:31.907+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:12:31.907+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:12:31.923+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:12:32.001+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:12:32.001+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41807"
time=2025-10-26T03:12:32.159+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T03:12:33.913+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
time=2025-10-26T03:12:33.969+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46242 keep=4 new=4096
[GIN] 2025/10/26 - 03:12:51 | 200 | 20.144515225s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:12:54 | 200 |  3.175896002s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:12:54.940+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T03:12:54.941+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T03:12:55.142+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.3 GiB" free_swap="0 B"
time=2025-10-26T03:12:55.142+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T03:12:55.231+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 38005"
time=2025-10-26T03:12:55.231+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:12:55.231+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:12:55.252+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:12:55.245+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T03:12:55.245+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38005"
time=2025-10-26T03:12:55.331+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:12:55.405+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:12:55.504+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T03:12:55.541+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T03:12:55.541+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T03:12:55.541+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T03:12:55.541+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T03:12:55.541+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T03:12:55.550+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T03:12:55.550+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T03:12:59.772+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.54 seconds"
[GIN] 2025/10/26 - 03:13:05 | 200 | 11.393718638s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:13:07 | 200 |   1.44396721s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:13:14 | 200 |  7.076500979s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:13:15 | 200 |  858.596025ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:13:23 | 200 |  8.332473687s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:13:25 | 200 |  1.413366453s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:18:34.680+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T03:18:34.874+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T03:18:34.875+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T03:18:35.096+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 41503"
time=2025-10-26T03:18:35.096+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T03:18:35.096+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:18:35.096+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:18:35.118+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:18:35.205+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:18:35.206+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41503"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T03:18:35.347+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T03:18:37.102+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
time=2025-10-26T03:18:37.137+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46227 keep=4 new=4096
[GIN] 2025/10/26 - 03:19:03 | 200 | 29.378943548s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:05 | 200 |  2.158432149s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:19:06.374+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T03:19:06.374+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T03:19:06.577+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T03:19:06.577+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T03:19:06.682+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 39023"
time=2025-10-26T03:19:06.683+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:19:06.683+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:19:06.694+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:19:06.696+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T03:19:06.696+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:39023"
time=2025-10-26T03:19:06.788+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:19:06.863+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:19:06.945+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T03:19:06.997+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T03:19:06.997+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T03:19:06.997+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T03:19:06.997+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T03:19:06.997+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T03:19:07.006+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T03:19:07.006+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T03:19:11.209+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.53 seconds"
[GIN] 2025/10/26 - 03:19:18 | 200 | 12.433037553s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:19 | 200 |  900.871758ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:24 | 200 |  5.004606114s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:25 | 200 |   721.52172ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:30 | 200 |  5.036456657s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:19:31 | 200 |  1.543881717s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:24:02.633+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46268 keep=4 new=4096
[GIN] 2025/10/26 - 03:25:02 | 200 | 59.880961768s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:09 | 200 |   6.86555091s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:25:09.696+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T03:25:09.697+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T03:25:09.898+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T03:25:09.899+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T03:25:09.988+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 34133"
time=2025-10-26T03:25:09.988+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:25:09.988+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:25:10.009+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:25:10.003+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T03:25:10.003+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34133"
time=2025-10-26T03:25:10.089+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
time=2025-10-26T03:25:10.260+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:25:10.406+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:25:10.547+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T03:25:10.547+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T03:25:10.547+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T03:25:10.547+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T03:25:10.547+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T03:25:10.556+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T03:25:10.556+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T03:25:18.317+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.33 seconds"
[GIN] 2025/10/26 - 03:25:24 | 200 | 15.163054905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:25 | 200 |  940.447673ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:31 | 200 |  6.080863292s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:32 | 200 |  841.875475ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:39 | 200 |  7.021976768s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:25:40 | 200 |  854.056981ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:28:25.952+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46206 keep=4 new=4096
[GIN] 2025/10/26 - 03:28:42 | 200 | 16.682932319s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:45 | 200 |  2.942711922s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:48 | 200 |  3.110732004s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:49 | 200 |  1.269274929s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:52 | 200 |  2.825527074s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:53 | 200 |  741.254173ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:57 | 200 |  3.576961179s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:28:57 | 200 |  759.634365ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:32:52.971+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46201 keep=4 new=4096
[GIN] 2025/10/26 - 03:33:19 | 200 | 27.100521957s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:24 | 200 |  4.057207245s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:28 | 200 |  4.647339326s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:29 | 200 |  797.282534ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:32 | 200 |  2.900072932s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:33 | 200 |  713.392562ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:37 | 200 |  4.593473682s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:33:38 | 200 |  769.976641ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:34:33.240+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46173 keep=4 new=4096
[GIN] 2025/10/26 - 03:34:53 | 200 | 20.008596789s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:34:58 | 200 |  4.934304179s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:00 | 200 |  2.609915216s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:01 | 200 |   799.76833ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:04 | 200 |  3.021270676s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:05 | 200 |   773.95391ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:08 | 200 |  3.029455971s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:35:09 | 200 |  713.792485ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:47:23.639+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T03:47:23.840+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.9 GiB" free_swap="0 B"
time=2025-10-26T03:47:23.840+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T03:47:24.077+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 36085"
time=2025-10-26T03:47:24.078+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T03:47:24.078+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:47:24.078+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:47:24.094+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:47:24.190+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:47:24.190+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:36085"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
time=2025-10-26T03:47:24.329+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T03:47:29.090+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.01 seconds"
time=2025-10-26T03:47:29.126+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46233 keep=4 new=4096
[GIN] 2025/10/26 - 03:47:40 | 200 | 16.862185763s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:47:44 | 200 |   4.23682987s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:47:44.883+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T03:47:44.884+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T03:47:45.087+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T03:47:45.088+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T03:47:45.176+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 34111"
time=2025-10-26T03:47:45.176+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:47:45.176+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:47:45.198+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:47:45.190+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T03:47:45.190+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:34111"
time=2025-10-26T03:47:45.277+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:47:45.350+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:47:45.449+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T03:47:45.485+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T03:47:45.485+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T03:47:45.485+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T03:47:45.485+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T03:47:45.485+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T03:47:45.494+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T03:47:45.494+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T03:47:49.731+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.55 seconds"
[GIN] 2025/10/26 - 03:47:57 | 200 | 12.631702025s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:48:00 | 200 |  3.544824507s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:48:02 | 200 |  1.451866684s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:48:05 | 200 |  3.913858661s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:52:58.094+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="63.9 GiB"
time=2025-10-26T03:52:58.094+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=68627333120 required="6.1 GiB"
time=2025-10-26T03:52:58.297+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="980.2 GiB" free_swap="0 B"
time=2025-10-26T03:52:58.298+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[63.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T03:52:58.518+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 40303"
time=2025-10-26T03:52:58.519+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:52:58.519+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:52:58.519+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:52:58.534+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:52:58.631+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:52:58.632+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:40303"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 65448 MiB free
time=2025-10-26T03:52:58.771+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T03:53:00.526+11:00 level=INFO source=server.go:637 msg="llama runner started in 2.01 seconds"
time=2025-10-26T03:53:00.562+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46247 keep=4 new=4096
[GIN] 2025/10/26 - 03:53:27 | 200 | 29.479822149s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:53:36 | 200 |  9.001230411s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T03:53:36.718+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T03:53:36.718+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T03:53:36.920+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.2 GiB" free_swap="0 B"
time=2025-10-26T03:53:36.921+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T03:53:37.012+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 40547"
time=2025-10-26T03:53:37.013+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T03:53:37.013+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T03:53:37.013+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T03:53:37.026+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T03:53:37.027+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:40547"
time=2025-10-26T03:53:37.114+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T03:53:37.211+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T03:53:37.267+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T03:53:37.344+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T03:53:37.344+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T03:53:37.344+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T03:53:37.344+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T03:53:37.344+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T03:53:37.353+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T03:53:37.354+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T03:53:41.565+11:00 level=INFO source=server.go:637 msg="llama runner started in 4.55 seconds"
[GIN] 2025/10/26 - 03:53:47 | 200 | 10.758960971s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:53:53 | 200 |  6.507695177s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:53:54 | 200 |  908.422514ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:53:59 | 200 |  5.542254052s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 03:54:00 | 200 |  849.892025ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:00:14.563+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T04:00:14.758+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.8 GiB" free_swap="0 B"
time=2025-10-26T04:00:14.758+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T04:00:14.979+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 34431"
time=2025-10-26T04:00:14.980+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T04:00:14.980+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T04:00:14.980+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T04:00:15.014+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T04:00:15.612+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T04:00:15.613+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:34431"
time=2025-10-26T04:00:15.734+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T04:00:20.245+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.27 seconds"
time=2025-10-26T04:00:20.282+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46241 keep=4 new=4096
[GIN] 2025/10/26 - 04:00:41 | 200 | 26.915689604s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:00:44 | 200 |  2.483029705s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:00:44.887+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T04:00:44.887+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T04:00:45.089+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.6 GiB" free_swap="0 B"
time=2025-10-26T04:00:45.090+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T04:00:45.173+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 46591"
time=2025-10-26T04:00:45.173+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T04:00:45.173+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T04:00:45.185+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T04:00:45.187+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T04:00:45.188+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:46591"
time=2025-10-26T04:00:45.272+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T04:00:45.371+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T04:00:45.436+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T04:00:45.502+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T04:00:45.502+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T04:00:45.502+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T04:00:45.502+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T04:00:45.502+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T04:00:45.512+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T04:00:45.512+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T04:00:55.237+11:00 level=INFO source=server.go:637 msg="llama runner started in 10.06 seconds"
[GIN] 2025/10/26 - 04:01:02 | 200 | 18.385765258s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:01:07 | 200 |  5.170324661s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:01:08 | 200 |  854.513019ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:01:16 | 200 |  7.888616331s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:01:17 | 200 |  805.701501ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:04:49.483+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46229 keep=4 new=4096
[GIN] 2025/10/26 - 04:05:09 | 200 | 19.984816067s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:12 | 200 |  2.995815364s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:16 | 200 |  3.776132935s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:19 | 200 |  3.691702382s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:20 | 200 |  738.832527ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:25 | 200 |  5.258984756s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:05:26 | 200 |  736.657095ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:07:27.884+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46170 keep=4 new=4096
[GIN] 2025/10/26 - 04:07:59 | 200 | 31.952348787s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:08:04 | 200 |  4.511183339s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:08:08 | 200 |  3.764393705s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:08:11 | 200 |  3.071151648s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:08:14 | 200 |  3.711601086s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:08:15 | 200 |  725.776692ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:09:53.355+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46179 keep=4 new=4096
[GIN] 2025/10/26 - 04:10:08 | 200 | 15.208183498s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:10:11 | 200 |  3.386267957s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:10:15 | 200 |  3.171013547s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:10:17 | 200 |  2.687910364s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:10:21 | 200 |  3.337402653s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:10:21 | 200 |  710.718531ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:10:58.630+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46159 keep=4 new=4096
[GIN] 2025/10/26 - 04:11:23 | 200 | 25.082551723s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:26 | 200 |  2.444310165s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:27 | 200 |  1.753343312s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:29 | 200 |  1.788183197s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:30 | 200 |  707.625859ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:32 | 200 |  1.712410292s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:11:33 | 200 |  1.189870674s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:11:37.370+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46146 keep=4 new=4096
[GIN] 2025/10/26 - 04:12:01 | 200 |  24.00284714s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:04 | 200 |  3.271152696s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:06 | 200 |  1.859224158s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:08 | 200 |  2.349491622s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:09 | 200 |   701.24925ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:11 | 200 |  2.196165668s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:12:12 | 200 |  668.092576ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:16:00.070+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46205 keep=4 new=4096
[GIN] 2025/10/26 - 04:16:27 | 200 | 27.092952367s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:28 | 200 |  1.534890879s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:32 | 200 |  3.395966361s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:35 | 200 |   3.24311139s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:36 | 200 |  759.112323ms |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:40 | 200 |  4.488694703s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:16:41 | 200 |  734.386528ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:16:56.214+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46149 keep=4 new=4096
[GIN] 2025/10/26 - 04:17:39 | 200 | 43.432666279s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:17:44 | 200 |  5.130446423s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:17:47 | 200 |  2.463165662s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:17:48 | 200 |  1.632906222s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:17:52 | 200 |  3.598396533s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:17:53 | 200 |   743.78845ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:22:15.236+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46260 keep=4 new=4096
[GIN] 2025/10/26 - 04:22:42 | 200 | 27.340777905s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:22:46 | 200 |  3.833933769s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:22:52 | 200 |  6.039446524s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:23:05 | 200 | 13.600968084s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:23:07 | 200 |  1.415786816s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:23:16 | 200 |  9.603763295s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:23:17 | 200 |  939.605486ms |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:30:45.350+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=84544258048 required="6.1 GiB"
time=2025-10-26T04:30:45.577+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="983.1 GiB" free_swap="0 B"
time=2025-10-26T04:30:45.578+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=37 layers.split="" memory.available="[78.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.1 GiB" memory.required.partial="6.1 GiB" memory.required.kv="576.0 MiB" memory.required.allocations="[6.1 GiB]" memory.weights.total="4.5 GiB" memory.weights.repeating="4.1 GiB" memory.weights.nonrepeating="486.9 MiB" memory.graph.full="384.0 MiB" memory.graph.partial="384.0 MiB"
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-26T04:30:45.813+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 4096 --batch-size 512 --n-gpu-layers 37 --threads 56 --parallel 1 --port 41821"
time=2025-10-26T04:30:45.814+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-10-26T04:30:45.814+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T04:30:45.814+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T04:30:45.830+11:00 level=INFO source=runner.go:815 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T04:30:46.301+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T04:30:46.301+11:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:41821"
time=2025-10-26T04:30:46.316+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100 80GB PCIe) - 80627 MiB free
llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                            general.license str              = apache-2.0
llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36
llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096
llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288
llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32
llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  145 tensors
llama_model_loader: - type  f16:   36 tensors
llama_model_loader: - type q4_K:  199 tensors
llama_model_loader: - type q6_K:   19 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.86 GiB (5.10 BPW) 
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 4096
print_info: n_layer          = 36
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 12288
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.19 B
print_info: general.name     = Qwen3 8B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 36 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 37/37 layers to GPU
load_tensors:        CUDA0 model buffer size =  4643.78 MiB
load_tensors:   CPU_Mapped model buffer size =   333.84 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.60 MiB
llama_kv_cache_unified: kv_size = 4096, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB
llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB
llama_context:      CUDA0 compute buffer size =   304.75 MiB
llama_context:  CUDA_Host compute buffer size =    16.01 MiB
llama_context: graph nodes  = 1374
llama_context: graph splits = 2
time=2025-10-26T04:30:51.580+11:00 level=INFO source=server.go:637 msg="llama runner started in 5.77 seconds"
time=2025-10-26T04:30:51.641+11:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=46227 keep=4 new=4096
[GIN] 2025/10/26 - 04:31:10 | 200 | 25.273091653s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:31:12 | 200 |   2.65279596s |       127.0.0.1 | POST     "/v1/chat/completions"
time=2025-10-26T04:31:13.362+11:00 level=INFO source=sched.go:546 msg="updated VRAM based on existing loaded models" gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 library=cuda total="79.2 GiB" available="72.6 GiB"
time=2025-10-26T04:31:13.362+11:00 level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-089f9e1c-b8e7-fc6f-3c13-cca7d8becc22 parallel=1 available=77989019648 required="14.9 GiB"
time=2025-10-26T04:31:13.563+11:00 level=INFO source=server.go:135 msg="system memory" total="1006.8 GiB" free="982.4 GiB" free_swap="0 B"
time=2025-10-26T04:31:13.563+11:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[72.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-10-26T04:31:13.654+11:00 level=INFO source=server.go:438 msg="starting llama server" cmd="/fs04/scratch2/lb64/ollama-bin/bin/ollama runner --ollama-engine --model /home/mvo1/lb64_scratch/ollama-models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 56 --parallel 1 --port 35971"
time=2025-10-26T04:31:13.654+11:00 level=INFO source=sched.go:481 msg="loaded runners" count=2
time=2025-10-26T04:31:13.654+11:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-10-26T04:31:13.671+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-10-26T04:31:13.668+11:00 level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-10-26T04:31:13.668+11:00 level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35971"
time=2025-10-26T04:31:13.757+11:00 level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes
load_backend: loaded CUDA backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /fs04/scratch2/lb64/ollama-bin/lib/ollama/libggml-cpu-icelake.so
time=2025-10-26T04:31:13.856+11:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-10-26T04:31:13.922+11:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-26T04:31:13.990+11:00 level=INFO source=ggml.go:365 msg="offloading 24 repeating layers to GPU"
time=2025-10-26T04:31:13.990+11:00 level=INFO source=ggml.go:371 msg="offloading output layer to GPU"
time=2025-10-26T04:31:13.990+11:00 level=INFO source=ggml.go:376 msg="offloaded 25/25 layers to GPU"
time=2025-10-26T04:31:13.990+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-10-26T04:31:13.990+11:00 level=INFO source=ggml.go:379 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-10-26T04:31:14.000+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-10-26T04:31:14.000+11:00 level=INFO source=ggml.go:668 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-10-26T04:31:21.708+11:00 level=INFO source=server.go:637 msg="llama runner started in 8.05 seconds"
[GIN] 2025/10/26 - 04:31:25 | 200 | 13.047608562s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:31:29 | 200 |  3.624833883s |       127.0.0.1 | POST     "/v1/chat/completions"
[GIN] 2025/10/26 - 04:31:33 | 200 |  3.619628147s |       127.0.0.1 | POST     "/v1/chat/completions"
[Raw Model Testing] Response time: 30.02s, Quiz time: 20.33s, Total: 50.35s
[Test] Logged: numerical_test - probability - Q30
Test completed: 30 new questions run, 0 skipped
✅ probability_test completed successfully on attempt 1

Completed all tests for Net_60
Completed all tests!
[STOP] Stopping ollama PID 3011547
[INFO] Sleeping 4h
