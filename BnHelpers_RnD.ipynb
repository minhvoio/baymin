{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4d4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Netica\n"
     ]
    }
   ],
   "source": [
    "bn_path = \"./nets/collection/\"\n",
    "from bni_netica.bni_netica import *\n",
    "from bni_netica.bni_netica import Net\n",
    "from bni_netica.bni_utils import findAllDConnectedNodes\n",
    "\n",
    "CancerNeapolitanNet = Net(bn_path+\"Cancer Neapolitan.neta\")\n",
    "ChestClinicNet = Net(bn_path+\"ChestClinic.neta\")\n",
    "ClassifierNet = Net(bn_path+\"Classifier.neta\")\n",
    "CoronaryRiskNet = Net(bn_path+\"Coronary Risk.neta\")\n",
    "FireNet = Net(bn_path+\"Fire.neta\")\n",
    "MendelGeneticsNet = Net(bn_path+\"Mendel Genetics.neta\")\n",
    "RatsNet = Net(bn_path+\"Rats.neta\")\n",
    "WetGrassNet = Net(bn_path+\"Wet Grass.neta\")\n",
    "RatsNoisyOr = Net(bn_path+\"Rats_NoisyOr.dne\")\n",
    "Derm = Net(bn_path+\"Derm 7.9 A.dne\")\n",
    "CauseEffectNet = Net(\"./nets/outputs/common_cause_effect.neta\")\n",
    "netDir = \"./nets/\"\n",
    "myNet = Net(netDir+\"NF_V1.dne\")\n",
    "listOfNets = [CancerNeapolitanNet, ChestClinicNet, ClassifierNet, CoronaryRiskNet, FireNet, MendelGeneticsNet, RatsNet, WetGrassNet, RatsNoisyOr, Derm]\n",
    "\n",
    "def printNet(net):\n",
    "    for node in net.nodes():\n",
    "        print(f\"{node.name()} -> {[child.name() for child in node.children()]}\")\n",
    "\n",
    "def is_connected(net, node1, node2):\n",
    "  relatedNodes = net.node(node1).getRelated(\"d_connected\")\n",
    "  return any(n.name() == node2 for n in relatedNodes)\n",
    "\n",
    "def stateIdx(node, spec):\n",
    "    if isinstance(spec, int):\n",
    "        return spec\n",
    "    st = node.state(spec)  # by state NAME\n",
    "    if st is None:\n",
    "        st = node.stateByTitle(spec)  # fallback by title\n",
    "    if st is None:\n",
    "        raise ValueError(f\"Unknown state '{spec}' for node '{node.name()}'\")\n",
    "    return st.stateNum\n",
    "\n",
    "def get_BN_node_states(net):\n",
    "    structure = \"\"\n",
    "    for node in net.nodes():\n",
    "        states = [s.name() for s in node.states()]\n",
    "        structure += f\"{node.name()} {states}\\n\"\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ded0f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aaf30b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_BN_node_states(myNet): Rainfall ['Below_average', 'Average', 'Above_average']\n",
      "Drought ['Yes', 'No']\n",
      "TreeCond ['Good', 'Damaged', 'Dead']\n",
      "PesticideUse ['High', 'Low']\n",
      "PesticideInRiver ['High', 'Low']\n",
      "RiverFlow ['Good', 'Poor']\n",
      "FishAbundance ['High', 'Medium', 'Low']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('get_BN_node_states(myNet):', get_BN_node_states(myNet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f3153",
   "metadata": {},
   "source": [
    "# Common Cause & Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b07c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _names(nodes):\n",
    "    return {n.name() for n in nodes}\n",
    "\n",
    "def ancestors(net, node):\n",
    "    return _names(net.node(node).getRelated(\"ancestors,exclude_self\"))\n",
    "\n",
    "def descendants(net, node):\n",
    "    return _names(net.node(node).getRelated(\"descendents,exclude_self\"))\n",
    "\n",
    "def get_common_cause(net, node1, node2):\n",
    "    return ancestors(net, node1) & ancestors(net, node2)\n",
    "\n",
    "def get_common_effect(net, node1, node2):\n",
    "    return descendants(net, node1) & descendants(net, node2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ba135",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f233e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Causes of X and E: {'Z'}\n",
      "Common Effects of B and E: {'C'}\n",
      "Common Causes of A and E: {'Z'}\n",
      "Common Effects of A and E: {'C'}\n"
     ]
    }
   ],
   "source": [
    "# net\n",
    "# A → B → C ← D ← E\n",
    "# A ← X ← Z → Y → E\n",
    "\n",
    "print('Common Causes of X and E:', get_common_cause(CauseEffectNet, \"X\", \"E\"))\n",
    "print('Common Effects of B and E:', get_common_effect(CauseEffectNet, \"B\", \"E\"))\n",
    "\n",
    "print('Common Causes of A and E:', get_common_cause(CauseEffectNet, \"A\", \"E\"))\n",
    "print('Common Effects of A and E:', get_common_effect(CauseEffectNet, \"A\", \"E\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4b933",
   "metadata": {},
   "source": [
    "# Check for change of dependancy of X, Y after observing Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761abbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager # release resource automatically after using it\n",
    "def _temporarily_set_findings(net, findings_dict):\n",
    "    \"\"\"findings_dict: {node_name: state_name_or_index}\"\"\"\n",
    "    saved = net.findings()  # current evidence (indices)\n",
    "    try:\n",
    "        # Apply new evidence\n",
    "        for k, v in findings_dict.items():\n",
    "            node = net.node(k)\n",
    "            if isinstance(v, int):\n",
    "                node.finding(v)\n",
    "            else:\n",
    "                node.finding(v)  # state(name)\n",
    "        net.update()\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore previous evidence\n",
    "        net.retractFindings()\n",
    "        for k, v in (saved or {}).items():\n",
    "            net.node(k).finding(v)\n",
    "        net.update()\n",
    "\n",
    "def does_Z_change_dependency(net, X, Y, Z):\n",
    "    \"\"\"\n",
    "    Returns (changed: bool, details: {'before': bool, 'after': bool})\n",
    "    where True means 'd-connected' (dependent), False means 'd-separated' (independent).\n",
    "    \"\"\"\n",
    "    zname = net.node(Z).name()\n",
    "    saved = net.findings()  # keep current evidence E to restore later\n",
    "    \n",
    "    try:\n",
    "        # ---- BEFORE: dependency without Z ----\n",
    "        if saved and zname in saved:\n",
    "            # ensure Z is not observed for the 'before' check\n",
    "            net.node(zname).retractFindings()\n",
    "            net.update()\n",
    "        dep_before = is_connected(net, X, Y)\n",
    "\n",
    "        # ---- AFTER: dependency under Z observed ----\n",
    "        with _temporarily_set_findings(net, {zname: 0}): # using state index 0 for Z\n",
    "            dep_after = is_connected(net, X, Y)\n",
    "\n",
    "        return (dep_before != dep_after), {\"before\": dep_before, \"after\": dep_after}\n",
    "    finally:\n",
    "        # Restore original evidence exactly as it was\n",
    "        net.retractFindings()\n",
    "        if saved:\n",
    "            net.findings(saved)\n",
    "        net.update()\n",
    "\n",
    "def evidences_block_XY(net, X, Y):\n",
    "    ans = []\n",
    "    evidences = findAllDConnectedNodes(net, X, Y)\n",
    "    # print('evidences:', [e.name() for e in evidences])\n",
    "    for e in evidences:\n",
    "        e_name = e.name()\n",
    "        if e_name != X and e_name != Y and does_Z_change_dependency(net, X, Y, e_name)[0]:\n",
    "            ans.append(e_name)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa790590",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d39ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net:  NativeFishV1\n",
      "Rainfall -> ['TreeCond', 'PesticideInRiver', 'RiverFlow']\n",
      "Drought -> ['TreeCond', 'RiverFlow']\n",
      "TreeCond -> []\n",
      "PesticideUse -> ['PesticideInRiver']\n",
      "PesticideInRiver -> ['FishAbundance']\n",
      "RiverFlow -> ['FishAbundance']\n",
      "FishAbundance -> []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Net: ', myNet.name())\n",
    "printNet(myNet)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d6fa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net:  NativeFishV1\n",
      "Rainfall -> ['TreeCond', 'PesticideInRiver', 'RiverFlow']\n",
      "Drought -> ['TreeCond', 'RiverFlow']\n",
      "TreeCond -> []\n",
      "PesticideUse -> ['PesticideInRiver']\n",
      "PesticideInRiver -> ['FishAbundance']\n",
      "RiverFlow -> ['FishAbundance']\n",
      "FishAbundance -> []\n",
      "\n",
      "Findings on PesticideInRiver: ['High', 'Low']\n",
      "\n",
      "['Rainfall']\n",
      "(True, {'before': False, 'after': True})\n"
     ]
    }
   ],
   "source": [
    "print('Net: ', myNet.name())\n",
    "printNet(myNet)\n",
    "print()\n",
    "\n",
    "findingsPest = myNet.node(\"PesticideInRiver\").stateNames()\n",
    "print('Findings on PesticideInRiver:', findingsPest)\n",
    "\n",
    "print()\n",
    "\n",
    "print(evidences_block_XY(myNet, \"RiverFlow\", \"PesticideInRiver\"))\n",
    "# print(does_Z_affect_XY(myNet, X=\"Rainfall\", Y=\"RiverFlow\", Z=\"FishAbundance\"))\n",
    "\n",
    "# print('does_Z_change_dependency:', end=' ')\n",
    "print(does_Z_change_dependency(myNet, X=\"Rainfall\", Y=\"Drought\", Z=\"TreeCond\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235be21",
   "metadata": {},
   "source": [
    "# Prob of X when observe Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc66cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _output_distribution(beliefs, node):\n",
    "    \"\"\"Pretty print a distribution (list of probabilities) with state names.\"\"\"\n",
    "    output = \"\"\n",
    "    for i, p in enumerate(beliefs):\n",
    "        state = node.state(i)\n",
    "        output += f\"  P({node.name()}={state.name()}) = {p:.4f}\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def prob_X_given_Y(net, X=None, Y=None, y_state=\"Yes\"):\n",
    "    \"\"\"\n",
    "    Returns P(X | Y = y_state), net_after_observation\n",
    "    y_state can be state names (str) or indices (int).\n",
    "    \"\"\"\n",
    "    \n",
    "    with _temporarily_set_findings(net, {Y: y_state}):\n",
    "        node_X = net.node(X)\n",
    "        # beliefs() is P(X | current findings)\n",
    "        return node_X.beliefs(), net\n",
    "    \n",
    "def prob_X_given_YZ(net, X=None, Y=None, y_state=\"Yes\", Z=None, z_state=\"Yes\"):\n",
    "    \"\"\"\n",
    "    Returns P(X = x_state | Y = y_state, Z = z_state), net_after_observation\n",
    "    x_state, y_state and z_state can be state names (str) or indices (int).\n",
    "    \"\"\"\n",
    "    \n",
    "    with _temporarily_set_findings(net, {Y: y_state, Z: z_state}):\n",
    "        node_X = net.node(X)\n",
    "        # beliefs() is P(X | current findings)\n",
    "        return node_X.beliefs(), net\n",
    "    \n",
    "def get_prob_X_given_Y(net, X=None, Y=None, y_state=\"Yes\"):\n",
    "    \"\"\"\n",
    "    Returns string output of prob_X_given_Y\n",
    "    \"\"\"\n",
    "    beliefs, net_after = prob_X_given_Y(net, X, Y, y_state)\n",
    "    output = f\"P({X} | {Y}={y_state}):\\n\"\n",
    "    output += _output_distribution(beliefs, net_after.node(X))\n",
    "    return output, net_after\n",
    "\n",
    "def get_prob_X_given_YZ(net, X=None, Y=None, y_state=\"Yes\", Z=None, z_state=\"Yes\"):\n",
    "    \"\"\"\n",
    "    Returns string output of prob_X_given_YZ\n",
    "    \"\"\"\n",
    "    beliefs, net_after = prob_X_given_YZ(net, X, Y, y_state, Z, z_state)\n",
    "    output = f\"P({X} | {Y}={y_state}, {Z}={z_state}):\\n\"\n",
    "    output += _output_distribution(beliefs, net_after.node(X))\n",
    "    return output, net_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07135f5",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f5a4ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Rainfall | TreeCond=Good):\n",
      "  P(Rainfall=Below_average) = 0.0857\n",
      "  P(Rainfall=Average) = 0.6909\n",
      "  P(Rainfall=Above_average) = 0.2235\n",
      "\n",
      "P(Rainfall | Drought=Yes, TreeCond=Good):\n",
      "  P(Rainfall=Below_average) = 0.0784\n",
      "  P(Rainfall=Average) = 0.6863\n",
      "  P(Rainfall=Above_average) = 0.2353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out, _ = get_prob_X_given_Y(myNet, X=\"Rainfall\", Y=\"TreeCond\", y_state=\"Good\")\n",
    "print(out)\n",
    "\n",
    "out2, _ = get_prob_X_given_YZ(myNet, X=\"Rainfall\", Y=\"Drought\", y_state=\"Yes\", Z=\"TreeCond\", z_state=\"Good\")\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e649b",
   "metadata": {},
   "source": [
    "# Detect Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e09c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical OR → best: Logical OR rmse: 0.0\n",
      "Noisy OR → best: Noisy OR rmse: 0.05\n",
      "Logical AND → best: Logical AND rmse: 0.0\n",
      "Noisy AND (toy) → best: Noisy OR rmse: 0.05\n",
      "Logical XOR → best: Logical XOR rmse: 0.0\n",
      "Logical XNOR → best: Logical XNOR rmse: 0.0\n",
      "Additive → best: Noisy OR rmse: 0.05\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import product\n",
    "\n",
    "# helpers\n",
    "def _to01(x):\n",
    "    if isinstance(x, str):\n",
    "        return 1 if x.lower().startswith('t') else 0\n",
    "    return int(bool(x))\n",
    "\n",
    "def _rmse(obs, pred):\n",
    "    return math.sqrt(sum((obs[k]-pred[k])**2 for k in obs)/len(obs))\n",
    "\n",
    "def _clip01(x): \n",
    "    return 0.0 if x < 0 else 1.0 if x > 1 else float(x)\n",
    "\n",
    "def _ensure_keys(d):\n",
    "    # normalize keys to (a,b) with 0/1\n",
    "    norm = {}\n",
    "    for k,v in d.items():\n",
    "        a,b = k\n",
    "        norm[(_to01(a), _to01(b))] = float(v)\n",
    "    # fill missing (shouldn't happen)\n",
    "    for a,b in product([0,1],[0,1]):\n",
    "        norm.setdefault((a,b), 0.0)\n",
    "    return norm\n",
    "\n",
    "# prototypes (deterministic)\n",
    "def _logical_or():\n",
    "    return {(1,1):1.0,(1,0):1.0,(0,1):1.0,(0,0):0.0}\n",
    "\n",
    "def _logical_and():\n",
    "    return {(1,1):1.0,(1,0):0.0,(0,1):0.0,(0,0):0.0}\n",
    "\n",
    "def _logical_xor():\n",
    "    # 1 iff exactly one true\n",
    "    return {(1,1):0.0,(1,0):1.0,(0,1):1.0,(0,0):0.0}\n",
    "\n",
    "def _logical_xnor():\n",
    "    # 1 iff both equal (often mislabeled \"XOR\" in sheets)\n",
    "    return {(1,1):1.0,(1,0):0.0,(0,1):0.0,(0,0):1.0}\n",
    "\n",
    "# parametric: Noisy-OR (with leak)\n",
    "# P11 = 1 - (1-l)(1-pA)(1-pB);  P10 = 1-(1-l)(1-pA); P01 = 1-(1-l)(1-pB); P00=l\n",
    "def _fit_noisy_or(obs):\n",
    "    l = obs[(0,0)]\n",
    "    denom = 1 - l\n",
    "    if denom <= 0:  # degenerate: l>=1 -> always 1\n",
    "        pred = {(a,b): 1.0 for a,b in obs}\n",
    "        params = {\"leak\": l, \"pA\": 1.0, \"pB\": 1.0}\n",
    "        return _rmse(obs, pred), pred, params\n",
    "    pA = 1.0 - (1.0 - obs[(1,0)]) / denom\n",
    "    pB = 1.0 - (1.0 - obs[(0,1)]) / denom\n",
    "    # constrain to [0,1] so we don't blow up badly\n",
    "    pA, pB = _clip01(pA), _clip01(pB)\n",
    "    def f(a,b):\n",
    "        return 1.0 - (1.0 - l) * (1.0 - pA)**a * (1.0 - pB)**b\n",
    "    pred = {(a,b): f(a,b) for a,b in obs}\n",
    "    return _rmse(obs, pred), pred, {\"leak\": l, \"pA\": pA, \"pB\": pB}\n",
    "\n",
    "# parametric: \"Noisy-AND\" (simple leaky multiplicative)\n",
    "# P(a,b) = l * (qA^a) * (qB^b); so P00=l, P10=l*qA, P01=l*qB, P11=l*qA*qB\n",
    "def _fit_noisy_and(obs):\n",
    "    l = obs[(0,0)]\n",
    "    if l <= 0:\n",
    "        # degenerate: l==0 -> model predicts zeros except maybe 0/0 division\n",
    "        pred = {(a,b): 0.0 for a,b in obs}\n",
    "        return _rmse(obs, pred), pred, {\"leak\": l, \"qA\": 0.0, \"qB\": 0.0}\n",
    "    qA = obs[(1,0)] / l\n",
    "    qB = obs[(0,1)] / l\n",
    "    qA, qB = _clip01(qA), _clip01(qB)\n",
    "    def f(a,b):\n",
    "        return l * (qA**a) * (qB**b)\n",
    "    pred = {(a,b): f(a,b) for a,b in obs}\n",
    "    return _rmse(obs, pred), pred, {\"leak\": l, \"qA\": qA, \"qB\": qB}\n",
    "\n",
    "# parametric: Additive (linear, clipped)\n",
    "# P(a,b) = clip(bias + wA*a + wB*b)\n",
    "def _fit_additive(obs):\n",
    "    # Solve least squares for [bias, wA, wB]\n",
    "    # X rows: (1, a, b)\n",
    "    import numpy as np\n",
    "    X = []\n",
    "    y = []\n",
    "    ordering = [(0,0),(1,0),(0,1),(1,1)]\n",
    "    for a,b in ordering:\n",
    "        X.append([1.0, float(a), float(b)])\n",
    "        y.append(obs[(a,b)])\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)  # [bias, wA, wB]\n",
    "    def f(a,b):\n",
    "        return _clip01(beta[0] + beta[1]*a + beta[2]*b)\n",
    "    pred = {(a,b): f(a,b) for a,b in obs}\n",
    "    return _rmse(obs, pred), pred, {\"bias\": float(beta[0]), \"wA\": float(beta[1]), \"wB\": float(beta[2])}\n",
    "\n",
    "# main\n",
    "def detect_relationship(cpt_Pz1, tol=1e-6):\n",
    "    \"\"\"\n",
    "    cpt_Pz1: dict with keys like ('T','F'), ('F','T'), etc. or (1,0)\n",
    "             values are P(Z=1 | A=a, B=b).\n",
    "    Returns dict: {label: {'rmse':..., 'pred':..., 'params':...}}, plus 'best'.\n",
    "    \"\"\"\n",
    "    obs = _ensure_keys(cpt_Pz1)\n",
    "\n",
    "    # logicals\n",
    "    models = {}\n",
    "    for label, proto in [\n",
    "        (\"Logical OR\", _logical_or()),\n",
    "        (\"Logical AND\", _logical_and()),\n",
    "        (\"Logical XOR\", _logical_xor()),\n",
    "        (\"Logical XNOR\", _logical_xnor()),\n",
    "    ]:\n",
    "        rmse = _rmse(obs, proto)\n",
    "        models[label] = {\"rmse\": rmse, \"pred\": proto, \"params\": None}\n",
    "\n",
    "    # parametrics\n",
    "    rmse_noisy_or, pred_noisy_or, par_noisy_or = _fit_noisy_or(obs)\n",
    "    models[\"Noisy OR\"] = {\"rmse\": rmse_noisy_or, \"pred\": pred_noisy_or, \"params\": par_noisy_or}\n",
    "\n",
    "    rmse_noisy_and, pred_noisy_and, par_noisy_and = _fit_noisy_and(obs)\n",
    "    models[\"Noisy AND\"] = {\"rmse\": rmse_noisy_and, \"pred\": pred_noisy_and, \"params\": par_noisy_and}\n",
    "\n",
    "    try:\n",
    "        rmse_add, pred_add, par_add = _fit_additive(obs)\n",
    "        models[\"Additive\"] = {\"rmse\": rmse_add, \"pred\": pred_add, \"params\": par_add}\n",
    "    except Exception:\n",
    "        # numpy not available -> skip additive\n",
    "        pass\n",
    "\n",
    "    # pick best\n",
    "    best_label = min(models.keys(), key=lambda k: models[k][\"rmse\"])\n",
    "    models[\"best\"] = best_label\n",
    "    return models\n",
    "\n",
    "# Logical OR (deterministic)\n",
    "logical_or = {('T','T'):1, ('T','F'):1, ('F','T'):1, ('F','F'):0}\n",
    "\n",
    "# “Noisy OR” example from your first sheet\n",
    "noisy_or = {('T','T'):0.7, ('T','F'):0.6, ('F','T'):0.5, ('F','F'):0.0}\n",
    "\n",
    "# Logical AND (deterministic)\n",
    "logical_and = {('T','T'):1, ('T','F'):0, ('F','T'):0, ('F','F'):0}\n",
    "\n",
    "# “Noisy AND” toy (monotone, biggest at TT)\n",
    "noisy_and = {('T','T'):0.7, ('T','F'):0.6, ('F','T'):0.5, ('F','F'):0.0}\n",
    "\n",
    "# Logical XOR / XNOR examples\n",
    "logical_xor  = {('T','T'):0, ('T','F'):1, ('F','T'):1, ('F','F'):0}\n",
    "logical_xnor = {('T','T'):1, ('T','F'):0, ('F','T'):0, ('F','F'):1}\n",
    "\n",
    "# Additive sheet example\n",
    "additive = {('T','T'):0.7, ('T','F'):0.6, ('F','T'):0.5, ('F','F'):0.0}\n",
    "\n",
    "for name, tbl in [\n",
    "    (\"Logical OR\", logical_or),\n",
    "    (\"Noisy OR\", noisy_or),\n",
    "    (\"Logical AND\", logical_and),\n",
    "    (\"Noisy AND (toy)\", noisy_and),\n",
    "    (\"Logical XOR\", logical_xor),\n",
    "    (\"Logical XNOR\", logical_xnor),\n",
    "    (\"Additive\", additive),\n",
    "]:\n",
    "    res = detect_relationship(tbl)\n",
    "    print(name, \"→ best:\", res[\"best\"], \"rmse:\", round(res[res[\"best\"]][\"rmse\"], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "114e8dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecstazine -> ['Neurofill', 'Social_Activity']\n",
      "Neurofill -> ['Social_Activity']\n",
      "Social_Activity -> ['Squeaking']\n",
      "Squeaking -> []\n",
      "Ecstazine ['given', 'not_given']\n",
      "Neurofill ['high', 'low']\n",
      "Social_Activity ['high', 'low']\n",
      "Squeaking ['short', 'long']\n",
      "\n",
      "RatsNet → Noisy OR {'leak': 0.09999998658895493, 'pA': 0.5999999238385105, 'pB': 0.7999999950329464}\n",
      "RatsNoisyOr → Noisy OR {'leak': 0.09999998658895493, 'pA': 0.5999999238385105, 'pB': 0.7999999950329464}\n",
      "Raw table: {('True', 'True'): 0.8999999761581421, ('True', 'False'): 0.8999999761581421, ('False', 'True'): 0.10000000894069672, ('False', 'False'): 0.009999999776482582}\n",
      "q1Net (Trav,Fraud) → FP=True: Noisy OR {'leak': 0.009999999776482582, 'pA': 0.8989898749300198, 'pB': 0.09090910014534781}\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager # release resource automatically after using it\n",
    "def _temporarily_set_findings(net, findings_dict):\n",
    "    \"\"\"findings_dict: {node_name: state_name_or_index}\"\"\"\n",
    "    saved = net.findings()  # current evidence (indices)\n",
    "    try:\n",
    "        # Apply new evidence\n",
    "        for k, v in findings_dict.items():\n",
    "            node = net.node(k)\n",
    "            if isinstance(v, int):\n",
    "                node.finding(v)\n",
    "            else:\n",
    "                node.finding(v)  # state(name)\n",
    "        net.update()\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore previous evidence\n",
    "        net.retractFindings()\n",
    "        for k, v in (saved or {}).items():\n",
    "            net.node(k).finding(v)\n",
    "        net.update()\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def _resolve_state_index(node, spec, use_title=False):\n",
    "    \"\"\"\n",
    "    Resolve a state spec (int or str) to an index for a given node.\n",
    "    If use_title=True, string is matched against state titles first, then names.\n",
    "    \"\"\"\n",
    "    if isinstance(spec, int):\n",
    "        return spec\n",
    "    if use_title:\n",
    "        st = node.stateByTitle(spec) or node.state(spec)\n",
    "    else:\n",
    "        st = node.state(spec) or node.stateByTitle(spec)\n",
    "    if st is None:\n",
    "        raise ValueError(f\"Unknown state '{spec}' for node '{node.name()}'\")\n",
    "    return st.stateNum\n",
    "\n",
    "def _state_names_by_indices(node, idxs):\n",
    "    names = []\n",
    "    for i in idxs:\n",
    "        st = node.state(i)\n",
    "        if st is None:\n",
    "            raise ValueError(f\"Bad state index {i} for node '{node.name()}'\")\n",
    "        names.append(st.name())\n",
    "    return names\n",
    "\n",
    "def cpt_Pequals_from_bn(\n",
    "    net,\n",
    "    child_name,\n",
    "    parent_names,\n",
    "    child_state=None,                # str or int (required if child has >1 state)\n",
    "    parent_state_sets=None,          # dict: {parent_name: [state specs (str or int), ...]}\n",
    "    iterate=\"all\",                   # \"all\" | \"first_two\" | \"given\"\n",
    "    use_titles=False                 # match strings against titles first if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a table of P(child = child_state | parent assignments), without assuming 'true/false'.\n",
    "\n",
    "    Returns:\n",
    "      dict mapping tuple of parent state NAMES (in the same order as parent_names)\n",
    "      to probability of the specified child state.\n",
    "      e.g. {('given','high'): 0.63, ('not given','low'): 0.12, ...}\n",
    "    \"\"\"\n",
    "    child = net.node(child_name)\n",
    "    if child is None:\n",
    "        raise ValueError(f\"Child node '{child_name}' not found\")\n",
    "\n",
    "    # Resolve which child state to measure\n",
    "    if child_state is None:\n",
    "        # default: first state (index 0)\n",
    "        child_idx = 0\n",
    "    else:\n",
    "        child_idx = _resolve_state_index(child, child_state, use_title=use_titles)\n",
    "\n",
    "    # For each parent, decide which state indices to iterate\n",
    "    parent_state_idxs = []\n",
    "    parent_state_names = []  # parallel names for pretty keys\n",
    "    for p_name in parent_names:\n",
    "        p_node = net.node(p_name)\n",
    "        if p_node is None:\n",
    "            raise ValueError(f\"Parent node '{p_name}' not found\")\n",
    "\n",
    "        if iterate == \"given\":\n",
    "            if not parent_state_sets or p_name not in parent_state_sets:\n",
    "                raise ValueError(f\"Provide parent_state_sets[{p_name}] when iterate='given'\")\n",
    "            idxs = [_resolve_state_index(p_node, s, use_title=use_titles) for s in parent_state_sets[p_name]]\n",
    "\n",
    "        elif iterate == \"first_two\":\n",
    "            # take first two states (works for many binary-ish nodes)\n",
    "            idxs = [0, 1] if p_node.numberStates() >= 2 else [0]\n",
    "\n",
    "        else:  # \"all\"\n",
    "            idxs = list(range(p_node.numberStates()))\n",
    "\n",
    "        parent_state_idxs.append(idxs)\n",
    "        parent_state_names.append(_state_names_by_indices(p_node, idxs))\n",
    "\n",
    "    # Iterate Cartesian product of selected parent states\n",
    "    table = {}\n",
    "    for combo_idxs in product(*parent_state_idxs):\n",
    "        # Build findings dict using indices (your ctx manager accepts int or str)\n",
    "        evid = {p: idx for p, idx in zip(parent_names, combo_idxs)}\n",
    "\n",
    "        with _temporarily_set_findings(net, evid):\n",
    "            prob = child.beliefs()[child_idx]\n",
    "\n",
    "        # Pretty key using parent state NAMES\n",
    "        key = tuple(names[i_pos] for names, i_pos in zip(parent_state_names, [idxs.index(i) for idxs, i in zip(parent_state_idxs, combo_idxs)]))\n",
    "        # But the above maps to relative positions; instead, resolve absolute names cleanly:\n",
    "        key = tuple(net.node(p).state(i).name() for p, i in zip(parent_names, combo_idxs))\n",
    "\n",
    "        table[key] = prob\n",
    "\n",
    "    return table\n",
    "\n",
    "def relabel_to01(tbl, parent_names, positives):\n",
    "    # positives: {'Ecstazine': 'given', 'Neurofill': 'high'}\n",
    "    p0, p1 = parent_names\n",
    "    new = {}\n",
    "    for (a_lbl, b_lbl), v in tbl.items():\n",
    "        a = 1 if a_lbl == positives[p0] else 0\n",
    "        b = 1 if b_lbl == positives[p1] else 0\n",
    "        new[(a, b)] = float(v)\n",
    "    return new\n",
    "\n",
    "def get_BN_node_states(net):\n",
    "    structure = \"\"\n",
    "    for node in net.nodes():\n",
    "        states = [s.name() for s in node.states()]\n",
    "        structure += f\"{node.name()} {states}\\n\"\n",
    "    return structure\n",
    "\n",
    "printNet(RatsNet)\n",
    "print(get_BN_node_states(RatsNet))\n",
    "\n",
    "tbl = cpt_Pequals_from_bn(\n",
    "    RatsNet, \"Social_Activity\", [\"Ecstazine\",\"Neurofill\"],\n",
    "    child_state=\"high\",\n",
    "    iterate=\"given\",\n",
    "    parent_state_sets={\"Ecstazine\":[\"given\",\"not_given\"], \"Neurofill\":[\"high\",\"low\"]}\n",
    ")\n",
    "tbl01 = relabel_to01(tbl, [\"Ecstazine\",\"Neurofill\"], {\"Ecstazine\":\"given\",\"Neurofill\":\"high\"})\n",
    "res = detect_relationship(tbl01)\n",
    "print(\"RatsNet →\", res[\"best\"], res[res[\"best\"]][\"params\"])\n",
    "\n",
    "tbl = cpt_Pequals_from_bn(\n",
    "    RatsNoisyOr, \"Social_Activity\", [\"Ecstazine\",\"Neurofill\"],\n",
    "    child_state=\"high\",\n",
    "    iterate=\"given\",\n",
    "    parent_state_sets={\"Ecstazine\":[\"given\",\"not_given\"], \"Neurofill\":[\"high\",\"low\"]}\n",
    ")\n",
    "tbl01 = relabel_to01(tbl, [\"Ecstazine\",\"Neurofill\"], {\"Ecstazine\":\"given\",\"Neurofill\":\"high\"})\n",
    "res = detect_relationship(tbl01)\n",
    "print(\"RatsNoisyOr →\", res[\"best\"], res[res[\"best\"]][\"params\"])\n",
    "\n",
    "q1Net = Net('nets/Q1-AccessmentLab.neta')\n",
    "# --- extract the CPT slice for FP=True ---\n",
    "tbl = cpt_Pequals_from_bn(\n",
    "    q1Net,                      # your network\n",
    "    child_name=\"FP\",\n",
    "    parent_names=[\"Trav\", \"Fraud\"],\n",
    "    child_state=\"True\",        # child state of interest\n",
    "    iterate=\"given\",\n",
    "    parent_state_sets={\n",
    "        \"Trav\":  [\"True\", \"False\"],   # order matters: positive first\n",
    "        \"Fraud\": [\"True\", \"False\"]\n",
    "    }\n",
    ")\n",
    "print(\"Raw table:\", tbl)  # e.g. {('True','True'): p11, ('True','False'): p10, ...}\n",
    "\n",
    "# --- map to binary keys (1 for the positive label you chose above) ---\n",
    "tbl01 = relabel_to01(\n",
    "    tbl,\n",
    "    parent_names=[\"Trav\", \"Fraud\"],\n",
    "    positives={\"Trav\": \"True\", \"Fraud\": \"True\"}\n",
    ")\n",
    "\n",
    "# --- detect relationship ---\n",
    "res = detect_relationship(tbl01)\n",
    "best = res[\"best\"]\n",
    "print(\"q1Net (Trav,Fraud) → FP=True:\", best, res[best][\"params\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79846d",
   "metadata": {},
   "source": [
    "# Build Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c50af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from math import comb\n",
    "from bni_netica.bni_netica import Net\n",
    "\n",
    "def _dirichlet(alphas):\n",
    "    # Dirichlet without numpy: sample Gammas and normalize\n",
    "    # alpha = 1.0 → uniform random distribution. values spread across [0,1], sometimes 0.9/0.1, sometimes 0.5/0.5\n",
    "    # alpha < 1.0 → sparse, more extreme values (e.g. 0.99/0.01)\n",
    "    # alpha > 1.0 → dense, more balanced values (e.g. 0.6/0.4)\n",
    "    import random\n",
    "    vals = [random.gammavariate(a, 1.0) for a in alphas]\n",
    "    s = sum(vals) or 1.0\n",
    "    return [v / s for v in vals]\n",
    "\n",
    "def build_random_bn(\n",
    "    n_nodes=8,\n",
    "    n_vstructures=None,      # None -> auto\n",
    "    n_common_causes=None,    # None -> auto\n",
    "    n_chains=None,           # None -> auto\n",
    "    name=\"RandomNet\",\n",
    "    seed=None,\n",
    "    states=(\"False\", \"True\"),\n",
    "    cpt_mode=\"random\",       # \"random\" or \"uniform\"\n",
    "    dirichlet_alpha=1.0,     # used when cpt_mode=\"random\"\n",
    "    extra_edge_prob=0.0,     # optional sparse extra edges (still acyclic) - Makes the graph denser => 1 mean DAG = maximum density\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: (net, node_names, edges)\n",
    "      - net: compiled Netica network\n",
    "      - node_names: list of node names (A,B,...)\n",
    "      - edges: set of (parent, child) tuples\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # ----- node names A, B, C ... then A1, B1 ... -----\n",
    "    def make_names(k):\n",
    "        if k <= 26:\n",
    "            return list(string.ascii_uppercase[:k])\n",
    "        base = list(string.ascii_uppercase)\n",
    "        out, idx = [], 0\n",
    "        while len(out) < k:\n",
    "            suffix = \"\" if idx == 0 else str(idx)\n",
    "            for ch in base:\n",
    "                out.append(f\"{ch}{suffix}\")\n",
    "                if len(out) == k:\n",
    "                    break\n",
    "            idx += 1\n",
    "        return out\n",
    "\n",
    "    node_names = make_names(n_nodes)\n",
    "\n",
    "    # ----- start an empty net & add nodes (all with same states) -----\n",
    "    net = Net()\n",
    "    net.name(name)\n",
    "    for nm in node_names:\n",
    "        net.addNode(nm, states=list(states))\n",
    "\n",
    "    # ----- pick a random topological order; forward edges only -> DAG -----\n",
    "    topo = node_names[:]\n",
    "    random.shuffle(topo)\n",
    "    pos = {nm: i for i, nm in enumerate(topo)}\n",
    "\n",
    "    edges = set()\n",
    "\n",
    "    def add_edge(u, v):\n",
    "        if u == v:\n",
    "            return False\n",
    "        if (u, v) in edges:\n",
    "            return False\n",
    "        if pos[u] >= pos[v]:  # only forward in topo order\n",
    "            return False\n",
    "        edges.add((u, v))\n",
    "        return True\n",
    "\n",
    "    # ----- auto-counts for motif requests -----\n",
    "    # The number of distinct ordered triples i<j<k is C(n,3).\n",
    "    # We cap each motif type well below that to avoid over-dense graphs.\n",
    "    max_triples = comb(n_nodes, 3) if n_nodes >= 3 else 0\n",
    "\n",
    "    def auto_count(requested, density=0.25):\n",
    "        if requested is None:\n",
    "            cap = int(max_triples * density)\n",
    "            return random.randint(0, cap) if cap > 0 else 0\n",
    "        return min(int(requested), max_triples)\n",
    "\n",
    "    n_vstructures = auto_count(n_vstructures, density=0.25)\n",
    "    n_common_causes = auto_count(n_common_causes, density=0.25)\n",
    "    n_chains = auto_count(n_chains, density=0.25)\n",
    "\n",
    "    # helper to sample a triple consistent with topo order (i<j<k)\n",
    "    def sample_triple():\n",
    "        i, j, k = sorted(random.sample(range(n_nodes), 3))\n",
    "        return topo[i], topo[j], topo[k]  # earliest, middle, latest\n",
    "\n",
    "    # ---- place v-structures: A -> C <- B (A<C, B<C) ----\n",
    "    attempts, placed = 0, 0\n",
    "    MAX_TRIES = 50 * max(1, n_vstructures)\n",
    "    while placed < n_vstructures and attempts < MAX_TRIES:\n",
    "        attempts += 1\n",
    "        A, B, C = sample_triple()   # A<B<C in order\n",
    "        ok = add_edge(A, C) & add_edge(B, C)\n",
    "        if ok:\n",
    "            placed += 1\n",
    "\n",
    "    # ---- place common causes: C -> A and C -> B (C<A, C<B) ----\n",
    "    attempts, placed = 0, 0\n",
    "    MAX_TRIES = 50 * max(1, n_common_causes)\n",
    "    while placed < n_common_causes and attempts < MAX_TRIES:\n",
    "        attempts += 1\n",
    "        C, A, B = sample_triple()   # C<A<B\n",
    "        ok = add_edge(C, A) & add_edge(C, B)\n",
    "        if ok:\n",
    "            placed += 1\n",
    "\n",
    "    # ---- place chains: A -> B -> C (A<B<C) ----\n",
    "    attempts, placed = 0, 0\n",
    "    MAX_TRIES = 50 * max(1, n_chains)\n",
    "    while placed < n_chains and attempts < MAX_TRIES:\n",
    "        attempts += 1\n",
    "        A, B, C = sample_triple()   # A<B<C\n",
    "        ok = add_edge(A, B)\n",
    "        ok &= add_edge(B, C)\n",
    "        if ok:\n",
    "            placed += 1\n",
    "\n",
    "    # optional sprinkle of extra forward edges (keeps DAG)\n",
    "    if extra_edge_prob > 0:\n",
    "        for i, u in enumerate(topo):\n",
    "            for v in topo[i+1:]:\n",
    "                if random.random() < extra_edge_prob:\n",
    "                    add_edge(u, v)\n",
    "\n",
    "    # ---- materialize edges in the Netica net ----\n",
    "    for (u, v) in edges:\n",
    "        net.node(u).addChildren([v])\n",
    "\n",
    "    # ---- initialize CPTs ----\n",
    "    # \"uniform\": each row uniform\n",
    "    # \"random\": each row ~ Dirichlet(alpha,...,alpha)\n",
    "    for n in net.nodes():\n",
    "        s = n.numberStates()\n",
    "        parents = n.parents()\n",
    "        num_rows = 1\n",
    "        for p in parents:\n",
    "            num_rows *= p.numberStates()\n",
    "\n",
    "        rows = []\n",
    "        for _ in range(num_rows):\n",
    "            if cpt_mode == \"uniform\":\n",
    "                rows.append([1.0 / s] * s)\n",
    "            else:\n",
    "                rows.append(_dirichlet([dirichlet_alpha] * s))\n",
    "        n.cpt(rows)\n",
    "        n.experience(1)\n",
    "\n",
    "    # compile + save\n",
    "    net.compile()\n",
    "    save_path = save_path + f\"{name}.dne\"\n",
    "    net.write(save_path)\n",
    "\n",
    "    return net, node_names, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466022cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./nets/outputs/\"\n",
    "\n",
    "# 1) Everything auto, random CPTs, saved to AutoBN.dne\n",
    "net, nodes, edges = build_random_bn(\n",
    "    n_nodes=10,\n",
    "    name=\"AutoBN\",\n",
    "    seed=7,\n",
    "    cpt_mode=\"random\",\n",
    "    dirichlet_alpha=1.0,\n",
    "    save_path=output_path\n",
    ")\n",
    "\n",
    "# 2) Explicit motif counts, uniform CPTs (i.e., 50/50 for binary)\n",
    "net2, nodes2, edges2 = build_random_bn(\n",
    "    n_nodes=8,\n",
    "    n_vstructures=3,\n",
    "    n_common_causes=2,\n",
    "    n_chains=1,\n",
    "    name=\"UniformBN\",\n",
    "    cpt_mode=\"uniform\",\n",
    "    save_path=output_path\n",
    ")\n",
    "\n",
    "# 3) Heavier randomization (skewed rows) by lowering alpha\n",
    "net3, nodes3, edges3 = build_random_bn(\n",
    "    n_nodes=12,\n",
    "    n_vstructures=None,        # auto\n",
    "    n_common_causes=None,      # auto\n",
    "    n_chains=None,             # auto\n",
    "    name=\"SkewBN\",\n",
    "    cpt_mode=\"random\",\n",
    "    dirichlet_alpha=0.3,       # more spiky probabilities\n",
    "    extra_edge_prob=0.05,\n",
    "    save_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1b9bd",
   "metadata": {},
   "source": [
    "## Test the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2ad108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net2, nodes2, edges2 = build_random_bn(\n",
    "    n_nodes=40,\n",
    "    n_vstructures=3,\n",
    "    n_common_causes=2,\n",
    "    n_chains=1,\n",
    "    name=\"FortyRandomBN\",\n",
    "    cpt_mode=\"random\",\n",
    "    extra_edge_prob=1.0/(40-1),  # approx 1 extra edge per node on average\n",
    "    save_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cca16",
   "metadata": {},
   "source": [
    "#### Archive codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a66aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def joint_prob(net, assignments):\n",
    "#     \"\"\"\n",
    "#     assignments: dict { node_name: state_index or state_name }\n",
    "#                  (state_name must exist on that node)\n",
    "#     Returns P(assignments | current findings)\n",
    "#     \"\"\"\n",
    "#     # Build a stable order\n",
    "#     items = list(assignments.items())\n",
    "\n",
    "#     # Make node list\n",
    "#     node_list = g.NewNodeList2_bn(len(items), net.eNet)\n",
    "\n",
    "#     # States array (Netica 'state_bn' = c_int)\n",
    "#     states = (state_bn * len(items))()\n",
    "\n",
    "#     for i, (name, val) in enumerate(items):\n",
    "#         node = net.node(name)\n",
    "#         if node is None:\n",
    "#             raise ValueError(f\"Unknown node '{name}'\")\n",
    "\n",
    "#         # Put node pointer into nodelist\n",
    "#         g.SetNthNode_bn(node_list, i, node.eId)\n",
    "\n",
    "#         # Resolve state index\n",
    "#         if isinstance(val, int):\n",
    "#             s_idx = val\n",
    "#         else:\n",
    "#             st = node.state(val)\n",
    "#             if st is None:\n",
    "#                 raise ValueError(f\"Unknown state '{val}' on node '{name}'\")\n",
    "#             s_idx = st.stateNum\n",
    "\n",
    "#         states[i] = s_idx\n",
    "\n",
    "#     # Make sure beliefs are up to date\n",
    "#     net.update()\n",
    "\n",
    "#     # Call Netica\n",
    "#     prob = g.JointProbability_bn(node_list, states)\n",
    "#     return float(prob)\n",
    "\n",
    "\n",
    "# def xy_joint_table_by_Z(net, X, Y, Z):\n",
    "#     nodeZ = net.node(Z)\n",
    "#     table = {}\n",
    "#     for zi in range(nodeZ.numberStates()):\n",
    "#         zname = nodeZ.state(zi).name()\n",
    "#         with temporarily_set_findings(net, {Z: zi}):\n",
    "#             # Example: return the joint table for X×Y under Z=zi\n",
    "#             nodeX, nodeY = net.node(X), net.node(Y)\n",
    "#             j = {}\n",
    "#             for xi in range(nodeX.numberStates()):\n",
    "#                 for yi in range(nodeY.numberStates()):\n",
    "#                     p = joint_prob(net, {X: xi, Y: yi})\n",
    "#                     j[(nodeX.state(xi).name(), nodeY.state(yi).name())] = p\n",
    "#             table[zname] = j\n",
    "#     return table\n",
    "\n",
    "# def does_Z_affect_XY(net, X, Y, Z, tol=1e-12):\n",
    "#     # the result table (a Python dict) that stores all the joint probabilities you computed for each value of Z.\n",
    "#     # dict { z_state_name: { (x_state_name, y_state_name): prob, ... }, ... }\n",
    "#     tbl = xy_joint_table_by_Z(net, X, Y, Z) \n",
    "    \n",
    "#     # Compare the first table to the others\n",
    "#     it = iter(tbl.values())\n",
    "#     first = next(it)\n",
    "#     for other in it:\n",
    "#         for k in first:\n",
    "#             if abs(first[k] - other[k]) > tol:\n",
    "#                 return True, tbl  # differs for at least one state of Z\n",
    "#     return False, tbl\n",
    "\n",
    "# relatedToPesticideUse = myNet.node(\"PesticideInRiver\").parents()\n",
    "# print('parents of PesticideInRiver:', [n.name() for n in relatedToPesticideUse])\n",
    "# set1 = set(n.name() for n in relatedToPesticideUse)\n",
    "\n",
    "# relatedToPesticideUse = myNet.node(\"RiverFlow\").parents()\n",
    "# print('parents of RiverFlow:', [n.name() for n in relatedToPesticideUse])\n",
    "# set2 = set(n.name() for n in relatedToPesticideUse)\n",
    "\n",
    "# print('Intersection:', set1.intersection(set2))\n",
    "\n",
    "# relatedToPesticideUse = myNet.node(\"PesticideInRiver\").getRelated(\"d_connected\")\n",
    "# print('relatedTo PesticideInRiver:', [n.name() for n in relatedToPesticideUse])\n",
    "# set1 = set(n.name() for n in relatedToPesticideUse)\n",
    "\n",
    "# relatedToPesticideUse = myNet.node(\"RiverFlow\").getRelated(\"d_connected\")\n",
    "# print('relatedTo RiverFlow:', [n.name() for n in relatedToPesticideUse])\n",
    "# set2 = set(n.name() for n in relatedToPesticideUse)\n",
    "\n",
    "# print('Intersection:', set1.intersection(set2))\n",
    "\n",
    "# print()\n",
    "\n",
    "# print('[d-connected nodes between PesticideInRiver and RiverFlow]')\n",
    "# nodes = bni_utils.findAllDConnectedNodes(myNet, \"PesticideInRiver\", \"RiverFlow\")\n",
    "# print([n.name() for n in nodes])\n",
    "\n",
    "# print('[d-connected nodes between PesticideUse and Rainfall]')\n",
    "\n",
    "# nodes = bni_utils.findAllDConnectedNodes(myNet, \"PesticideUse\", \"Rainfall\")\n",
    "# print([n.name() for n in nodes])\n",
    "\n",
    "# print('[d-connected nodes between PesticideUse and FishAbundance]')\n",
    "# nodes = bni_utils.findAllDConnectedNodes(myNet, \"PesticideUse\", \"FishAbundance\")\n",
    "# print([n.name() for n in nodes])\n",
    "\n",
    "# print('[d-connected nodes between PesticideUse and Rainfall | PesticideInRiver]')\n",
    "# myNet.node('PesticideInRiver').finding('Low')\n",
    "# nodes = bni_utils.findAllDConnectedNodes(myNet, \"PesticideUse\", \"Rainfall\")\n",
    "# print([n.name() for n in nodes])\n",
    "# myNet.retractFindings()\n",
    "\n",
    "# print('[d-connected nodes between PesticideUse and Rainfall | FishAbundance]')\n",
    "# myNet.node('FishAbundance').finding('Low')\n",
    "# arcs = bni_utils.findAllDConnectedNodes(myNet, \"PesticideUse\", \"Rainfall\", {\"arcs\": True})\n",
    "# print([arc for arc in arcs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c9718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
