{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdc10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BAYESIAN NETWORK MODEL PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "Total tests: 540\n",
      "Models tested: gpt-oss:latest, llama3.1:70b, qwen3:8b\n",
      "Test types: elementary_test, numerical_test\n",
      "Question categories: dependency, common_cause, common_effect, blocked_evidence, probability, evidence_change_relationship\n",
      "Network size: 5 nodes\n",
      "================================================================================\n",
      "\n",
      "OVERALL PERFORMANCE SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|    |     Model      |  Framework  |   Avg Score |   Std Dev |   Total Tests |   Improvement |\n",
      "+====+================+=============+=============+===========+===============+===============+\n",
      "|  0 | gpt-oss:latest |  Raw Model  |       0.594 |     0.491 |           180 |         0.333 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|  1 | gpt-oss:latest |   BayMin    |       0.928 |     0.259 |           180 |         0.333 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|  2 |  llama3.1:70b  |  Raw Model  |       0.361 |     0.48  |           180 |         0.311 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|  3 |  llama3.1:70b  |   BayMin    |       0.672 |     0.469 |           180 |         0.311 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|  4 |    qwen3:8b    |  Raw Model  |       0.506 |     0.5   |           180 |         0.306 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "|  5 |    qwen3:8b    |   BayMin    |       0.811 |     0.391 |           180 |         0.306 |\n",
      "+----+----------------+-------------+-------------+-----------+---------------+---------------+\n",
      "\n",
      "\n",
      "DETAILED BREAKDOWN BY QUESTION TYPE\n",
      "================================================================================\n",
      "\n",
      "DEPENDENCY QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.733 |        1     |         0.267 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.467 |        0.633 |         0.167 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.767 |        1     |         0.233 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "COMMON CAUSE QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.8   |        0.967 |         0.167 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.467 |        0.933 |         0.467 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.667 |        0.967 |         0.3   |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "COMMON EFFECT QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.667 |        0.933 |         0.267 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.4   |        0.7   |         0.3   |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.667 |        1     |         0.333 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "BLOCKED EVIDENCE QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.233 |        0.9   |         0.667 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.267 |        0.833 |         0.567 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.267 |        0.433 |         0.167 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "PROBABILITY QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.4   |        0.967 |         0.567 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.033 |        0.033 |         0     |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.033 |        0.5   |         0.467 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "EVIDENCE CHANGE RELATIONSHIP QUESTIONS\n",
      "--------------------------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.733 |        0.8   |         0.067 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.533 |        0.9   |         0.367 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.633 |        0.967 |         0.333 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "\n",
      "PERFORMANCE BY TEST TYPE\n",
      "================================================================================\n",
      "\n",
      "ELEMENTARY TEST\n",
      "------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.633 |        0.92  |         0.287 |     150 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.427 |        0.8   |         0.373 |     150 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.6   |        0.873 |         0.273 |     150 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "NUMERICAL TEST\n",
      "------------------------------\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|    |     Model      |   Raw Avg |   BayMin Avg |   Improvement |   Tests |\n",
      "+====+================+===========+==============+===============+=========+\n",
      "|  0 | gpt-oss:latest |     0.4   |        0.967 |         0.567 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  1 |  llama3.1:70b  |     0.033 |        0.033 |         0     |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "|  2 |    qwen3:8b    |     0.033 |        0.5   |         0.467 |      30 |\n",
      "+----+----------------+-----------+--------------+---------------+---------+\n",
      "\n",
      "\n",
      "RUNTIME ANALYSIS\n",
      "================================================================================\n",
      "+----+----------------+-----------------------+--------------------------+--------------------+\n",
      "|    |     Model      |   Raw Avg Runtime (s) |   BayMin Avg Runtime (s) |  Runtime Overhead  |\n",
      "+====+================+=======================+==========================+====================+\n",
      "|  0 | gpt-oss:latest |                 68.25 |                    42.31 |      -25.94s       |\n",
      "+----+----------------+-----------------------+--------------------------+--------------------+\n",
      "|  1 |  llama3.1:70b  |                 37.06 |                    54.63 |      +17.56s       |\n",
      "+----+----------------+-----------------------+--------------------------+--------------------+\n",
      "|  2 |    qwen3:8b    |                 74.75 |                    50.73 |      -24.02s       |\n",
      "+----+----------------+-----------------------+--------------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def parse_and_print_results(csv_file_path):\n",
    "    \"\"\"\n",
    "    Parse the test log CSV and print a comprehensive performance table.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing test results\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"BAYESIAN NETWORK MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total tests: {len(df)}\")\n",
    "    print(f\"Models tested: {', '.join(df['model'].unique())}\")\n",
    "    print(f\"Test types: {', '.join(df['test_type'].unique())}\")\n",
    "    print(f\"Question categories: {', '.join(df['question_set_name'].unique())}\")\n",
    "    print(f\"Network size: {df['network_size'].iloc[0]} nodes\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary statistics\n",
    "    summary_data = []\n",
    "    \n",
    "    # Group by model and question type\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        # Raw model performance\n",
    "        raw_scores = model_data['raw_model_score'].values\n",
    "        raw_avg = np.mean(raw_scores)\n",
    "        raw_std = np.std(raw_scores)\n",
    "        \n",
    "        # BayMin framework performance  \n",
    "        baymin_scores = model_data['baymin_score'].values\n",
    "        baymin_avg = np.mean(baymin_scores)\n",
    "        baymin_std = np.std(baymin_scores)\n",
    "        \n",
    "        # Performance improvement\n",
    "        improvement = baymin_avg - raw_avg\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model,\n",
    "            'Framework': 'Raw Model',\n",
    "            'Avg Score': f\"{raw_avg:.3f}\",\n",
    "            'Std Dev': f\"{raw_std:.3f}\",\n",
    "            'Total Tests': len(raw_scores),\n",
    "            'Improvement': f\"{improvement:+.3f}\"\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model,\n",
    "            'Framework': 'BayMin',\n",
    "            'Avg Score': f\"{baymin_avg:.3f}\",\n",
    "            'Std Dev': f\"{baymin_std:.3f}\",\n",
    "            'Total Tests': len(baymin_scores),\n",
    "            'Improvement': f\"{improvement:+.3f}\"\n",
    "        })\n",
    "    \n",
    "    # Print overall summary table\n",
    "    print(\"\\nOVERALL PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 80)\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(tabulate(summary_df, headers='keys', tablefmt='grid', stralign='center'))\n",
    "    \n",
    "    # Detailed breakdown by question type\n",
    "    print(\"\\n\\nDETAILED BREAKDOWN BY QUESTION TYPE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for question_type in df['question_set_name'].unique():\n",
    "        print(f\"\\n{question_type.upper().replace('_', ' ')} QUESTIONS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        type_data = df[df['question_set_name'] == question_type]\n",
    "        type_summary = []\n",
    "        \n",
    "        for model in type_data['model'].unique():\n",
    "            model_type_data = type_data[type_data['model'] == model]\n",
    "            \n",
    "            raw_avg = np.mean(model_type_data['raw_model_score'].values)\n",
    "            baymin_avg = np.mean(model_type_data['baymin_score'].values)\n",
    "            improvement = baymin_avg - raw_avg\n",
    "            \n",
    "            type_summary.append({\n",
    "                'Model': model,\n",
    "                'Raw Avg': f\"{raw_avg:.3f}\",\n",
    "                'BayMin Avg': f\"{baymin_avg:.3f}\",\n",
    "                'Improvement': f\"{improvement:+.3f}\",\n",
    "                'Tests': len(model_type_data)\n",
    "            })\n",
    "        \n",
    "        type_df = pd.DataFrame(type_summary)\n",
    "        print(tabulate(type_df, headers='keys', tablefmt='grid', stralign='center'))\n",
    "    \n",
    "    # Performance by test type (elementary vs numerical)\n",
    "    print(\"\\n\\nPERFORMANCE BY TEST TYPE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for test_type in df['test_type'].unique():\n",
    "        print(f\"\\n{test_type.upper().replace('_', ' ')}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        test_data = df[df['test_type'] == test_type]\n",
    "        test_summary = []\n",
    "        \n",
    "        for model in test_data['model'].unique():\n",
    "            model_test_data = test_data[test_data['model'] == model]\n",
    "            \n",
    "            raw_avg = np.mean(model_test_data['raw_model_score'].values)\n",
    "            baymin_avg = np.mean(model_test_data['baymin_score'].values)\n",
    "            improvement = baymin_avg - raw_avg\n",
    "            \n",
    "            test_summary.append({\n",
    "                'Model': model,\n",
    "                'Raw Avg': f\"{raw_avg:.3f}\",\n",
    "                'BayMin Avg': f\"{baymin_avg:.3f}\",\n",
    "                'Improvement': f\"{improvement:+.3f}\",\n",
    "                'Tests': len(model_test_data)\n",
    "            })\n",
    "        \n",
    "        test_df = pd.DataFrame(test_summary)\n",
    "        print(tabulate(test_df, headers='keys', tablefmt='grid', stralign='center'))\n",
    "    \n",
    "    # Runtime analysis\n",
    "    print(\"\\n\\nRUNTIME ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    runtime_summary = []\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        raw_runtime = model_data['raw_model_runtime'].values\n",
    "        baymin_runtime = model_data['baymin_runtime'].values\n",
    "        \n",
    "        runtime_summary.append({\n",
    "            'Model': model,\n",
    "            'Raw Avg Runtime (s)': f\"{np.mean(raw_runtime):.2f}\",\n",
    "            'BayMin Avg Runtime (s)': f\"{np.mean(baymin_runtime):.2f}\",\n",
    "            'Runtime Overhead': f\"{np.mean(baymin_runtime) - np.mean(raw_runtime):+.2f}s\"\n",
    "        })\n",
    "    \n",
    "    runtime_df = pd.DataFrame(runtime_summary)\n",
    "    print(tabulate(runtime_df, headers='keys', tablefmt='grid', stralign='center'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Test the function\n",
    "csv_path = \"benchmarking/results/latest/test_log.csv\"\n",
    "parse_and_print_results(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ab6423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionType NetworkSize [Raw] gpt-oss:20b llama3.1:70b qwen3:8b [BayMin] gpt-oss:20b llama3.1:70b qwen3:8b\n",
      "Blocked Evidence 5 23 26 26 90 83 43\n",
      "Common Cause 5 80 46 66 96 93 96\n",
      "Common Effect 5 66 40 66 93 70 100\n",
      "Dependency 5 73 46 76 100 63 100\n",
      "Evidence Change Relationship 5 73 53 63 80 90 96\n",
      "Probability 5 40 3 3 96 3 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MODEL_ORDER = ['gpt-oss:latest', 'llama3.1:70b', 'qwen3:8b']\n",
    "MODEL_LABEL = {\n",
    "\t'gpt-oss:latest': 'gpt-oss:20b',\n",
    "\t'llama3.1:70b': 'llama3.1:70b',\n",
    "\t'qwen3:8b': 'qwen3:8b',\n",
    "}\n",
    "\n",
    "\n",
    "def _pct(x):\n",
    "\tif len(x) == 0 or np.isnan(x).all():\n",
    "\t\treturn None\n",
    "\treturn round(float(np.nanmean(x) * 100.0), 1)\n",
    "\n",
    "\n",
    "def print_compact_performance_table(csv_file_path: str) -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tPrint a compact table per requirement:\n",
    "\tQuestionType NetworkSize Raw[gpt-oss:20b llama3.1:70b qwen3:8b] BayMin[gpt-oss:20b llama3.1:70b qwen3:8b]\n",
    "\tExample row: \"Dependency 5 60 30 50 90 70 80\"\n",
    "\tReturns the DataFrame used to print, for reuse/saving if needed.\n",
    "\t\"\"\"\n",
    "\t# Load\n",
    "\tdf = pd.read_csv(csv_file_path)\n",
    "\n",
    "\t# Aggregate means per (question_set_name, network_size, model)\n",
    "\tagg = (\n",
    "\t\tdf.groupby(['question_set_name', 'network_size', 'model'])\n",
    "\t\t.agg(raw_mean=('raw_model_score', 'mean'), baymin_mean=('baymin_score', 'mean'))\n",
    "\t\t.reset_index()\n",
    "\t)\n",
    "\n",
    "\t# Ensure all models appear even if missing for a question type\n",
    "\tqns = sorted(df['question_set_name'].unique().tolist())\n",
    "\tnsizes = sorted(df['network_size'].unique().tolist())\n",
    "\trows = []\n",
    "\tfor q in qns:\n",
    "\t\tfor ns in nsizes:\n",
    "\t\t\trow = {\n",
    "\t\t\t\t'QuestionType': q.replace('_', ' ').title(),\n",
    "\t\t\t\t'NetworkSize': ns,\n",
    "\t\t\t}\n",
    "\t\t\t# Raw block\n",
    "\t\t\tfor m in MODEL_ORDER:\n",
    "\t\t\t\tval = agg[(agg['question_set_name'] == q) & (agg['network_size'] == ns) & (agg['model'] == m)]['raw_mean']\n",
    "\t\t\t\tscore = _pct(val.values) if len(val) else None\n",
    "\t\t\t\trow[f'Raw {MODEL_LABEL[m]}'] = score\n",
    "\t\t\t# BayMin block\n",
    "\t\t\tfor m in MODEL_ORDER:\n",
    "\t\t\t\tval = agg[(agg['question_set_name'] == q) & (agg['network_size'] == ns) & (agg['model'] == m)]['baymin_mean']\n",
    "\t\t\t\tscore = _pct(val.values) if len(val) else None\n",
    "\t\t\t\trow[f'BayMin {MODEL_LABEL[m]}'] = score\n",
    "\t\t\trows.append(row)\n",
    "\n",
    "\twide_df = pd.DataFrame(rows)\n",
    "\n",
    "\t# Desired column ordering\n",
    "\tcols = (\n",
    "\t\t['QuestionType', 'NetworkSize'] +\n",
    "\t\t[*(f'Raw {MODEL_LABEL[m]}' for m in MODEL_ORDER)] +\n",
    "\t\t[*(f'BayMin {MODEL_LABEL[m]}' for m in MODEL_ORDER)]\n",
    "\t)\n",
    "\twide_df = wide_df[cols]\n",
    "\n",
    "\t# Print header with groups\n",
    "\traw_labels = [MODEL_LABEL[m] for m in MODEL_ORDER]\n",
    "\tbay_labels = [MODEL_LABEL[m] for m in MODEL_ORDER]\n",
    "\theader = (\n",
    "\t\t\"QuestionType NetworkSize \"\n",
    "\t\t\"[Raw] \" + \" \".join(raw_labels) + \" \"\n",
    "\t\t\"[BayMin] \" + \" \".join(bay_labels)\n",
    "\t)\n",
    "\tprint(header)\n",
    "\tfor _, r in wide_df.iterrows():\n",
    "\t\tvals = [\n",
    "\t\t\tstr(r['QuestionType']),\n",
    "\t\t\tstr(r['NetworkSize']),\n",
    "\t\t\t*[(\"\" if pd.isna(r[c]) else str(int(r[c]))) for c in [f'Raw {lab}' for lab in raw_labels]],\n",
    "\t\t\t*[(\"\" if pd.isna(r[c]) else str(int(r[c]))) for c in [f'BayMin {lab}' for lab in bay_labels]],\n",
    "\t\t]\n",
    "\t\tprint(\" \".join(vals))\n",
    "\n",
    "\treturn wide_df\n",
    "\n",
    "# Example usage (uncomment to run in the notebook):\n",
    "_ = print_compact_performance_table('benchmarking/results/latest/test_log.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cc2397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Raw</th>\n",
       "      <th colspan=\"3\" halign=\"left\">BayMin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>gpt-oss:20b</th>\n",
       "      <th>llama3.1:70b</th>\n",
       "      <th>qwen3:8b</th>\n",
       "      <th>gpt-oss:20b</th>\n",
       "      <th>llama3.1:70b</th>\n",
       "      <th>qwen3:8b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuestionType</th>\n",
       "      <th>NetworkSize</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Blocked Evidence</th>\n",
       "      <th>5</th>\n",
       "      <td>23.3</td>\n",
       "      <td>26.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>76.7</td>\n",
       "      <td>86.7</td>\n",
       "      <td>53.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>86.7</td>\n",
       "      <td>86.7</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>86.7</td>\n",
       "      <td>96.7</td>\n",
       "      <td>56.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>93.3</td>\n",
       "      <td>93.3</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Common Cause</th>\n",
       "      <th>5</th>\n",
       "      <td>80.0</td>\n",
       "      <td>46.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>86.7</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.3</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>86.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Common Effect</th>\n",
       "      <th>5</th>\n",
       "      <td>66.7</td>\n",
       "      <td>40.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>93.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>86.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>96.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Dependency</th>\n",
       "      <th>5</th>\n",
       "      <td>73.3</td>\n",
       "      <td>46.7</td>\n",
       "      <td>76.7</td>\n",
       "      <td>96.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.3</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>93.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>86.7</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Evidence Change Relationship</th>\n",
       "      <th>5</th>\n",
       "      <td>73.3</td>\n",
       "      <td>53.3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>86.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>80.0</td>\n",
       "      <td>96.7</td>\n",
       "      <td>83.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.3</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>83.3</td>\n",
       "      <td>46.7</td>\n",
       "      <td>93.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Probability</th>\n",
       "      <th>5</th>\n",
       "      <td>40.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>93.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>96.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>43.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Raw                        \\\n",
       "                                         gpt-oss:20b llama3.1:70b qwen3:8b   \n",
       "QuestionType                 NetworkSize                                     \n",
       "Blocked Evidence             5                  23.3         26.7     26.7   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "Common Cause                 5                  80.0         46.7     66.7   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "Common Effect                5                  66.7         40.0     66.7   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "Dependency                   5                  73.3         46.7     76.7   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "Evidence Change Relationship 5                  73.3         53.3     63.3   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "Probability                  5                  40.0          3.3      3.3   \n",
       "                             10                    -            -        -   \n",
       "                             30                    -            -        -   \n",
       "                             60                    -            -        -   \n",
       "\n",
       "                                              BayMin                        \n",
       "                                         gpt-oss:20b llama3.1:70b qwen3:8b  \n",
       "QuestionType                 NetworkSize                                    \n",
       "Blocked Evidence             5                  76.7         86.7     53.3  \n",
       "                             10                 86.7         86.7     50.0  \n",
       "                             30                 86.7         96.7     56.7  \n",
       "                             60                 93.3         93.3     80.0  \n",
       "Common Cause                 5                  90.0         86.7     93.3  \n",
       "                             10                100.0         93.3    100.0  \n",
       "                             30                100.0        100.0     93.3  \n",
       "                             60                100.0        100.0     86.7  \n",
       "Common Effect                5                  93.3         70.0     93.3  \n",
       "                             10                 90.0         80.0     90.0  \n",
       "                             30                 86.7         60.0     90.0  \n",
       "                             60                 90.0         70.0     96.7  \n",
       "Dependency                   5                  96.7         90.0     93.3  \n",
       "                             10                 90.0         93.3    100.0  \n",
       "                             30                 93.3         90.0    100.0  \n",
       "                             60                 90.0         86.7    100.0  \n",
       "Evidence Change Relationship 5                  86.7         90.0     80.0  \n",
       "                             10                 80.0         96.7     83.3  \n",
       "                             30                 80.0         73.3     90.0  \n",
       "                             60                 83.3         46.7     93.3  \n",
       "Probability                  5                  93.3          0.0     60.0  \n",
       "                             10                 96.7          0.0     26.7  \n",
       "                             30                 90.0          3.3     43.3  \n",
       "                             60                 90.0          0.0     33.3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def build_performance_table(csv_file_path: str, baymin_csv_file_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame indexed by (QuestionType, NetworkSize) with grouped column\n",
    "    headers: top-level framework (Raw, BayMin) and second-level model labels\n",
    "    (gpt-oss:20b, llama3.1:70b, qwen3:8b). Values are mean percentage scores.\n",
    "    Includes all network sizes present for each question type.\n",
    "    Raw means are computed ONLY from csv_file_path.\n",
    "    BayMin means are computed ONLY from baymin_csv_file_path (or csv_file_path if None).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Load sources separately (prevents leakage) ---\n",
    "    df_raw = pd.read_csv(csv_file_path)\n",
    "    df_bay = pd.read_csv(baymin_csv_file_path) if baymin_csv_file_path else df_raw.copy()\n",
    "\n",
    "    # --- 2) Aggregate separately ---\n",
    "    agg_raw = (\n",
    "        df_raw.groupby(['question_set_name', 'network_size', 'model'])['raw_model_score']\n",
    "              .mean()\n",
    "              .rename('raw_mean')\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    agg_bay = (\n",
    "        df_bay.groupby(['question_set_name', 'network_size', 'model'])['baymin_score']\n",
    "              .mean()\n",
    "              .rename('baymin_mean')\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- 3) Outer-merge the two aggregates on keys ---\n",
    "    agg = pd.merge(\n",
    "        agg_raw, agg_bay,\n",
    "        on=['question_set_name', 'network_size', 'model'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # --- 4) Build MultiIndex columns: (Framework, ModelLabel) ---\n",
    "    top, bottom = [], []\n",
    "    for fw in ['Raw', 'BayMin']:\n",
    "        for m in MODEL_ORDER:\n",
    "            top.append(fw)\n",
    "            bottom.append(MODEL_LABEL[m])\n",
    "    cols = pd.MultiIndex.from_arrays([top, bottom])\n",
    "\n",
    "    # --- 5) Assemble rows (include all sizes seen per question type across both sources) ---\n",
    "    rows, row_index = [], []\n",
    "    for q in sorted(agg['question_set_name'].dropna().unique().tolist()):\n",
    "        sub = agg[agg['question_set_name'] == q]\n",
    "        # union of sizes that appear in either raw or baymin for this question type\n",
    "        available_sizes = sorted(sub['network_size'].dropna().unique().tolist())\n",
    "        for ns in available_sizes:\n",
    "            row_vals = []\n",
    "            # Raw block (ordered by MODEL_ORDER)\n",
    "            for m in MODEL_ORDER:\n",
    "                val = sub[(sub['network_size'] == ns) & (sub['model'] == m)]['raw_mean']\n",
    "                row_vals.append(_pct(val.values) if len(val) and pd.notna(val.values[0]) else None)\n",
    "            # BayMin block (ordered by MODEL_ORDER)\n",
    "            for m in MODEL_ORDER:\n",
    "                val = sub[(sub['network_size'] == ns) & (sub['model'] == m)]['baymin_mean']\n",
    "                row_vals.append(_pct(val.values) if len(val) and pd.notna(val.values[0]) else None)\n",
    "            rows.append(row_vals)\n",
    "            row_index.append((q.replace('_', ' ').title(), ns))\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples(row_index, names=['QuestionType', 'NetworkSize'])\n",
    "    table = pd.DataFrame(rows, index=idx, columns=cols)\n",
    "\n",
    "\t\t# Replace NaN values and 0 values with \"-\" for better readability\n",
    "    table = table.fillna(\"-\")\n",
    "    excluded = table.columns[table.columns.get_level_values(0) == 'BayMin']\n",
    "    cols_to_change = table.columns.difference(excluded)\n",
    "    table.loc[:, cols_to_change] = table.loc[:, cols_to_change].replace(0, \"-\")\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def show_performance_table(csv_file_path: str, baymin_csv_file_path: str = None) -> pd.DataFrame:\n",
    "\t\"\"\"Build and display the grouped performance table in the notebook.\"\"\"\n",
    "\ttable = build_performance_table(csv_file_path, baymin_csv_file_path)\n",
    "\tdisplay(table)\n",
    "\treturn table\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "_ = show_performance_table('test_log.csv', 'baymin_test_log.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280860f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_latex(\n",
    "    table,\n",
    "    caption=\"Accuracy Comparison across Models and Network Sizes.\",\n",
    "    label=\"tab:performance\",\n",
    "    filename=None,\n",
    "    escape=True,\n",
    "    frameworks_order=(\"Raw\", \"BayMin\"),\n",
    "    starred=True,                 # use \\begin{table*} ... \\end{table*}\n",
    "    placement=\"[t]\",              # top placement like the sample\n",
    "    size_cmd=\"\\\\small\",           # \\small before tabular\n",
    "    end_size_cmd=\"\\\\normalsize\",  # \\normalsize after tabular\n",
    "    add_vertical_bar=True,         # add vertical bar between Raw | BayMin\n",
    "    display_require_packages=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Render a MultiIndex performance DataFrame to LaTeX in the requested style.\n",
    "\n",
    "    Expected table.columns to be a MultiIndex of (Framework, ModelName),\n",
    "    and table.index to be a MultiIndex of (QuestionType, NetworkSize).\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def tex_escape(s: str) -> str:\n",
    "        if not escape or s is None:\n",
    "            return s\n",
    "        # Minimal escaping for underscores; extend if needed\n",
    "        return str(s).replace(\"_\", \"\\\\_\")\n",
    "\n",
    "    # Sanity: ensure frameworks exist in columns level 0\n",
    "    col_level0 = table.columns.get_level_values(0)\n",
    "    frameworks_present = [fw for fw in frameworks_order if fw in set(col_level0)]\n",
    "    # Also include any unexpected frameworks at the end to avoid dropping columns\n",
    "    frameworks_present += [fw for fw in col_level0.unique() if fw not in frameworks_present]\n",
    "\n",
    "    # Count columns per framework\n",
    "    fw_cols = {fw: table.columns[col_level0 == fw] for fw in frameworks_present}\n",
    "    raw_count = len(fw_cols.get(\"Raw\", []))\n",
    "    bay_count = len(fw_cols.get(\"BayMin\", []))\n",
    "    left_count = sum(len(fw_cols[fw]) for fw in frameworks_present[:frameworks_present.index(\"BayMin\")] if fw in fw_cols)\n",
    "    # Column spec: two left columns (ll), then model columns; insert a '|' before BayMin group if requested\n",
    "    col_spec_parts = [\"l\", \"l\"]\n",
    "    for fw in frameworks_present:\n",
    "        if add_vertical_bar and fw == \"BayMin\":\n",
    "            col_spec_parts.append(\"|\")\n",
    "        col_spec_parts.extend([\"c\"] * len(fw_cols[fw]))\n",
    "    col_spec = \"\".join(col_spec_parts)\n",
    "\n",
    "    # Build LaTeX\n",
    "    lines = []\n",
    "    if display_require_packages:\n",
    "        lines.append(\"% Required packages:\")\n",
    "        lines.append(\"% \\\\usepackage{booktabs}\")   # top/mid/bottom rule\n",
    "        lines.append(\"% \\\\usepackage{multirow}\")   # multirow Question Type\n",
    "        lines.append(\"% \\\\usepackage{makecell}\")   # optional: \\\\thead\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    env = \"table*\" if starred else \"table\"\n",
    "    lines.append(f\"\\\\begin{{{env}}}{placement}\")\n",
    "    lines.append(\"  \\\\centering\")\n",
    "    lines.append(f\"  \\\\caption{{{caption}}}\")\n",
    "    lines.append(f\"  \\\\label{{{label}}}\")\n",
    "    if size_cmd:\n",
    "        lines.append(f\"  {size_cmd}\")\n",
    "\n",
    "    lines.append(f\"  \\\\begin{{tabular}}{{{col_spec}}}\")\n",
    "    lines.append(\"  \\\\toprule\")\n",
    "\n",
    "    # Header Row 1: empty for the two stub columns, then bold framework blocks\n",
    "    first_hdr_cells = [\"\", \"\"]\n",
    "    for fw in frameworks_present:\n",
    "        n = len(fw_cols[fw])\n",
    "        if n > 0:\n",
    "            if n == 1:\n",
    "                first_hdr_cells.append(f\"\\\\textbf{{{tex_escape(fw)}}}\")\n",
    "            else:\n",
    "                first_hdr_cells.append(f\"\\\\multicolumn{{{n}}}{{c}}{{\\\\textbf{{{tex_escape(fw)}}}}}\")\n",
    "    lines.append(\"  \" + \" & \".join(first_hdr_cells) + \" \\\\\\\\\")\n",
    "\n",
    "    # cmidrules aligning with framework blocks (columns start at 1)\n",
    "    # Two stub columns are #1 and #2. Framework blocks start at 3.\n",
    "    start_col = 3\n",
    "    for fw in frameworks_present:\n",
    "        n = len(fw_cols[fw])\n",
    "        if n > 0:\n",
    "            end_col = start_col + n - 1\n",
    "            lines.append(f\"  \\\\cmidrule(lr){{{start_col}-{end_col}}}\")\n",
    "            start_col = end_col + 1\n",
    "\n",
    "    # Header Row 2: stub headers then bold model names in order\n",
    "    second_hdr = [\"\\\\textbf{Question Type}\", \"\\\\textbf{Net}\"]\n",
    "    for fw in frameworks_present:\n",
    "        for _, model_name in fw_cols[fw]:\n",
    "            second_hdr.append(f\"\\\\textbf{{{tex_escape(model_name)}}}\")\n",
    "    lines.append(\"  \" + \" & \".join(second_hdr) + \" \\\\\\\\\")\n",
    "    lines.append(\"  \\\\midrule\")\n",
    "\n",
    "    # Body with \\multirow blocks for Question Type\n",
    "    # Group by Question Type (level 0 of index)\n",
    "    if not isinstance(table.index, pd.MultiIndex) or table.index.nlevels != 2:\n",
    "        raise ValueError(\"table.index must be a MultiIndex with (QuestionType, NetworkSize).\")\n",
    "\n",
    "    for qtype, subdf in table.groupby(level=0, sort=False):\n",
    "        qtype_tex = tex_escape(qtype)\n",
    "        n_rows = len(subdf)\n",
    "\n",
    "        # Iterate network sizes in index order\n",
    "        first_row = True\n",
    "        for (qt, net), row in subdf.iterrows():\n",
    "            net_tex = tex_escape(net)\n",
    "\n",
    "            # Left stub columns\n",
    "            if first_row:\n",
    "                left_stub = f\"\\\\multirow{{{n_rows}}}{{*}}{{{qtype_tex}}} & {net_tex}\"\n",
    "                first_row = False\n",
    "            else:\n",
    "                left_stub = f\" & {net_tex}\"\n",
    "\n",
    "            # Data columns in frameworks order\n",
    "            data_cells = []\n",
    "            for fw in frameworks_present:\n",
    "                for col in fw_cols[fw]:\n",
    "                    val = row[col]\n",
    "                    if pd.isna(val) or val == \"-\" or val == \"---\":\n",
    "                        data_cells.append(\"---\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            x = float(val)\n",
    "                            # Match sample: show 1 decimal; keep integers like 100 without .0 if exactly integer\n",
    "                            # if abs(x - round(x)) < 1e-9:\n",
    "                            if abs(x) == 100:\n",
    "                                data_cells.append(f\"{int(round(x))}\")\n",
    "                            else:\n",
    "                                data_cells.append(f\"{x:.1f}\")\n",
    "                        except (TypeError, ValueError):\n",
    "                            data_cells.append(tex_escape(str(val)))\n",
    "\n",
    "            lines.append(\"  \" + \" & \".join([left_stub] + data_cells) + \" \\\\\\\\\")\n",
    "        lines.append(\"  \\\\midrule\")\n",
    "\n",
    "    # Replace final midrule with bottomrule\n",
    "    if lines[-1].strip() == \"\\\\midrule\" or lines[-1].strip() == \"  \\\\midrule\":\n",
    "        lines[-1] = \"  \\\\bottomrule\"\n",
    "    else:\n",
    "        lines.append(\"  \\\\bottomrule\")\n",
    "\n",
    "    lines.append(\"  \\\\end{tabular}\")\n",
    "    if end_size_cmd:\n",
    "        lines.append(f\"  {end_size_cmd}\")\n",
    "    lines.append(f\"\\\\end{{{env}}}\")\n",
    "\n",
    "    latex_code = \"\\n\".join(lines)\n",
    "\n",
    "    if filename:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "\n",
    "    return latex_code\n",
    "\n",
    "\n",
    "def generate_latex_table(csv_file_path: str, baymin_csv_file_path: str = None, \n",
    "                        filename: str = None, display_require_packages: bool = True):\n",
    "    \"\"\"\n",
    "    Generate LaTeX table from CSV data and optionally save to file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to main CSV file\n",
    "        baymin_csv_file_path (str, optional): Path to BayMin CSV file\n",
    "        filename (str, optional): Output filename for LaTeX code\n",
    "    \n",
    "    Returns:\n",
    "        str: LaTeX table code\n",
    "    \"\"\"\n",
    "    # Build the performance table\n",
    "    table = build_performance_table(csv_file_path, baymin_csv_file_path)\n",
    "    \n",
    "    # Convert to LaTeX\n",
    "    latex_code = table_to_latex(table, filename=filename, display_require_packages=display_require_packages)\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Generate LaTeX table and save to file\n",
    "latex_code = generate_latex_table('test_log.csv', \n",
    "                                 'baymin_test_log.csv', \n",
    "                                 'performance_table.tex',\n",
    "                                 display_require_packages=False)\n",
    "\n",
    "print(latex_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
